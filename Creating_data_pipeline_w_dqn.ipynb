{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d571c4-eb8e-4391-bded-c7a3b6d92ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaa3a5-85ba-4c34-9e86-043e3da783d2",
   "metadata": {},
   "source": [
    "# Thoughts :\n",
    "\n",
    "- so the degree of being a friend or a foe is\n",
    "  a. degree of foe:  the amount of maximum possible value decressed only because of other players actions\n",
    "      - over time...\n",
    "      - the latest action.\n",
    "  b. degree of friend: the amount of maximum possible value increase only because of the other players actions.\n",
    "\n",
    "  This has be to taken at every valid action...right?\n",
    "\n",
    "\n",
    "  C is an observer:\n",
    "      A (-1|t)A(+1|t+1)A(+1|t+2)B(-1|t+3)B(-1|t+3)\n",
    "\n",
    "      - then the Friend(A|C) =  the amount of V(C) increased by A's actions.\n",
    "      - foe(A|C) = the amount of V(C) decreased by A's actions\n",
    "\n",
    "- but there is a catch... the decision of friend and foe cannot be based on or should be conditioned on ...that friend did not make as self sacrifising dicisions.... i mean there could be actions which went wrong but had a different intentions....\n",
    "\n",
    "- basically : P( actual intentions of of B | change is value of A  ) ... but how do we figure this out??\n",
    "\n",
    "- okay ... so here is what I shall do ... i'll only identify how the sequential dilemma is handeled by the DT.\n",
    "\n",
    "- but it would be better if we would identify and categorize the behaviour...should i use the change in value as an observation? ... but it would be redundant...\n",
    "\n",
    "\n",
    "- can we promote relationship building ... and to be sly ? and will that really improve the chances to win?\n",
    "\n",
    "\n",
    "\n",
    "- can it be a choice to maximize someone elses rewards? ... v(a') = v(a) + b_1.v(b) + b_2.v(c)\n",
    "\n",
    "    - where b_1 and b_2 are actions? choice based on v(a') ... but estimating and maximizing v(a)\n",
    "    - the idea is v(a) is proportional to v(a')\n",
    "    - a positive b_1 is supporting ... a negative b_1 is definance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b67f2-caa5-4376-a7c7-23ca3a24d0fd",
   "metadata": {},
   "source": [
    "Causality Confusion: The agent can't directly link its actions to the rewards because the environment changes independently. This makes it difficult to learn which actions are beneficial or harmful.\n",
    "\n",
    "\n",
    "Reward Prediction Difficulty: Traditional reinforcement learning relies on predicting future rewards based on actions. In this case, the rewards are unpredictable and not directly tied to the agent's choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8d2a1-8289-472e-b9a3-6131ff56bbb1",
   "metadata": {},
   "source": [
    " - so the time other agents are taking actions.... as it is a sequential process.. the agent can model it as a markoves chains rather than markoves decisions process.... with Monte Carlo Simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21220d9-3b92-4654-aa68-e3f025dfa67e",
   "metadata": {},
   "source": [
    "- I need to one hot encode the agent id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc26ea9-360b-489f-aec7-4fe2555249cb",
   "metadata": {},
   "source": [
    "- there is another way ... always quote the current agent as 1\n",
    "\n",
    "- the states would be very similar but the reward function would change a lot .... how do we handle that : State Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d8f03-2893-40f2-8149-1d5f1ba55fd1",
   "metadata": {},
   "source": [
    "# model arguments and model def."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "id": "3631e674-d526-483a-a01d-eab02a646c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embe = nn.Embedding(10, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "id": "4300fe80-6690-4811-aa0c-1a34b1c99cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4950, -0.3356,  1.8569],\n",
       "         [ 0.1044,  0.1453, -0.1328],\n",
       "         [-0.6046, -0.1475,  0.1430],\n",
       "         [ 0.0443,  1.2195,  0.6806]],\n",
       "\n",
       "        [[-0.6046, -0.1475,  0.1430],\n",
       "         [-0.1446,  1.0510, -1.0658],\n",
       "         [ 0.1044,  0.1453, -0.1328],\n",
       "         [-0.0913,  0.1200, -1.5220]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input_ = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "embedding(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "id": "3f88ce4d-7632-4f16-84ba-1bb2dc6fd3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1148,  2.2052,  0.3921,  0.6445, -0.0270,  1.6642,  0.8254,  0.2439,\n",
       "         -0.4210,  0.8453,  2.0378, -0.7272, -0.5376, -0.4040,  1.1793, -0.5006,\n",
       "          1.4405,  2.0716,  1.3809,  0.8354,  0.2485, -0.4491,  0.0299,  0.8506,\n",
       "         -0.6823, -0.9633, -1.2667, -2.0696,  0.3557, -0.6044, -0.8769]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 1471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embe(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491b8943-edaa-4b3f-a589-80ba6649ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'exp1_ddqn_'#os.path.basename(__file__)[: -len(\".py\")]\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    TB_log:bool = True\n",
    "    custom_env = True\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"MountainCar-v0\"#\"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 5000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4#0.001#0.001 #2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1#4\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.999 #0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.98 #0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 10#4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "\n",
    "    learning_starts: int = 25e3\n",
    "    tau: float = 0.005\n",
    "    buffer_size: int = 10000\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 256\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 8\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 10\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "\n",
    "#gae_lambda\": 0.98, \"ent_coef\": 0.01, \"vf_coef\": 0.5, \"max_grad_norm\": 0.5, \"batch_size\": 64, \"n_epochs\": 4\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,envs,ob_space=None):\n",
    "        if not ob_space:\n",
    "            ob_space = envs.single_observation_space.shape\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(ob_space).prod(), 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(ob_space).prod(), 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "    def predict(self, x, action=None):\n",
    "        return get_action_and_value(self, x, action=None)[0]\n",
    "\n",
    "\n",
    "\n",
    "class Agent_shared(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(64, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "    def predict(self, x, action=None):\n",
    "        return get_action_and_value(self, x, action=None)[0]\n",
    "\n",
    "\n",
    "\n",
    "class Agent_shared_v1(nn.Module):\n",
    "    def __init__(self, action_space = None, ob_space=None):\n",
    "        super(Agent_shared_v1, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 64)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.actor = layer_init(nn.Linear(64, action_space), std=0.01)\n",
    "        self.critic = layer_init(nn.Linear(64, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    def predict(self, x, action=None):\n",
    "        return get_action_and_value(self, x, action=None)[0]\n",
    "    def get_valid_action(self,x):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        return probs.probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent_shared_v2(nn.Module):\n",
    "    def __init__(self, action_space = None, ob_space=None):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, action_space), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "    def predict(self, x, action=None):\n",
    "        return get_action_and_value(self, x, action=None)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent_shared_v1_risk(nn.Module):\n",
    "    def __init__(self, action_space = None, ob_space=None):\n",
    "        super(Agent_shared_v1_risk, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 64)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.actor_1 = layer_init(nn.Linear(64, action_space), std=0.01)\n",
    "        self.actor_2 = nn.Sequential(layer_init(nn.Linear(64, 2), std=0.01), nn.Softmax(dim=1),)\n",
    "        \n",
    "        self.critic = layer_init(nn.Linear(64, 1), std=1)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None,training=False):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor_1(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        \n",
    "        if action is None:\n",
    "            action_1 = probs.sample()[:,None]\n",
    "            action_2 = self.actor_2(hidden)[:,[0]]\n",
    "    \n",
    "            action = torch.concat((action_1,action_2),1)\n",
    "        else:\n",
    "            action_1,action_2 = action[:,0],action[:,1]\n",
    "\n",
    "        if (len(x)>1) or (training):\n",
    "            ret_p = probs.log_prob(action_1)*action_2\n",
    "        else:\n",
    "            ret_p = probs.log_prob(action_1[:,0])*action_2[:,0]\n",
    "\n",
    "        \n",
    "        \n",
    "        return action, ret_p, probs.entropy(), self.critic(hidden)\n",
    "\n",
    "    def predict(self, x, action=None):\n",
    "        return get_action_and_value(self, x, action=None)[0]\n",
    "    def get_valid_action(self,x):\n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        \n",
    "        action_2 = self.actor_2(hidden)[:,[0]]\n",
    "        \n",
    "        return probs.probs,action_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, action_space = None, ob_space=None):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space+action_space, 256)),\n",
    "            nn.GELU(),\n",
    "            #layer_init(nn.Linear(256, 256)),\n",
    "            #nn.GELU(),\n",
    "            layer_init(nn.Linear(256, 64)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(64, 1)),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        #self.actor_1 = layer_init(nn.Linear(64, action_space), std=0.01)\n",
    "        #self.actor_2 = nn.Sequential(layer_init(nn.Linear(64, 2), std=0.01), nn.Softmax(dim=1),)\n",
    "    def forward(self, x,a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        val = self.network(x)\n",
    "        #hidden = self.network(x)\n",
    "        #logits = self.actor_1(hidden)\n",
    "        #probs = Categorical(logits=logits)\n",
    "\n",
    "        #action_1 = probs.sample()[:,None]\n",
    "        #action_2 = self.actor_2(hidden)[:,[0]]\n",
    "\n",
    "        #action = torch.concat((action_1,action_2),1)\n",
    "\n",
    "        return val #action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Actor_ddqn(nn.Module):\n",
    "    def __init__(self,env, action_space = None, ob_space=None):\n",
    "        super().__init__()\n",
    "        #self.fc1 = nn.Linear(ob_space, 256)\n",
    "        #self.fc2 = nn.Linear(256, 256)\n",
    "        #self.fc_mu = nn.Linear(256, action_space)\n",
    "\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 256)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(256, 64)),\n",
    "            nn.GELU(),\n",
    "            #layer_init(nn.Linear(64, 64)),\n",
    "            #nn.GELU(),\n",
    "        )\n",
    "        self.actor_1 = layer_init(nn.Linear(64, action_space), std=0.01)\n",
    "        self.actor_2 = nn.Sequential(layer_init(nn.Linear(64, 2), std=0.01), nn.Softmax(dim=1),)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space(1).high - env.action_space(1).low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space(1).high + env.action_space(1).low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor_1(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        action_1 = probs.sample()[:,None]\n",
    "        action_2 = self.actor_2(hidden)[:,[0]]\n",
    "\n",
    "        action = torch.concat((action_1,action_2),1)\n",
    "\n",
    "        return action#* self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#    def forward(self, x):\n",
    "#        return self.network(x)\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6887c025-f68b-4bdd-b17e-db1416710462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent_shared_v1_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae96bd-b838-4469-a20d-e527cdf9d50b",
   "metadata": {},
   "source": [
    "# env model class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e3de9d-5b77-4d7d-94d2-b92bbda62b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(env_name,max_episode_steps=200)\n",
    "\n",
    "env = gym.make('MountainCar-v0',max_episode_steps=2000)\n",
    "\n",
    "class model_class_2(type(env)):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_list = np.zeros((3,2))\n",
    "        self.observation_space = gym.spaces.Box(low=-10 ,high=20, shape=(6,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            truncated = True\n",
    "        self.state_list[:2,:] = self.state_list[1:,:]\n",
    "        self.state_list[2,:] = observation\n",
    "        info['observation'] = observation.astype(np.float32)\n",
    "\n",
    "        return self.state_list.reshape(-1), reward, terminated, truncated, info\n",
    "        \n",
    "    \n",
    "    def reset(\n",
    "            self,\n",
    "            *,\n",
    "            seed: Optional[int] = None,\n",
    "            options: Optional[dict] = None,\n",
    "            default = False,\n",
    "            \n",
    "            init_state=[-5,0]\n",
    "                    ):\n",
    "            super().reset(seed=seed)\n",
    "            # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "            # state/observations.\n",
    "            if default:\n",
    "                low, high = utils_gym.maybe_parse_reset_bounds(options, -0.6, -0.4)\n",
    "                self.state = np.array([self.np_random.uniform(low=low, high=high), 0])\n",
    "            else:\n",
    "                self.state = np.array(init_state,dtype='float32')\n",
    "    \n",
    "            if self.render_mode == \"human\":\n",
    "                self.render()\n",
    "\n",
    "            self.state_list = np.array([self.state],dtype=np.float32).repeat(3,axis =0)\n",
    "            \n",
    "            return self.state_list.reshape(-1), {'observation':self.state}\n",
    "        \n",
    "\n",
    "\n",
    "def make_env(env_id, idx, capture_video, run_name,custom_env=False,max_episode_steps=2000):\n",
    "    def thunk():\n",
    "\n",
    "                    \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "                \n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "\n",
    "            env = gym.make(env_id)\n",
    "\n",
    "            \n",
    "            #env = gym.make(env_id)\n",
    "        if custom_env:\n",
    "            #print('here')\n",
    "            env = model_class_2(env=env,max_episode_steps=max_episode_steps)\n",
    "        \n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e28d7e-52a8-4e4e-842c-6e8449dd850c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae9a766-0e71-4501-83f1-9bbb32f4ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_main():\n",
    "    args = Args()#tyro.cli(Args)\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    TB_log = args.TB_log\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    if TB_log:    \n",
    "        writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "        writer.add_text(\n",
    "            \"hyperparameters\",\n",
    "            \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "        )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, i, args.capture_video, run_name,custom_env=args.custom_env) for i in range(args.num_envs)],\n",
    "    )\n",
    "\n",
    "    \n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "    ob_space_shape = envs.single_observation_space.shape#(3, 2)\n",
    "    #ob_space = gym.spaces.Box(low=-10 ,high=20, shape=(3, 2), dtype=np.float32)\n",
    "    #envs.single_observation_space = ob_space\n",
    "    agent = Agent(envs,ob_space=None).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # ALGO Logic: Storage setup\n",
    "    \n",
    "    \n",
    "    \n",
    "    #change needed here\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + ob_space_shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=args.seed)\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    for iteration in range(1, args.num_iterations + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                #print(next_obs.shape)\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        if global_step%100 ==0:\n",
    "                            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                        if TB_log:  \n",
    "                            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if TB_log:  \n",
    "            writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "            writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "            writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "            writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "            writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        if global_step%100 ==0:\n",
    "            print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        if TB_log:  \n",
    "            writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1e8ab-1073-42ef-92cd-6e43413165d7",
   "metadata": {},
   "source": [
    "# Risk env def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e530149b-715c-4452-96bf-7118f183d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\gymnasium\\spaces\\box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "\n",
    "class agent_(np.int8):\n",
    "    def bucket_set(self,val=10):\n",
    "        self.bucket = val\n",
    "    def add(self,val=0):\n",
    "        self.bucket+=val\n",
    "        return self\n",
    "    def sub(self,val=0):\n",
    "        self.bucket-=val\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self,default_attack_all = True,render_= False,agent_count = 3,env_=None,use_placement_perc = False,verbose = False):\n",
    "        # internally self.board.squares holds a flat representation of tic tac toe board\n",
    "        # where an empty board is [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        # where indexes are column wise order\n",
    "        # 0 3 6\n",
    "        # 1 4 7\n",
    "        # 2 5 8\n",
    "\n",
    "        # empty -- 0\n",
    "        # player 0 -- 1\n",
    "        # player 1 -- 2\n",
    "        self.verbose = verbose\n",
    "        self.env_ = env_\n",
    "        self.agents = [agent_(0)] + [ agent_(i+1) for i in range(agent_count)]\n",
    "        self.default_attack_all = default_attack_all\n",
    "        self.render_ = render_\n",
    "        self.unoccupied = agent_(0)\n",
    "        self.unoccupied.bucket_set(0)\n",
    "        self.reset_board()\n",
    "        self.territory_count = len(self.territories)\n",
    "        self.edge_count = len(self.edges)\n",
    "        self.attack_dist_higher = NormalDist(mu=0, sigma=1)\n",
    "        self.attack_dist_lower = NormalDist(mu=0, sigma=0.1)\n",
    "        self.max_trials = 4\n",
    "        self.max_bad_trials =4\n",
    "        self.base_action_mask =  np.zeros(self.territory_count+self.edge_count +2, dtype='int8')\n",
    "\n",
    "        self.use_placement_perc = use_placement_perc\n",
    "        #send = new_troop_count//(1/action[1])\n",
    "        \n",
    "        \n",
    "        #[0,1,2,3,4,5,6,7,8,9]\n",
    "        self.territory_map = [[0,0],[0,1],\n",
    "         [0,5],[0,6],\n",
    "         [2,6],[2,5],[2,4],                \n",
    "         [2,2],[2,1],[2,0]]\n",
    "        \n",
    "        \n",
    "        self.colors_territory = [(255,255,0),(128,255,0),(0,128,255),(128,0,255),\n",
    "                            (255,0,0),(255,128,0),(0,255,255),(0,0,255),\n",
    "                            (255,0,255),(255,0,128)]\n",
    "        \n",
    "    \n",
    "    def reset_board(self):\n",
    "        self.continents  = np.empty(4, object)\n",
    "        self.continents[:] = [[0,1],[2,3],[4,5,6],[7,8,9]]\n",
    "        \n",
    "        self.territories = np.zeros((10,2),dtype = object)\n",
    "\n",
    "\n",
    "        #0  1   --   2  3\n",
    "        #   |        |\n",
    "        # 9 8 7 -- 6 5 4  \n",
    "\n",
    "        self.territories[:,0] = self.unoccupied\n",
    "        \n",
    "        self.edges  = np.array([[0, 1],\n",
    "                                 [1, 0],\n",
    "                                 [1, 2],\n",
    "                                 [1, 8],\n",
    "                                 [2, 1],\n",
    "                                 [2, 5],\n",
    "                                 [2, 3],\n",
    "                                 [3, 2],\n",
    "                                 [4, 5],\n",
    "                                 [5, 6],\n",
    "                                 [5, 2],\n",
    "                                 [5, 4],\n",
    "                                 [6, 7],\n",
    "                                 [6, 5],\n",
    "                                 [7, 8],\n",
    "                                 [7, 6],\n",
    "                                 [8, 9],\n",
    "                                 [8, 1],\n",
    "                                 [8, 7],\n",
    "                                 [9, 8]],dtype='int8')\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.action_list = np.zeros((len(self.territories) + len(self.edges),2))\n",
    "\n",
    "        self.current_agent = self.agents[1]\n",
    "        self.agent_counter = 0\n",
    "        self.phase = 0\n",
    "        self.old_phase = -1\n",
    "        self.bad_trials = 0\n",
    "\n",
    "        #[i.bucket_set(int(i)*10) for i in self.agents]\n",
    "        [i.bucket_set(3) for i in self.agents]\n",
    "\n",
    "        self.territory_changes = np.zeros(len(self.agents))\n",
    "        self.set_cycle()\n",
    "        self.reset_screen()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def reset_screen(self):\n",
    "        if self.render_:\n",
    "            \n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((7 * 125, 3* 125))\n",
    "                \n",
    "    def calculated_action_mask(self, agent_sel,phase = None):\n",
    "        if type(phase) == type(None):\n",
    "            phase = self.phase\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        if phase == 0:\n",
    "            \n",
    "            action_mask =  np.array([1 if i in[agent_sel,0] else 0  for i in self.territories[:,0]]+[0]*self.edge_count +[1,1], dtype='int8')\n",
    "        elif phase ==1:\n",
    "            action_mask =  np.array([0]*self.territory_count+[ 1 if ((agent_sel == self.territories[i[0],0]) and (self.territories[i[0],1]>1)) and (agent_sel != self.territories[i[1],0]) else 0\n",
    "                                                                  for i in self.edges] +[1,1], dtype='int8')\n",
    "\n",
    "        else:\n",
    "            action_mask =  np.array([0]*self.territory_count+[ 1 if ((agent_sel == self.territories[i[0],0] == self.territories[i[1],0]) and (self.territories[i[0],1]>1)) else 0\n",
    "                                                                  for i in self.edges] +[0,1], dtype='int8')\n",
    "\n",
    "        \n",
    "\n",
    "        return action_mask\n",
    "    \n",
    "    def take_action(self, agent_sel,action,phase = None,mask = None):\n",
    "        # if spot is empty\n",
    "        #action[0] is action_index\n",
    "            #phase 1 : action[1] # of new troops to be added\n",
    "            #phase 2 : action[1] % maximum number of troops to be sent forward if attack succeeds. #this is kept to 1 by default.\n",
    "\n",
    "        self.reset_reward()\n",
    "        \n",
    "        if type(phase) == type(None):\n",
    "            phase = self.phase\n",
    "\n",
    "        action[0] = np.int8(action[0])\n",
    "        \n",
    "        legal,end_turn = self.check_legal(agent_sel,action,phase,mask)\n",
    "        if legal and not(end_turn):\n",
    "\n",
    "\n",
    "            if phase ==0:\n",
    "\n",
    "                if self.use_placement_perc:\n",
    "                    \n",
    "                    self.placement(agent_sel,action,convert_percent = self.use_placement_perc,troops=self.agents[agent_sel].bucket)\n",
    "                else:\n",
    "                    self.placement(agent_sel,action,convert_percent = self.use_placement_perc)\n",
    "            \n",
    "            elif phase == 1:\n",
    "                self.attack(agent_sel,action)\n",
    "            else:\n",
    "                self.fortify(agent_sel,action)\n",
    "\n",
    "                end_turn = 2\n",
    "                \n",
    "\n",
    "        \n",
    "        if self.render_:\n",
    "            self.render()\n",
    "\n",
    "        self.ddnt_get_a_chance_to_place()\n",
    "\n",
    "        return legal,end_turn, self.territory_changes\n",
    "        \n",
    "    def check_legal(self, agent_sel,action,phase = None,mask = None):\n",
    "\n",
    "        agent = self.agents[agent_sel]\n",
    "        \n",
    "        if type(phase) == type(None):\n",
    "            phase = self.phase\n",
    "\n",
    "        \n",
    "        if mask == None:\n",
    "            mask = self.calculated_action_mask( agent_sel,phase)\n",
    "\n",
    "        if mask[action[0]] ==0:\n",
    "            if self.verbose :\n",
    "                print(1)\n",
    "            self.bad_trials +=1\n",
    "\n",
    "            _,end_=self.check_bad_trials(base_end = 0)\n",
    "            \n",
    "            return False, end_ \n",
    "        # only changing unoccipied or self occupied territory and within bounds\n",
    "        # for phase 0 : action only adds new troops to territory\n",
    "\n",
    "        if action[0] == 31:\n",
    "            return True,2\n",
    "        elif action[0] == 30:\n",
    "            return True,1\n",
    "        else:\n",
    "            if phase ==0:\n",
    "\n",
    "                if self.use_placement_perc:\n",
    "                    if (action[1] >1) or (action[1] <0):\n",
    "                        if self.verbose :\n",
    "                            print(2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                    if agent.bucket == 0:\n",
    "                        if self.verbose :\n",
    "                            print(2.1)\n",
    "                        self.bad_trials +=1\n",
    "                        return False, 1\n",
    "                    if (agent.bucket)*action[1] <1:\n",
    "                        if self.verbose :\n",
    "                            print(2.2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                \n",
    "                else:    \n",
    "                \n",
    "                    if action[1] <1:\n",
    "                        if self.verbose :\n",
    "                            print(2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                    if agent.bucket == 0:\n",
    "                        if self.verbose :\n",
    "                            print(2.1)\n",
    "                        self.bad_trials +=1\n",
    "                        return False, 1\n",
    "                    if agent.bucket < action[1]:\n",
    "                        if self.verbose :\n",
    "                            print(2.2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                    \n",
    "            elif phase ==3:\n",
    "                if self.use_placement_perc:\n",
    "                    if (action[1] >1) or (action[1] <0):\n",
    "                        if self.verbose :\n",
    "                            print(2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "\n",
    "                    edge = self.edges[action[0]-self.territory_count]\n",
    "                    territory_a = self.territories[edge[0]]\n",
    "                    \n",
    "                    if (territory_a)*action[1] <1:\n",
    "                        if self.verbose :\n",
    "                            print(2.2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                \n",
    "                else:    \n",
    "                \n",
    "                    if action[1] <1:\n",
    "                        if self.verbose :\n",
    "                            print(2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                    edge = self.edges[action[0]-self.territory_count]\n",
    "                    territory_a = self.territories[edge[0]]\n",
    "                    \n",
    "                    if territory_a < action[1]:\n",
    "                        if self.verbose :\n",
    "                            print(2.2)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_                \n",
    "            \n",
    "            \n",
    "            \n",
    "            else:\n",
    "                #phase 1 what percentage of troops to move forward. >=1\n",
    "                if not(self.default_attack_all):\n",
    "                    if action[1] >1:\n",
    "                        if self.verbose :\n",
    "                            print(3)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "                    if self.territories[\n",
    "                                    self.edges[action[0]-self.territory_count][0]\n",
    "                                    ][1]//(1/action[1]) <1: #assuming nobody dies\n",
    "                        if self.verbose :\n",
    "                            print(4)\n",
    "                        self.bad_trials +=1\n",
    "                        _,end_=self.check_bad_trials(base_end = 0)\n",
    "                        return False, end_\n",
    "    \n",
    "            if (self.current_agent == agent) :\n",
    "                if phase != self.old_phase:\n",
    "                    self.agent_counter =0 # meaning change in phase\n",
    "                    self.old_phase = phase\n",
    "                \n",
    "                elif (self.agent_counter >= self.max_trials) and (phase == self.phase):\n",
    "                    if self.verbose :\n",
    "                        print(4.2)\n",
    "                    \n",
    "                    if self.phase ==0:\n",
    "                        return False, 1\n",
    "                    elif self.phase ==1:\n",
    "                        return False, 1\n",
    "                    else:\n",
    "                        return False, 2\n",
    "                        \n",
    "                elif (self.agent_counter < self.max_trials) and (phase == self.phase):\n",
    "                    self.agent_counter +=1\n",
    "                    \n",
    "    \n",
    "                    #elif (phase == 2):\n",
    "                    #    self.agent_counter = self.max_trials\n",
    "                    #    return True, 2\n",
    "            \n",
    "            else:\n",
    "                self.set_phase(phase)\n",
    "                self.set_current_agent(agent)\n",
    "                self.old_phase = phase\n",
    "    \n",
    "        \n",
    "        return True,0\n",
    "            \n",
    "    def check_bad_trials(self,base_end = 0):\n",
    "        if self.bad_trials >= self.max_bad_trials:\n",
    "            if self.verbose :\n",
    "                print(0.1)\n",
    "            if self.phase ==0:\n",
    "                return False, 1\n",
    "            elif self.phase ==1:\n",
    "                return False, 1\n",
    "            else:\n",
    "                return False, 2\n",
    "                \n",
    "        return False, base_end \n",
    "        \n",
    "\n",
    "    def placement(self,agent_sel,action,convert_percent = False,troops=[],attack=False): #action = territory index, changein troops\n",
    "        territory = self.territories[action[0]]\n",
    "\n",
    "        if convert_percent:\n",
    "            \n",
    "            act_ = self.convert_percent_to_troops(troops,action[1],ceil=False)\n",
    "        else:\n",
    "            act_ = action[1]\n",
    "        \n",
    "        if self.verbose :\n",
    "            print('curr_agent_board',self.current_agent,agent_sel,self.agents[agent_sel], 'act',act_ , 'target',action[0],self.territories[action[0]]    )\n",
    "        if not(attack) and agent_sel !=self.unoccupied:\n",
    "            self.agents[agent_sel].sub(act_)\n",
    "        \n",
    "        #print(self.agents[agent_sel],agent_sel)\n",
    "        if self.territories[action[0]][0] ==0:\n",
    "            self.territory_changes[agent_sel] += 1\n",
    "        \n",
    "        self.territories[action[0]] = [self.agents[agent_sel],territory[1]+act_]\n",
    "\n",
    "    def fortify(self,agent_sel,action):\n",
    "\n",
    "\n",
    "        edge = self.edges[action[0]-self.territory_count]\n",
    "        \n",
    "        territory_a = self.territories[edge[0]]\n",
    "        territory_b = self.territories[edge[1]]\n",
    "\n",
    "        agent = self.agents[agent_sel]\n",
    "        \n",
    "        troop_count = territory_a[1]\n",
    "        \n",
    "        if self.use_placement_perc:\n",
    "            send = self.convert_percent_to_troops(troop_count,action[1],ceil=False,fortify=True)\n",
    "            #send = new_troop_count//(1/action[1])\n",
    "        else:\n",
    "            send = min(troop_count,action[1]) - 1 #send all troops but 1\n",
    "\n",
    "        \n",
    "        self.placement(agent,[edge[0],-send],convert_percent = False,attack=True)\n",
    "        self.placement(agent,[edge[1],send],convert_percent = False,attack=True)\n",
    "\n",
    "\n",
    "    def set_phase(self,phase=0):\n",
    "        self.old_phase = self.phase\n",
    "        self.phase = phase\n",
    "        self.bad_trials =0\n",
    "        self.agent_counter =0\n",
    "        \n",
    "    def set_current_agent(self,agent):\n",
    "        self.current_agent = agent\n",
    "    def set_cycle(self,cycle=0):\n",
    "        self.cycle=cycle\n",
    "    def reset_reward(self):\n",
    "        self.territory_changes = np.zeros(len(self.agents))\n",
    "\n",
    "    def convert_percent_to_troops(self,troops,percent,ceil=False, fortify=False):\n",
    "\n",
    "        if ceil and (percent >0.9):\n",
    "            send = troops -1\n",
    "        else:\n",
    "            send = troops//(1/(percent+0.00000001))\n",
    "\n",
    "            if (send == troops) and (fortify) : # when trying to send all the troops during fortification... you have to leave atleast 1\n",
    "                send-=1\n",
    "        return send\n",
    "\n",
    "    \n",
    "    def attack(self,agent_sel,action): # this does not care about mask.... run check_legal before attacking\n",
    "        self.reset_reward()\n",
    "        eta = 0.0000001\n",
    "        #get edge\n",
    "        #get troops on a and troops on b\n",
    "        if self.verbose :\n",
    "            print(0)\n",
    "        edge = self.edges[action[0]-self.territory_count]\n",
    "        \n",
    "        territory_a = self.territories[edge[0]]\n",
    "        territory_b = self.territories[edge[1]]\n",
    "        \n",
    "        agent_a = self.agents[territory_a[0]] \n",
    "        agent_b = self.agents[territory_b[0]] \n",
    "        \n",
    "        # simplify - not roll, but just check if random is less than win_chance\n",
    "        diff = (territory_a[1]- territory_b[1])\n",
    "\n",
    "        win_chance = self.attack_dist_higher.cdf( diff/(territory_b[1]+eta)  ) if diff>0 else self.attack_dist_lower.cdf( diff/(territory_b[1] +eta) )\n",
    "        if self.verbose :\n",
    "            print(win_chance)\n",
    "\n",
    "        if win_chance > np.random.rand():\n",
    "            #opponent_territory becomes unoccupied, and troops become zero\n",
    "            #winner looses same number of troops as opponent , self troops -1 , whichever is lesser\n",
    "            \n",
    "\n",
    "            #calculate total troops left , troops to retain at pos a and troops to send at pos b \n",
    "            loss = min(territory_b[1],territory_a[1]-1)\n",
    "            new_troop_count = territory_a[1] - loss\n",
    "\n",
    "            #print(loss,new_troop_count)\n",
    "            \n",
    "            \n",
    "            send = 0\n",
    "            \n",
    "            if new_troop_count>1:\n",
    "                if not(self.default_attack_all):\n",
    "                    send = self.convert_percent_to_troops(new_troop_count,action[1],ceil=False)\n",
    "                    #send = new_troop_count//(1/action[1])\n",
    "                else:\n",
    "                    send = new_troop_count - 1 #send all troops but 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            if (send>0): # if we want to send then send, else just vacate the territory\n",
    "                self.placement(agent_a,[edge[0],-loss-send],convert_percent = False,attack=True) #-\n",
    "\n",
    "                if self.territories[edge[1]][0] !=0: #well it would be doublecounting othervise\n",
    "                    self.territory_changes[agent_a] += 1\n",
    "                    \n",
    "                self.placement(agent_a,[edge[1],send],convert_percent = False,attack=True) #loose equal troops always\n",
    "\n",
    "\n",
    "                \n",
    "                self.territory_changes[agent_b] -= 1\n",
    "            else:\n",
    "                self.placement(agent_a,[edge[0],-loss],convert_percent = False,attack=True) #-\n",
    "                self.placement(self.unoccupied,[edge[1],-territory_b[1]],convert_percent = False,attack=True) #-\n",
    "                self.territory_changes[agent_b] -= 1\n",
    "                \n",
    "            if (agent_b != 0):\n",
    "                if self.check_territory_count_N0(agent_b ):\n",
    "                    if self.verbose :\n",
    "                        print('here----')\n",
    "                    if type(self.env_) != type(None):\n",
    "                        self.env_.terminations[agent_b] =True\n",
    "                        if self.verbose :\n",
    "                            print('termination --',self.env_.terminations)\n",
    "                        self.territory_changes[agent_a] += 1\n",
    "                        self.territory_changes[agent_b] -= 100#50\n",
    "        \n",
    "        else:\n",
    "            loss = min(territory_b[1]-1,territory_a[1]-1)\n",
    "            self.placement(agent_a,[edge[0],-loss],convert_percent = False,attack=True) #-\n",
    "            self.placement(agent_b,[edge[1],-loss],convert_percent = False,attack=True) #-\n",
    "\n",
    "    def check_territory_count_N0(self,agent):\n",
    "        k = sum(self.territories[:,0] == agent)\n",
    "        if self.verbose :\n",
    "            print('die?',agent,k)\n",
    "        return (k ==0)\n",
    "\n",
    "    def ddnt_get_a_chance_to_place(self):\n",
    "        if 0 not in self.territories[:,0]:\n",
    "            for i in self.env_.possible_agents:\n",
    "                if not (self.env_.terminations[i]):\n",
    "                    if i not in self.territories[:,0]:\n",
    "                        self.env_.terminations[i] =True\n",
    "                        if self.verbose :\n",
    "                            print('termination --2',self.env_.terminations)\n",
    "                        self.territory_changes[i] -= 100#50\n",
    "                        \n",
    "        \n",
    "    def game_status(self):\n",
    "        wins = np.array( np.unique(self.territories[:,0], return_counts=True)).T\n",
    "        #all territory belong to a player\n",
    "        #only one player lives and others dont have anything, its phase 1 and not the 1st cycle\n",
    "        if len(wins)==1:\n",
    "            status = wins[0][0]\n",
    "        elif len(wins)==2 and self.cycle !=0 and (0 in wins[:,0]):\n",
    "            status = wins[:,0][wins[:,0] !=0][0]\n",
    "        else:\n",
    "            status = 0\n",
    "        return (status),wins\n",
    "\n",
    "    # returns:\n",
    "    # -1 for no winner\n",
    "    # 1 -- agent 0 wins\n",
    "    # 2 -- agent 1 wins\n",
    "    def check_for_winner(self):\n",
    "        pass\n",
    "\n",
    "    def check_game_over(self,agent):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return (self.territories)\n",
    "\n",
    "\n",
    "    def render(self,last_action_txt=''):\n",
    "        #pygame.time.wait(500)\n",
    "        self.screen.fill((255, 255, 255))  \n",
    "        width = 125\n",
    "        number_font = pygame.font.SysFont( None, 40 ) \n",
    "        number_font_2 = pygame.font.SysFont( None, 20 ) \n",
    "        for i,j in zip(self.territories,self.territory_map):\n",
    "            cell_left = j[1]*width\n",
    "            cell_top =  j[0]*width\n",
    "            pygame.draw.rect(self.screen, self.colors_territory[i[0]], (cell_left, cell_top, width, width))\n",
    "            \n",
    "            # make the number from grid[row][col] into an image\n",
    "            if i[0]:\n",
    "                number_text = 'P'+str(i[0])+'-'+str(i[1])\n",
    "            else:\n",
    "                number_text = 'PFree'+'-'+str(i[1])\n",
    "            \n",
    "            #number_text  = f'P{ (i[0] if i[0] else 'Free') }-{i[1]}'\n",
    "            number_image = number_font.render( number_text, True, (0,0,0), (255,255,255) )\n",
    "        \n",
    "            # centre the image in the cell by calculating the margin-distance\n",
    "            # Note: there is no \"height\", cells are square (w by w)\n",
    "            margin_x = ( width-1 - number_image.get_width() ) // 2\n",
    "            margin_y = ( width-1 - number_image.get_height() ) // 2\n",
    "        \n",
    "            # Draw the number image\n",
    "            self.screen.blit( number_image, ( cell_left+2 + margin_x, cell_top+2 + margin_y ) )\n",
    "\n",
    "        # make the number from grid[row][col] into an image\n",
    "        cell_left = 3*width\n",
    "        cell_top =  1*width\n",
    "        \n",
    "        number_text  = \"|\".join([f'P{i}-: bucket {i.bucket}, T:{sum(self.territories[:,0]==i)} ' for i in self.agents[1:]])\n",
    "        number_image = number_font_2.render( number_text, True, (0,0,0), (255,255,255) )\n",
    "    \n",
    "        # centre the image in the cell by calculating the margin-distance\n",
    "        # Note: there is no \"height\", cells are square (w by w)\n",
    "        margin_x = ( width-1 - number_image.get_width() ) // 2\n",
    "        margin_y = ( width-1 - number_image.get_height() ) // 2\n",
    "\n",
    "        # Draw the number image\n",
    "        self.screen.blit( number_image, ( cell_left+2 + margin_x, cell_top+2 + margin_y ) )\n",
    "\n",
    "        # make the number from grid[row][col] into an image\n",
    "        cell_left = 3*width\n",
    "        cell_top =  0.8*width\n",
    "        \n",
    "        number_text  = f'Phase :{self.phase}, cur_Player :{self.current_agent}, turn count:{self.agent_counter}, bad turns:{self.bad_trials},act{last_action_txt} '\n",
    "        number_image = number_font_2.render( number_text, True, (0,0,0), (255,255,255) )\n",
    "    \n",
    "        # centre the image in the cell by calculating the margin-distance\n",
    "        # Note: there is no \"height\", cells are square (w by w)\n",
    "        margin_x = ( width-1 - number_image.get_width() ) // 2\n",
    "        margin_y = ( width-1 - number_image.get_height() ) // 2\n",
    "\n",
    "        # Draw the number image\n",
    "        self.screen.blit( number_image, ( cell_left+2 + margin_x, cell_top+2 + margin_y ) )\n",
    "            \n",
    "        \n",
    "        pygame.draw.line(self.screen,(0, 0, 0),[2*width,0.5*width],[5*width,0.5*width],1)\n",
    "        pygame.draw.line(self.screen,(0, 0, 0),[1.5*width,1*width],[1.5*width,2*width],1)\n",
    "        pygame.draw.line(self.screen,(0, 0, 0),[5.5*width,1*width],[5.5*width,2*width],1)\n",
    "        pygame.draw.line(self.screen,(0, 0, 0),[3*width,2.5*width],[4*width,2.5*width],1)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        pygame.display.update() \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class raw_env(AECEnv, EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"name\": \"risk_tiny_v1\",\n",
    "        \"is_parallelizable\": False,\n",
    "        \"render_fps\": 1,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, render_mode: str | None = None, screen_height: int | None = 400, default_attack_all : bool| None = False,\n",
    "        render_:bool|None= False,agent_count : int|None = 3,use_placement_perc : bool|None = False, add_onturn : int | None =1\n",
    "    , verbose:bool|None=False , bad_mov_penalization=0.1):\n",
    "        super().__init__()\n",
    "        EzPickle.__init__(self, render_mode, screen_height)\n",
    "        self.verbose = verbose\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_height = screen_height\n",
    "        self.screen = None\n",
    "        self.phases = [0,1,2]\n",
    "        self.use_placement_perc = use_placement_perc\n",
    "        self.add_onturn = add_onturn\n",
    "        self.last_action_txt =''\n",
    "        self.bad_mov_penalization = bad_mov_penalization\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        \n",
    "        \n",
    "        self.board = Board(default_attack_all = default_attack_all, render_=render_,agent_count = agent_count,env_=self,use_placement_perc=use_placement_perc,\n",
    "                          verbose = self.verbose)\n",
    "\n",
    "        self.agents = self.board.agents[1:] #0th is unoccupied\n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.last_agent = self.agents[0]\n",
    "\n",
    "\n",
    "        if self.use_placement_perc:\n",
    "            self.action_spaces = {j: {i: ( Box(low=np.array([0,0.1]), \n",
    "                                                 high=np.array([self.board.territory_count+ self.board.edge_count+2,\n",
    "                                                                         1]), \n",
    "                                     shape=(2,), dtype=(np.float16)       \n",
    "                                                            )\n",
    "                                      if j == 0 \n",
    "                                      else Box(low=np.array([self.board.territory_count,0]), \n",
    "                                                 high=np.array([self.board.territory_count + self.board.edge_count+2,\n",
    "                                                                         1]), \n",
    "                                     shape=(2,), dtype=(np.float16)       \n",
    "                                                            )\n",
    "                                     )\n",
    "                                      \n",
    "                                      \n",
    "                                      \n",
    "                                      for i in self.agents}\n",
    "                                for j in self.phases\n",
    "                             }\n",
    "\n",
    "        else:\n",
    "        \n",
    "            self.action_spaces = {j: {i: ( Box(low=np.array([0,0]), \n",
    "                                                 high=np.array([self.board.territory_count+ self.board.edge_count+2,\n",
    "                                                                         10]), \n",
    "                                     shape=(2,), dtype=(np.int8)       \n",
    "                                                            )\n",
    "                                      if j == 0 \n",
    "                                      else \n",
    "\n",
    "                                          ( Box(low=np.array([self.board.territory_count,0]), \n",
    "                                                 high=np.array([self.board.territory_count + self.board.edge_count+2,\n",
    "                                                                         10]), shape=(2,), dtype=(np.int8)  )\n",
    "\n",
    "                                          if j ==1\n",
    "                                          \n",
    "                                           else \n",
    "                                          \n",
    "                                          Box(low=np.array([self.board.territory_count,0]), \n",
    "                                                 high=np.array([self.board.territory_count + self.board.edge_count+2,\n",
    "                                                                         1]), shape=(2,), dtype=(np.float16)    )\n",
    "                                     ))\n",
    "                                      \n",
    "                                      \n",
    "                                      \n",
    "                                      for i in self.agents }\n",
    "                                for j in self.phases\n",
    "                             }\n",
    "\n",
    "\n",
    "                                \n",
    "        \n",
    "        self.observation_spaces = {\n",
    "            i: Dict(\n",
    "                {\n",
    "                    \"observation\":  Box(low=np.zeros(self.board.territories.shape), \n",
    "                                    high=np.ones(self.board.territories.shape)*np.array([agent_count,100]), \n",
    "                                    shape=self.board.territories.shape, dtype=agent_\n",
    "                    )\n",
    "                                \n",
    "                    \n",
    "                    \n",
    "                    ,\n",
    "                    \"action_mask\": Box(low=0, high=1, shape=(self.board.territory_count + self.board.edge_count +2,), dtype=np.int8),\n",
    "                }\n",
    "                \n",
    "            )\n",
    "            for i in self.agents\n",
    "        }\n",
    "\n",
    "        self.rewards = {i: 0 for i in self.agents}\n",
    "        self.terminations = {i: False for i in self.agents}\n",
    "        self.truncations = {i: False for i in self.agents}\n",
    "        #self.infos = {str(i): {} for i in self.agents}\n",
    "        self.infos = {i: {} for i in self.agents}\n",
    "\n",
    "        self.cycle = 0\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        self._phase_selector = agent_selector(self.phases)\n",
    "\n",
    "\n",
    "        self.handle_change(next_cycle=0,reset_cycle=1,\n",
    "                      next_agent=0,reset_agent=1,\n",
    "                      next_phase=0,reset_phase=1)\n",
    "        \n",
    "\n",
    "    def observe(self, agent):\n",
    "        \n",
    "        #observation is the same for everyone atm\n",
    "        observation = np.array(self.board.territories,dtype=np.int8)\n",
    "\n",
    "        action_mask = np.array(self._get_mask(agent),dtype=np.int8)\n",
    "\n",
    "        return {\"observation\": observation, \"action_mask\": action_mask}\n",
    "\n",
    "    def _get_mask(self, agent):\n",
    "        \n",
    "        action_mask = np.zeros(self.board.territory_count + self.board.edge_count, dtype=np.int8)\n",
    "\n",
    "        # Per the documentation, the mask of any agent other than the\n",
    "        # currently selected one is all zeros.\n",
    "        if agent == self.agent_selection:\n",
    "            action_mask = self.board.calculated_action_mask(agent,phase = self.phase_selection)\n",
    "\n",
    "        return action_mask\n",
    "\n",
    "    #def _get_obs(self):\n",
    "    #    return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    #def _get_info(self):\n",
    "    #    return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        \n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[self.phase_selection][agent]\n",
    "\n",
    "\n",
    "    def reward_function(self,legal=True,end=0,territory_changes=[0,0,0,0],status=0,wins_=[[0,0],[0,0],[0,0],[0,0]]):\n",
    "\n",
    "        if not(legal) and end:\n",
    "            #print('subtracting')\n",
    "            self.rewards[self.agents[self.agent_selection-1]] -= self.bad_mov_penalization\n",
    "            self.curr_rewards[self.agents[self.agent_selection-1]] = -self.bad_mov_penalization\n",
    "        elif (status != 0) and (self.all_deployed(wins_)): #game ended \n",
    "            if self.verbose :\n",
    "                print('status',status)\n",
    "            winner = status  # either TTT_PLAYER1_WIN or TTT_PLAYER2_WIN\n",
    "            #loser = winner ^ 1  # 0 -> 1; 1 -> 0\n",
    "            if self.verbose :\n",
    "                print(winner)\n",
    "            self.rewards[self.agents[winner-1]] += 100 #high reward forces the model to not just aqcuire territories, but also win\n",
    "            self.curr_rewards[self.agents[winner-1]]=100\n",
    "            for i in self.agents:\n",
    "                try:\n",
    "                    if i != winner:\n",
    "                        self.rewards[self.agents[i-1]] -= 100\n",
    "                        self.curr_rewards[self.agents[i-1]]=-100\n",
    "                except Exception as e:\n",
    "                    print(i,self.agents,self.rewards,winner)\n",
    "                    raise Exception(e)  \n",
    "\n",
    "            # once either play wins or there is a draw, game over, both players are done\n",
    "            self.terminations = {i: True for i in self.agents}\n",
    "            self._accumulate_rewards()\n",
    "        else:\n",
    "            for i,j in enumerate(territory_changes[1:]):\n",
    "                \n",
    "                #self.rewards[self.agents[i]]+=j\n",
    "                self.curr_rewards[self.agents[i]]=j\n",
    "                self.rewards[self.agents[i]]+=j\n",
    "                \n",
    "\n",
    "        #this is wrong ... only killed by some one elses actions\n",
    "        #if (self.agent_selection not in wins_) and self.cycle>0:\n",
    "        #    self.terminations[agent] =True\n",
    "    #\n",
    "    def handle_change(self,next_cycle=0,reset_cycle=0,\n",
    "                      next_agent=0,reset_agent=0,\n",
    "                      next_phase=0,reset_phase=0):\n",
    "        if reset_cycle:\n",
    "            self.cycle =0\n",
    "            self.board.set_cycle( self.cycle)\n",
    "        if next_cycle:\n",
    "            self.cycle+=1\n",
    "            self.board.set_cycle( self.cycle)\n",
    "            \n",
    "        if next_agent:\n",
    "            \n",
    "            if len(self._agent_selector.agent_order): # but this is not updating\n",
    "                if self.verbose :\n",
    "                    print('curr_agent_board',self.board.current_agent, 'curr_agent',self.agent_selection     )\n",
    "                \n",
    "                self.board.current_agent.add(self.add_onturn)\n",
    "                self.agent_selection = self._agent_selector.next()\n",
    "                while self.terminations[self.agent_selection] :\n",
    "                    self.agent_selection = self._agent_selector.next()\n",
    "                \n",
    "                self.board.set_current_agent(self.agent_selection)\n",
    "            \n",
    "        if reset_agent:\n",
    "            self.agent_selection = self._agent_selector.reset()\n",
    "            while self.terminations[self.agent_selection] :\n",
    "                    self.agent_selection = self._agent_selector.next()\n",
    "            self.board.set_current_agent(self.agent_selection)\n",
    "            \n",
    "        if next_phase:\n",
    "            self.phase_selection = self._phase_selector.next()\n",
    "            \n",
    "            self.board.set_phase(self.phase_selection)\n",
    "        if reset_phase:\n",
    "            self.phase_selection = self._phase_selector.reset()\n",
    "            self.board.set_phase(self.phase_selection)\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "         \n",
    "\n",
    "    # action in this case is a value from 0 to 8 indicating position to move on tictactoe board\n",
    "    def step(self, action):\n",
    "        #print()\n",
    "\n",
    "        self.last_action_txt = ''\n",
    "        if (\n",
    "            self.terminations[self.agent_selection] or ( self.agent_selection in self.kill_list ) ):\n",
    "            end = 3\n",
    "        elif(sum(list(self.terminations.values()) ) >=2):\n",
    "            end =4\n",
    "        else:\n",
    "            legal,end,territory_changes = self.board.take_action(self.agent_selection, action)\n",
    "\n",
    "            #if ends and illegal... give a negative rewards?\n",
    "            \n",
    "            status,wins_ = self.board.game_status()\n",
    "            self.reward_function(legal,end,territory_changes,status,wins_)\n",
    "            self.last_action_txt = f'p{self.agent_selection}, action{action}'\n",
    "  \n",
    "\n",
    "\n",
    "        if self.verbose :\n",
    "            print('live_list',self._agent_selector.agent_order,'phase',self._phase_selector.agent_order)\n",
    "        \n",
    "        \n",
    "        \n",
    "        not_removed = self.handle_post_cycle()#self.handle_terminations()\n",
    "        \n",
    "        \n",
    "        if end==1:\n",
    "            if self.verbose :\n",
    "                print('1_')\n",
    "            self.handle_change(next_cycle=0,reset_cycle=0,\n",
    "                              next_agent=0,reset_agent=0,\n",
    "                              next_phase=1,reset_phase=0)\n",
    "\n",
    "        elif end==2:\n",
    "            if self.verbose :\n",
    "                print('2_')\n",
    "            self.handle_change(next_cycle=1,reset_cycle=0,\n",
    "                              next_agent=not_removed,reset_agent=0,\n",
    "                              next_phase=0,reset_phase=1)\n",
    "        elif end == 3:\n",
    "            if self.verbose :\n",
    "                print('3_')\n",
    "            self.handle_change(next_cycle=0,reset_cycle=0,\n",
    "                              next_agent=1,reset_agent=0,\n",
    "                              next_phase=0,reset_phase=1)\n",
    "            self.last_action_txt = f'p{self.agent_selection}, action 3_'\n",
    "        elif end == 4:\n",
    "            if self.verbose :\n",
    "                print('3_')\n",
    "            self.last_action_txt = f' P {self.agent_selection} WON!'\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self.render(self.last_action_txt)\n",
    "\n",
    "        \n",
    "#######################\n",
    "\n",
    "        \n",
    "\n",
    "        #if self._deads_step_first():\n",
    "        #self._accumulate_rewards()\n",
    "        #else:\n",
    "        #    self._clear_rewards()\n",
    "        #    \n",
    "        #if self._agent_selector.agent_order:\n",
    "        #    self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "        #if self.env.frames >= self.env.max_cycles:\n",
    "        #    self.terminations = dict(zip(self.agents, [True for _ in self.agents]))\n",
    "\n",
    "        #self._cumulative_rewards[agent] = 0\n",
    "        #self._accumulate_rewards()\n",
    "        #self._deads_step_first()\n",
    "        #self.steps += 1\n",
    "##########################\n",
    "\n",
    "\n",
    "        #observation = self.observe( self.agent_selection)#self._get_obs()\n",
    "        #info = {observation['action_mask']}\n",
    "        #observation = observation['observation']\n",
    "\n",
    "        #return observation, reward, terminated, False, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def check_who_died(self):\n",
    "        status,wins_ = self.board.game_status()\n",
    "        if (self.all_deployed(wins_)):\n",
    "            self.kill_list = list(set(self.agents ) - set(wins_[:,0]))\n",
    "        else:\n",
    "            self.kill_list = []\n",
    "    \n",
    "    def handle_post_cycle(self):\n",
    "        not_removed = 1 \n",
    "        if self._agent_selector.is_last(): #last person 2_\n",
    "            self.check_who_died()\n",
    "            iter_agents = self.agents[:]\n",
    "            \n",
    "            for agent in self.kill_list:#self.terminations:\n",
    "                if ( agent in self._agent_selector.agent_order):\n",
    "                    #print(agent,self.kill_list)\n",
    "                    not_removed=0\n",
    "                    self.terminations[agent]=True #if in killed list add to termination\n",
    "                    iter_agents.remove(agent)\n",
    "                    #self.agents.remove(agent)      #remove from list of alive agents\n",
    "\n",
    "            if not(not_removed):\n",
    "                self.kill_list = []\n",
    "                \n",
    "                self._agent_selector.reinit(iter_agents)\n",
    "                #self._agent_selector.reinit(self.agents)  \n",
    "                if self.verbose :\n",
    "                    print('updated_order',self._agent_selector.agent_order)\n",
    "        \n",
    "        return 1#not_removed\n",
    "        \n",
    "    \n",
    "    def handle_terminations(self):\n",
    "        #is_last = self._agent_selector.is_last()\n",
    "        not_removed = 1 \n",
    "        \n",
    "        iter_agents = self.agents[:]\n",
    "        for agent in self.terminations:\n",
    "            if (self.terminations[agent] or self.truncations[agent]) and ( agent in self._agent_selector.agent_order) :\n",
    "                iter_agents.remove(agent)\n",
    "                not_removed=0\n",
    "                if self.verbose :\n",
    "                    print('removed',agent)\n",
    "        \n",
    "        self.last_agent = self.agent_selection\n",
    "        if not(not_removed):\n",
    "            self._agent_selector.reinit(self.agents)\n",
    "            if self.verbose :\n",
    "                print('removed something',self._agent_selector.agent_order)\n",
    "            \n",
    "        \n",
    "        return not_removed\n",
    "\n",
    "    def all_deployed(self,wins):\n",
    "        for i in self.agents:\n",
    "            if not(self.init_deployment[i]) and ( i in wins[:,0]):\n",
    "                self.init_deployment[i] =True\n",
    "\n",
    "        return sum(self.init_deployment.values()) ==len(self.agents)\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \n",
    "        self.board.reset_board()\n",
    "        self.curr_rewards = {i: 0 for i in self.agents}\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {i: 0 for i in self.agents}\n",
    "        self._cumulative_rewards = {i: 0 for i in self.agents}\n",
    "        self.terminations = {i: False for i in self.agents}\n",
    "        self.truncations = {i: False for i in self.agents}\n",
    "        #self.infos = {str(i): {} for i in self.agents}\n",
    "        self.infos = {i: {} for i in self.agents}\n",
    "        # selects the first agent\n",
    "        self._agent_selector.reinit(self.agents)\n",
    "\n",
    "        \n",
    "        self.handle_change(next_cycle=0,reset_cycle=1,\n",
    "                              next_agent=0,reset_agent=1,\n",
    "                              next_phase=0,reset_phase=1)\n",
    "        \n",
    "        self.kill_list = []\n",
    "\n",
    "        if self.render_mode is not None and self.screen is None:\n",
    "            pygame.init()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.board.reset_screen()\n",
    "            \n",
    "            self.screen = self.board.screen\n",
    "            pygame.display.set_caption(\"Tiny Risk\")\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            self.screen = pygame.Surface((400, 700))\n",
    "\n",
    "        self.init_deployment = {agent:False for agent in self.agents}\n",
    "\n",
    "        \n",
    "\n",
    "        #observation = self._get_obs()\n",
    "        #info = self._get_info()\n",
    "\n",
    "        #if self.render_mode == \"human\":\n",
    "        #    self._render_frame()\n",
    "\n",
    "        #return observation, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def render(self,last_action_txt=''):\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "        self.board.render(last_action_txt)\n",
    "\n",
    "        #if self.render_mode == \"human\":\n",
    "        \n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        observation = np.array(pygame.surfarray.pixels3d(self.screen))\n",
    "\n",
    "        return (\n",
    "            np.transpose(observation, axes=(1, 0, 2))\n",
    "            if self.render_mode == \"rgb_array\"\n",
    "            else None\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def env_risk(**kwargs):\n",
    "    \n",
    "    kwargs['render_mode'] = kwargs['render_mode'] if kwargs['render_mode'] != \"ansi\" else \"human\"\n",
    "    env = raw_env(**kwargs)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if kwargs['render_mode'] == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    #env = wrappers.TerminateIllegalWrapper(env, illegal_reward=-1)\n",
    "    #env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "                render_ = True,agent_count  = 3,use_placement_perc=True)\n",
    "\n",
    "env = env_risk(**env_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09299899-ac40-4fce-9618-2f32eb1a4a45",
   "metadata": {},
   "source": [
    "# training configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e3ac9-41f1-4878-bfd1-ef83a2572b29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4b9e8ed-a06b-4940-a900-1d32cb36af94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.board.territories[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11eb69fe-a6d9-40ca-8885-595b948cbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import  connect_four_v3 as game\n",
    "\n",
    "\n",
    "#connect_four_v3\n",
    "\n",
    "args = Args()#tyro.cli(Args)\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "TB_log = args.TB_log\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "\n",
    "\n",
    "TB_log = True\n",
    "if TB_log:    \n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e7e742-d60e-4770-9735-bed5172000eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "playe_r = 1#\"agent_1\" #\n",
    "\n",
    "num_steps = 1000000\n",
    "action_shape = (2,)\n",
    "\n",
    "\n",
    "env.reset(seed=42)\n",
    "\n",
    "def obs_converter( data, num_classes = 4, col =0 ):\n",
    "\n",
    "    if col != None:\n",
    "        return torch.concat((nn.functional.one_hot(\n",
    "                                                        data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes),\n",
    "                                  data[:,~col,None]),axis=1)[:,1:]\n",
    "\n",
    "def map_agent_phase_hot(data,num_classes = 3):\n",
    "    return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:]\n",
    "\n",
    "def map_agent_phase_vector(data,num_classes = 3):\n",
    "    return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                        num_classes = num_classes)[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_obs = obs_converter(torch.tensor(env.last()[0]['observation']),num_classes = 4)\n",
    "\n",
    "ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "action_mask_shape = env.observation_space(playe_r)['action_mask'].shape\n",
    "\n",
    "device= 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4806e0-7eb2-47e6-8507-7c7efec989f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_config_dict = dict(action_space = env.action_space(playe_r).shape[0],\n",
    "                            ob_space=np.prod(ob_space_shape)+np.prod(action_mask_shape)\n",
    "                                 +1*( len(env.possible_agents) -1) #the current_agent +1#who actor agent was\n",
    "                                 +1*(len(env.phases) -1)#the current phase\n",
    "                                 +1 # the number of troops\n",
    "                       )\n",
    "actor_config_dict =  dict(env=env,action_space = env.observation_space(playe_r)['action_mask'].shape[0],\n",
    "                            ob_space=np.prod(ob_space_shape)+np.prod(action_mask_shape)\n",
    "                                 +1*( len(env.possible_agents) -1) #the current_agent +1#who actor agent was\n",
    "                                 +1*(len(env.phases) -1)#the current phase\n",
    "                                 +1 # the number of troops\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5c345df6-5d5b-4584-b76b-4d1828fc5b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_space': 2, 'ob_space': 77}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnet_config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a41206d-a2a3-4537-84cb-b94abd049383",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor_ddqn(**actor_config_dict).to(device)\n",
    "qf1 = QNetwork(**qnet_config_dict).to(device)\n",
    "qf1_target = QNetwork(**qnet_config_dict).to(device)\n",
    "target_actor = Actor_ddqn(**actor_config_dict).to(device)\n",
    "\n",
    "\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()), lr=args.learning_rate)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0319e032-4bca-46f2-ab78-14b8de4ce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        Box(low =0, high=2000, shape =(qnet_config_dict['ob_space']+1,), dtype=int),\n",
    "        Box(low =0, high=2000, shape =(2,), dtype=float),\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8c151d-71e5-40a1-92ca-42f4ef91f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#env = game.env(render_mode=None)#\"human\")\n",
    "\n",
    "\n",
    "\n",
    "#agent_mod = Agent_shared_v1_risk(action_space = env.observation_space(playe_r)['action_mask'].shape[0],\n",
    "#                            ob_space=np.prod(ob_space_shape)+np.prod(action_mask_shape)+1#the current_agent +1#who actor agent was\n",
    "#                                 +1 #the current phase\n",
    "#                                 +1 # the number of troops\n",
    "#                                           )\n",
    "\n",
    "#lr = 0.01\n",
    "#optimizer = optim.AdamW(agent_mod.parameters(), lr=lr\n",
    "#                        #args.learning_rate\n",
    "#                       # , eps=1e-5\n",
    "#                       )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obs = torch.zeros((num_steps,) + ob_space_shape).to(device)\n",
    "actions = torch.zeros((num_steps, ) + action_shape).to(device)\n",
    "action_masks = torch.zeros((num_steps, ) + action_mask_shape).to(device)\n",
    "\n",
    "\n",
    "current_agent = torch.ones((num_steps,1)).to(device)*0#-1\n",
    "\n",
    "current_phase = torch.zeros((num_steps,1)).to(device)\n",
    "\n",
    "current_troops_count = torch.zeros((num_steps,len(env.possible_agents))).to(device)\n",
    "\n",
    "logprobs = torch.zeros((num_steps, )).to(device)\n",
    "\n",
    "rewards = torch.zeros((num_steps, len(env.possible_agents))).to(device)\n",
    "rewards_2 = torch.zeros((num_steps, len(env.possible_agents))).to(device)\n",
    "dones = torch.zeros((num_steps, len(env.possible_agents))).to(device)\n",
    "values = torch.zeros((num_steps,  )).to(device)\n",
    "episodes = torch.ones((num_steps, )).to(device)*-1\n",
    "\n",
    "t_next = torch.zeros((num_steps, len(env.possible_agents))).to(device)\n",
    "\n",
    "\n",
    "\n",
    "agent_list = list(env.agents)\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "faulting_player = \"\"\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "phase = 0\n",
    "args.minibatch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "37497953-2f40-48b4-b9c8-7930959a8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchaudio==2.2.0\n",
    "#torchdata==0.7.1\n",
    "#torchtext==0.17.0\n",
    "#torchvision==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9ef7c-982c-4445-bdff-c86e93987c9a",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33cd800-1994-442e-972b-3fdfcfb02243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "52aa1716-6579-469b-8f0f-2990f9130881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any( np.array([ curr_reward_list[i]  for i in env.agents if i != the_hero_agent]) == 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9d8373-81d8-4769-9f23-466595ca0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: dict_values([True, True, True]) ,total_reward: {1: 105.63000000000008, 2: -200.01, 3: -109.36999999999999} ,iteration: 999 ,episode: 0\n",
      "done: dict_values([True, True, True]) ,total_reward: {1: 103.31000000000004, 2: -200.16, 3: -112.66} ,iteration: 999 ,episode: 1\n",
      "done: dict_values([True, True, True]) ,total_reward: {1: -200.70999999999998, 2: 106.86000000000003, 3: -98.49} ,iteration: 999 ,episode: 2\n",
      "done: dict_values([True, True, True]) ,total_reward: {1: 105.79000000000005, 2: -106.24, 3: -200.03} ,iteration: 999 ,episode: 4\n"
     ]
    }
   ],
   "source": [
    "the_hero_agent = 1\n",
    "\n",
    "num_episodes =5\n",
    "env = env_risk(**(env_config | {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}))\n",
    "\n",
    "args.gamma = gam = 0.99\n",
    "\n",
    "gamma_t = {i:0 for i in env.possible_agents}\n",
    "\n",
    "env.reset(42)\n",
    "\n",
    "num_iterations = 1000\n",
    "\n",
    "episode_time_lim = 5000\n",
    "\n",
    "\n",
    "\n",
    "draw_count = 0\n",
    "\n",
    "draw_territory_count = 0\n",
    "\n",
    "first_count = 0\n",
    "second_count = 0\n",
    "third_count = 0\n",
    "\n",
    "third_count_draw = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for iteration in range(1, num_iterations):#args.num_iterations) :\n",
    "    #if args.anneal_lr:\n",
    "    #        frac = 1.0 - (iteration - 1.0) / 10000#args.num_iterations\n",
    "    #        lrnow = frac * lr#args.learning_rate\n",
    "    #        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "\n",
    "    step = 0\n",
    "    fault_condition = False\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "    \n",
    "    for episode in range(num_episodes):#num_episodes):\n",
    "        \n",
    "        total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "        action=1\n",
    "        if fault_condition:\n",
    "            env = env_risk(**(env_config | {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False}))#game.env(render_mode=None)\n",
    "        env.reset()\n",
    "        fault_condition = False\n",
    "        #if step>0:\n",
    "            #print( observation['action_mask'])\n",
    "        \n",
    "\n",
    "        step_count = 0\n",
    "\n",
    "        bad_move_count = 0\n",
    "\n",
    "        bad_move_phase_count = {i:0 for i in env.phases}\n",
    "        move_count =  {i:0 for i in env.phases}\n",
    "\n",
    "\n",
    "        is_draw = 0\n",
    "        draw_territory_count = 0\n",
    "        is_third = 0\n",
    "        \n",
    "        for agent in env.agent_iter():\n",
    "            e_t = env.terminations\n",
    "            if sum(e_t.values()) <2:\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                observation['observation'] =  obs_converter(torch.tensor(observation['observation'],device=device))\n",
    "                \n",
    "                episodes[step] = episode + (iteration-1)*num_episodes\n",
    "                current_phase[step] = phase\n",
    "                \n",
    "                #global_step += args.num_envs\n",
    "            \n",
    "                if True:#action != None: # this is when a player is removed\n",
    "                    \n",
    "                    obs[step] = observation['observation']#torch.Tensor(observation['observation'],device=device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                    action_masks[step] = torch.Tensor(observation['action_mask'],device=device)\n",
    "                    curr_agent = agent#int(agent[-1])\n",
    "                    current_agent[step] = curr_agent\n",
    "                    current_phase[step] = env.phase_selection\n",
    "                    phase_mapping = map_agent_phase_hot(env.phase_selection,num_classes = len(env.phases))\n",
    "                    curr_agent_mapping = map_agent_phase_hot(int(curr_agent)-1,num_classes = len(env.possible_agents))\n",
    "                    \n",
    "                    current_troops_count[step] = torch.Tensor([env.board.agents[i].bucket for i in env.possible_agents])\n",
    "                    \n",
    "\n",
    "                \n",
    "                #obs[step] = next_obs\n",
    "                #dones[step] = next_done\n",
    "            \n",
    "                #print(agent,action,termination or truncation, reward,info,observation['action_mask'],env.terminations,env.truncations)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                model_in = torch.Tensor(np.hstack((observation['observation'].reshape(-1),observation['action_mask'].reshape(-1),\n",
    "                                                   phase_mapping,\n",
    "                                                    curr_agent_mapping,\n",
    "                                                   [env.board.agents[curr_agent].bucket ]))[None,:]#.repeat(3,axis = 0)\n",
    "                                        )\n",
    "        \n",
    "                \n",
    "                if termination or truncation:\n",
    "                    action = None\n",
    "                    with torch.no_grad():\n",
    "                        act, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                        values[step] = value.flatten() # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                                        #have taken at this point and what would have been its value\n",
    "                    actions[step] = act #even after going what would have been\n",
    "                    logprobs[step] = logprob        \n",
    "                else:\n",
    "                    mask = observation[\"action_mask\"]\n",
    "                    # this is where you would insert your policy\n",
    "                    #action = env.action_space(agent).sample(mask)\n",
    "            \n",
    "                    # ALGO LOGIC: action logic\n",
    "\n",
    "                    #epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
    "        \n",
    "        \n",
    "                    #if np.random.rand() > (episode + (iteration-1)*num_episodes)/(num_iterations*num_episodes):\n",
    "\n",
    "                    if (global_step < args.learning_starts) or (\n",
    "                        np.random.rand() > min(((episode + (iteration-1)*num_episodes)/((num_iterations*num_episodes)/10))\n",
    "                            , 0.95)) or (agent != the_hero_agent):\n",
    "\n",
    "                        \n",
    "                        action = env.action_space(agent).sample()\n",
    "                        part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                        action = torch.Tensor([[[part_0],[np.around(action[1],2)]]])\n",
    "                        action = action[:,:,0]\n",
    "                        #with torch.no_grad():\n",
    "                            #print(next_obs.shape)\n",
    "                            #value = q_network(torch.Tensor(model_in).to(device))\n",
    "                            #action = actor(torch.Tensor(model_in).to(device))\n",
    "                            #action += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "                            #action = actions.cpu().numpy().clip(env.action_space(agent).low, envs.action_space(agent).high)\n",
    "                                \n",
    "                            #action, logprob, _, value = agent_mod.get_action_and_value(model_in,action = action)\n",
    "                            #action = action[:,:,0]\n",
    "                            #actions[step] = action\n",
    "\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            action = actor(torch.Tensor(model_in).to(device))\n",
    "                        #action += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "                        #action = actions.cpu().numpy().clip(env.action_space(agent).low, env.action_space(agent).high)\n",
    "                        #value = q_network(torch.Tensor(obs).to(device))\n",
    "                        #action = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "                                \n",
    "                    #else:\n",
    "                    \n",
    "                    #    with torch.no_grad():\n",
    "                            #print(next_obs.shape)\n",
    "                            \n",
    "                    #        action, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                    actions[step] = action\n",
    "                    \n",
    "\n",
    "\n",
    "                    if not observation['action_mask'][action[:,0].long()]: \n",
    "                        fault_condition =True\n",
    "                        faulting_player = agent\n",
    "\n",
    "                        if the_hero_agent == curr_agent:\n",
    "                            bad_move_count+=1\n",
    "                            bad_move_phase_count[int(current_phase[step][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                            #print('here',agent, action, observation['action_mask'])\n",
    "                    \n",
    "\n",
    "                    if the_hero_agent == curr_agent:\n",
    "                        move_count[int(current_phase[step][0])]+=1\n",
    "                    \n",
    "                #print('here',agent, action)\n",
    "                if action != None :\n",
    "                    act_2 = action.detach().numpy()[0]#list([action.detach().numpy()[0][0], max(action.detach().numpy()[0][1],0.1) ])\n",
    "                    act_2 = list([act_2[0], max(act_2[1],0.001) ])\n",
    "                else:\n",
    "                    act_2 = action\n",
    "                    \n",
    "                env.step(act_2 if action != None else None)\n",
    "                \n",
    "\n",
    "\n",
    "                if action == None:\n",
    "                    rewards[step] = np.zeros(len(env.possible_agents)) # should i keep it -1? .... hm i dont think so .\n",
    "                    dones[step] = np.zeros(len(env.possible_agents)) # frankly the guys is already done so we really dont have to do anything here.... this is the state post termination for a loser \n",
    "                    # but btw this is for the next agent ... action == None means in the last action the previous agent would have been removed.\n",
    "                    #values[step] = \n",
    "                else:\n",
    "\n",
    "                    rewards_2[step] = torch.Tensor([env.curr_rewards[i] for i in env.possible_agents])\n",
    "                    curr_reward_list =  env.curr_rewards\n",
    "                    if (step_count == (episode_time_lim-1)):\n",
    "                        curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "\n",
    "                    for i in env.possible_agents:\n",
    "                        if i != curr_agent:\n",
    "                            gamma_t[i]+=1\n",
    "                        else:\n",
    "                            gamma_t[i] =0\n",
    "\n",
    "\n",
    "\n",
    "                        if (step_count == (episode_time_lim-1)):\n",
    "                            cr_rew = -100\n",
    "                            term = True\n",
    "                        else:\n",
    "                            cr_rew = env.curr_rewards[i]\n",
    "                            term = env.terminations[i]\n",
    "                        \n",
    "\n",
    "                        \n",
    "\n",
    "                        rewards[step-gamma_t[i],i-1] += (gam**gamma_t[i])*cr_rew\n",
    "                        \n",
    "                        t_next[step-gamma_t[i],i-1] = gamma_t[i]\n",
    "\n",
    "                        \n",
    "                        dones[step-gamma_t[i],i-1] = torch.Tensor([term]) #so the panetly has to be added but attributions is really difficult\n",
    "\n",
    "                #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                \n",
    "                if sum(curr_reward_list.values()) == -300:\n",
    "                    #print('here')\n",
    "                    is_draw=1\n",
    "\n",
    "                \n",
    "                for age_i in env.possible_agents:\n",
    "                    \n",
    "                    total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                            \n",
    "                    \n",
    "                    #rewards[step] = torch.Tensor([env.curr_rewards[i] for i in env.possible_agents]) #reward\n",
    "                    #dones[step] = torch.Tensor([env.terminations[i] for i in env.possible_agents ])#termination or truncation\n",
    "\n",
    "\n",
    "\n",
    "                # so for the current actor gama_1 is probably 1 ... and remember to which back index to add.\n",
    "                # maybe increase t, that is gamma^t * R[]\n",
    "                # if the current actor, \n",
    "\n",
    "\n",
    "                \n",
    "                step +=1\n",
    "                global_step+=1\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                #print(step-1,curr_agent,env.phase_selection,env.curr_rewards,rewards[step-1],action,gamma_t)\n",
    "                #print('\\naction',action,\n",
    "                #      '\\naction_valid',observation['action_mask'][int(action[0][0,None])],\n",
    "                #      '\\nagent',agent,\n",
    "                #      '\\nselected_agent',env.agent_selection,\n",
    "                #      '\\ncurr_agent',env.board.current_agent,\n",
    "                #      '\\ncurr phase',env.phase_selection,\n",
    "                #      '\\nbad_trail count',env.board.bad_trials,\n",
    "                #      '\\nmax_bad trails', env.board.max_bad_trials,\n",
    "                #      '\\nreward',reward,\n",
    "                #      '\\nreward',env.curr_rewards,\n",
    "                #      '\\nrewards',env.rewards,\n",
    "                #      '\\nbuckets', [env.board.agents[i].bucket for i in env.agents]\n",
    "                #     )\n",
    "                #print('kill_list',env.kill_list,'term',e_t, 'buckets',[i.bucket for i in env.board.agents])\n",
    "            \n",
    "            \n",
    "            \n",
    "            else:\n",
    "                print('done:',env.terminations.values(),\",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                break    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            step_count+=1\n",
    "            \n",
    "            if (global_step == num_steps) :# or (fault_condition and (fa ulting_player != agent) and (len(env.agents)==0)):\n",
    "                break\n",
    "            elif (step_count == episode_time_lim):\n",
    "                break\n",
    "                \n",
    "        #print(rewards[step-2])\n",
    "        if global_step == num_steps:\n",
    "            break\n",
    "\n",
    "\n",
    "        position = 3\n",
    "        for k_,(i_,j_) in enumerate(sorted([(j_,i_) for i_,j_ in total_rewards.items()],reverse=True) \n",
    "              ):\n",
    "            if j_==the_hero_agent:\n",
    "                position = k_+1\n",
    "        #print(sum(curr_reward_list.values()),curr_reward_list.values())\n",
    "        if is_draw:\n",
    "            draw_count +=1\n",
    "            if position ==3:\n",
    "                third_count_draw +=1\n",
    "                \n",
    "            #    draw_territory_count = 0\n",
    "            #else:\n",
    "            draw_territory_count = int(observation['observation'][:,the_hero_agent].sum())\n",
    "\n",
    "            writer.add_scalar(\"win_charts/3rd_position_all_prop\",int(position ==3),((episode + (iteration-1)*num_episodes)+1))\n",
    "            \n",
    "            writer.add_scalar(\"win_charts/draw_count\",draw_count,global_step)\n",
    "            writer.add_scalar(\"win_charts/draw\",1,((episode + (iteration-1)*num_episodes)+1))\n",
    "            writer.add_scalar(\"win_charts/draw_to_total_count\",draw_count/(episode + (iteration-1)*num_episodes +1),global_step)\n",
    "            \n",
    "            writer.add_scalar(\"win_charts/draw_territory_count\",draw_territory_count,global_step)\n",
    "\n",
    "        \n",
    "            writer.add_scalar(\"win_charts/third_place_in_draw\",third_count_draw,global_step)\n",
    "            writer.add_scalar(\"win_charts/third_place_in_draw_ratio\",third_count_draw/draw_count,global_step)\n",
    "        else:\n",
    "            \n",
    "            first_count += (position == 1)\n",
    "            second_count += (position == 2)\n",
    "            third_count += (position == 3)\n",
    "            writer.add_scalar(\"win_charts/1st_position_prop\",int(position==1),((episode + (iteration-1)*num_episodes)+1))\n",
    "            writer.add_scalar(\"win_charts/2nd_position_prop\",int(position==2),((episode + (iteration-1)*num_episodes)+1))\n",
    "            writer.add_scalar(\"win_charts/3rd_position_prop\",int(position==3),((episode + (iteration-1)*num_episodes)+1))\n",
    "            writer.add_scalar(\"win_charts/3rd_position_all_prop\",int(position==3),((episode + (iteration-1)*num_episodes)+1))\n",
    "            writer.add_scalar(\"win_charts/draw\",0,((episode + (iteration-1)*num_episodes)+1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            writer.add_scalar(\"win_charts/1st_position\",first_count,((episode + (iteration-1)*num_episodes)+1))#global_step)\n",
    "\n",
    "        \n",
    "            writer.add_scalar(\"win_charts/2nd_position\",second_count,((episode + (iteration-1)*num_episodes)+1))#global_step)\n",
    "            writer.add_scalar(\"win_charts/3rd_position\",third_count,((episode + (iteration-1)*num_episodes)+1))#global_step)  \n",
    "            \n",
    "            writer.add_scalar(\"win_charts/1st_position_to_total_terminated\",first_count/((episode + (iteration-1)*num_episodes)-draw_count+1),((episode + (iteration-1)*num_episodes)+1))#global_step)\n",
    "\n",
    "        \n",
    "            writer.add_scalar(\"win_charts/2nd_position_to_total_terminated\",second_count/((episode + (iteration-1)*num_episodes)-draw_count+1),((episode + (iteration-1)*num_episodes)+1))#global_step)\n",
    "            writer.add_scalar(\"win_charts/3rd_position_to_total_terminated\",third_count/((episode + (iteration-1)*num_episodes)-draw_count+1),((episode + (iteration-1)*num_episodes)+1))#global_step)             \n",
    "\n",
    "        writer.add_scalar(\"win_charts/3rd_position_to_total\",(third_count+third_count_draw)/(episode + (iteration-1)*num_episodes +1 ),((episode + (iteration-1)*num_episodes)+1))#global_step)\n",
    "\n",
    "        \n",
    "        writer.add_scalar(\"charts/epsilon\",((episode + (iteration-1)*num_episodes)/((num_iterations*num_episodes)/10)),global_step)\n",
    "        writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), global_step)\n",
    "        writer.add_scalar(\"new_charts/bad_move_count_per_episode\",bad_move_count,global_step)\n",
    "        \n",
    "        writer.add_scalar(\"new_charts/bad_move_count_position_per_episode\",bad_move_phase_count[0],global_step)\n",
    "        writer.add_scalar(\"new_charts/bad_move_count_attack_per_episode\",bad_move_phase_count[1],global_step)\n",
    "        writer.add_scalar(\"new_charts/bad_move_count_fortify_per_episode\",bad_move_phase_count[2],global_step)\n",
    "\n",
    "        writer.add_scalar(\"new_charts/total_moves\",sum(move_count.values()),global_step)\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"new_charts/bad_move_to_step_count_per_episode\",bad_move_count/(sum(move_count.values())+1),global_step)\n",
    "\n",
    "        writer.add_scalar(\"new_charts/bad_move_to_step_position_per_episode\",bad_move_phase_count[0]/( move_count[0]+1),global_step)\n",
    "        writer.add_scalar(\"new_charts/bad_move_to_step_attack_per_episode\",bad_move_phase_count[1]/( move_count[1]+1),global_step)\n",
    "        writer.add_scalar(\"new_charts/bad_move_to_step_fortify_per_episode\",bad_move_phase_count[2]/( move_count[2]+1),global_step)\n",
    "\n",
    "\n",
    "        values_total = {i:0 for i in env.possible_agents}\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"charts/episodic_length\", (episodes[:step] == (episode + (iteration-1)*num_episodes)).sum(), global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            cur_index = torch.where((current_agent[:,0] == i) &( episodes == (episode + (iteration-1)*num_episodes)  ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], global_step)\n",
    "\n",
    "    \n",
    "        #writer.add_scalar(\"charts/avg_per_epi_value\", np.mean(list(values_total.values())), global_step)\n",
    "\n",
    "    # only learning from my experience\n",
    "\n",
    "    #create_ a list of next observations\n",
    "    # but only for hero agent.\n",
    "    t_range = torch.Tensor(np.arange(0,step)).to(dtype=torch.int)\n",
    "    hero_steps = [current_agent == the_hero_agent][0][:,0][:step]\n",
    "    \n",
    "    next_indecies = (t_next[:step,the_hero_agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "\n",
    "    selected_t_next = t_next[:,the_hero_agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "    infos = [dir({})]*step #t_next[:step,0]\n",
    "    b_obs_a = torch.concat(( obs.reshape(-1,np.prod(ob_space_shape)) ,\n",
    "                            action_masks.reshape(-1,np.prod(action_mask_shape)),\n",
    "                            map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)+1)[:,1:],\n",
    "                            \n",
    "                            #map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)),\n",
    "                            map_agent_phase_vector(current_phase,num_classes=len(env.phases)),\n",
    "                           current_troops_count[:,the_hero_agent-1,None],selected_t_next\n",
    "                           ),axis =1)\n",
    "\n",
    "    for i in zip(b_obs_a[:step][hero_steps], b_obs_a[next_indecies], \n",
    "       actions[:step][hero_steps], rewards[:step][hero_steps][:,the_hero_agent-1,None], \n",
    "       dones[:step][hero_steps][:,the_hero_agent-1,None], infos):\n",
    "        rb.add(*i)\n",
    "\n",
    "\n",
    "    for epoch in range(args.update_epochs):\n",
    "        \n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                collected_t_next = data.next_observations[:,-1]\n",
    "                next_state_actions = target_actor(data.next_observations[:,:-1].to(dtype=torch.float))\n",
    "                qf1_next_target = qf1_target(data.next_observations[:,:-1], next_state_actions)\n",
    "\n",
    "                \n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * (args.gamma**(collected_t_next+1)).view(-1) * (qf1_next_target).view(-1)\n",
    "        \n",
    "            qf1_a_values = qf1(data.observations[:,:-1].to(dtype=torch.float32), data.actions.to(dtype=torch.float32)).view(-1)\n",
    "            qf1_loss = nn.functional.mse_loss(qf1_a_values, next_q_value)\n",
    "            \n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf1_loss.backward()\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            #if global_step % args.policy_frequency == 0:\n",
    "            actor_loss = -qf1(data.observations[:,:-1].to(dtype=torch.float32), actor(data.observations[:,:-1].to(dtype=torch.float32))).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "        \n",
    "            # update the target network\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            ind_ = epoch + (iteration-1)*args.update_epochs\n",
    "            \n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), ind_)\n",
    "            \n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), ind_)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), ind_)\n",
    "            #print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "            \n",
    "    if global_step%100 ==0:\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))       \n",
    "    \n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    avg_episode_length = np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "    writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e4714-0df2-4a24-b119-f9f66872e2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab7ac4-96f2-43d9-8fe2-721d0cd3d102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7646750-f38b-49bd-b4fc-f7e5a6d6292c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f3d95-2b65-4046-a803-2b911197078e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726efe0-f6da-4608-85d9-47f0cdad7d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bebf0-cd82-4509-8bdf-c0526212c538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866b545-31e1-4121-a143-5c8f52183ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65239764-50ea-47a9-bd87-207627c5323d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = env_risk(**(env_config | {\"render_mode\" : None,\"verbose\":False}))\n",
    "\n",
    "for i__ in range(1):\n",
    "    env.reset()#seed=42)\n",
    "    #print(env_1.infos.keys())\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    total_rewards = {i:0 for i in env.possible_agents}\n",
    "    \n",
    "    #so there is an issue ...reward =2 sometimes ... doubling?\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        e_t = env.terminations\n",
    "        if sum(e_t.values()) <2:\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            action = env.action_space(agent).sample()\n",
    "            \n",
    "            #if env_1.phase_selection ==1:\n",
    "            #    action = [int(action[0]),action[1]]\n",
    "            part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "            #print(part_0)\n",
    "            \n",
    "            #action = [part_0,np.around(action[1],2) #min(action[1],env_1.board.agents[agent].bucket ) \n",
    "            #                  if env_1.phase_selection==0 else  action[1]]\n",
    "    \n",
    "            action = [part_0,np.around(action[1],2)]\n",
    "    \n",
    "    \n",
    "            for i in env.possible_agents:\n",
    "                total_rewards[i]+=env.curr_rewards[i]\n",
    "            \n",
    "            env.step(action)\n",
    "            print(env.board.territories)\n",
    "            print('\\naction',action,\n",
    "                  '\\naction_valid',observation['action_mask'][action[0]],\n",
    "                  '\\nagent',agent,\n",
    "                  '\\nselected_agent',env.agent_selection,\n",
    "                  '\\ncurr_agent',env.board.current_agent,\n",
    "                  '\\ncurr phase',env.phase_selection,\n",
    "                  '\\nbad_trail count',env.board.bad_trials,\n",
    "                  '\\nmax_bad trails', env.board.max_bad_trials,\n",
    "                  '\\nreward',reward,\n",
    "                  '\\nreward',env.curr_rewards,\n",
    "                  '\\nrewards',env.rewards,\n",
    "                  '\\nbuckets', [env.board.agents[i].bucket for i in env.agents]\n",
    "                 )\n",
    "            print('kill_list',env.kill_list,'term',e_t, 'buckets',[i.bucket for i in env.board.agents])\n",
    "            #if sum(env.curr_rewards.values()) <0:\n",
    "            #    input()\n",
    "        else:\n",
    "            print(env.board.territories)\n",
    "            print('done',env.terminations.values(),env.curr_rewards)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7caec800-0ebb-44a4-80a1-ad7073bc9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "id": "4b9d3d8f-a24b-40ca-876f-0b6d3ad2a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/1340412715.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return nn.functional.one_hot(torch.tensor(data[:,0].clone().detach()).long(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_agent_phase_vector(current_phase,num_classes=len(env.phases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "81c68751-eef0-4e68-bc49-6c1260fc5747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/1340412715.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return nn.functional.one_hot(torch.tensor(data[:,0].clone().detach()).long(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [1, 0],\n",
       "        ...,\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)+1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "abbf32e3-5676-45ac-8dc8-155c836124ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_agent[:,0] == i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "e9db5f25-90ab-4e57-9230-343f0c6db436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_agent[:100,0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "0e0fbecb-f0b1-498c-b9c6-533d4421b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/2524108104.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nn.functional.one_hot(torch.tensor(current_agent[:100,0].clone().detach()+1).long(),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[875], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "nn.functional.one_hot(torch.tensor(current_agent[:100,0].clone().detach()+1).long(), \n",
    "                                                        num_classes = 3+1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "40cd3283-e1a8-4537-9c7c-9f748b181974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/4170305161.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return nn.functional.one_hot(torch.tensor(data[:,0].clone().detach()+1).long(),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[870], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m current_agent\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmap_agent_phase_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpossible_agents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[843], line 21\u001b[0m, in \u001b[0;36mmap_agent_phase_vector\u001b[0;34m(data, num_classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_agent_phase_vector\u001b[39m(data,num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "current_agent.unique()\n",
    "map_agent_phase_vector(current_agent[:4,],num_classes=len(env.possible_agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "67e2710f-7dd8-4854-95da-ce676979641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/4170305161.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return nn.functional.one_hot(torch.tensor(data[:,0].clone().detach()+1).long(),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[851], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmap_agent_phase_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpossible_agents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[843], line 21\u001b[0m, in \u001b[0;36mmap_agent_phase_vector\u001b[0;34m(data, num_classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_agent_phase_vector\u001b[39m(data,num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "b98bf744-6eab-4457-bed9-d61d91aef981",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99cfef-039b-4550-9585-f04ffc49b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.mse_loss.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "38ae40a2-d79d-4832-a579-2bfebe0738e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.update_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fe4992da-9e20-49ae-8a5e-daaa92c6d900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8985a796-73b8-40e7-8423-d782b2173111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (385346547.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[73], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    with torch.no_grad():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#env.close()\n",
    "    with torch.no_grad():\n",
    "        #advantages = torch.zeros_like(rewards).to(device)\n",
    "    \n",
    "        for i_epi in range((iteration-1)*num_episodes,episode + (iteration-1)*num_episodes +1):\n",
    "            for agents_id in agent_list:\n",
    "                agents_id_i = agents_id-1\n",
    "                 \n",
    "                cur_index = torch.where((current_agent[:,0] == agents_id) &( episodes == i_epi))[0]\n",
    "\n",
    "                #values\n",
    "            \n",
    "                # bootstrap value if not done\n",
    "            \n",
    "                #next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                \n",
    "                lastgaelam = 0\n",
    "        \n",
    "        \n",
    "                last_index = cur_index[-1]\n",
    "                nextnonterminal = 0 #1.0 it is the last and it is terminal ... so the next would be terminal too ... there should not be a next\n",
    "                delta = rewards[last_index,agents_id_i] - values[last_index] # so even this should be find because ... rewards is already updated..\n",
    "                #print(agents_id,last_index,nextnonterminal,delta,rewards[last_index],values[last_index])\n",
    "                advantages[last_index] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                \n",
    "                \n",
    "                for o,t in reversed(list(enumerate(cur_index))[:-1]):\n",
    "                    \n",
    "        \n",
    "                    nextnonterminal = 1.0 - dones[cur_index[o + 1],agents_id_i]\n",
    "                    nextvalues = values[int(t  + t_next[t,agents_id_i] +1) ] #t  + t_next[t,agents_id]+1 is when the player acts next ...\n",
    "                    delta = rewards[t,agents_id_i] + (args.gamma**t_next[t,agents_id_i]) * nextvalues * nextnonterminal - values[t] # t is the index when the player acted\n",
    "                    #print(agents_id,o,t,cur_index[o + 1],nextnonterminal,nextvalues,delta,rewards[t])\n",
    "                    advantages[t,agents_id_i] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        \n",
    "        returns = advantages[range(advantages.shape[0]),current_agent[:,0].to(dtype=int)-1] + values\n",
    "        \n",
    "        \n",
    "    #print(agent_list)\n",
    "    b_obs = obs.reshape(obs.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(actions.shape)\n",
    "    b_current_phase = current_phase.reshape(current_phase.shape)\n",
    "    b_current_troops_count = current_troops_count[np.arange(len(current_troops_count)),current_agent[:,0].to(int)-1,None]\n",
    "    b_action_masks = action_masks.reshape(action_masks.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    b_t_next =t_next.reshape(t_next.shape)\n",
    "    b_obs_a = torch.concat(( b_obs.reshape(-1,np.prod(ob_space_shape)) ,\n",
    "                            b_action_masks.reshape(-1,np.prod(action_mask_shape)),current_agent,current_phase,\n",
    "                           b_current_troops_count\n",
    "                           ),axis =1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(step)#args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, step, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "    \n",
    "            _, newlogprob, entropy, newvalue = agent_mod.get_action_and_value(b_obs_a[mb_inds],# b_obs[mb_inds].reshape(-1,np.prod(ob_space_shape)),#\n",
    "                                                                              \n",
    "                                                                          b_actions.long()[mb_inds],training=True)\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "    \n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "    \n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "    \n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "    \n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent_mod.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "    \n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "\n",
    "    mean_value_per_episode_per_batch = np.mean([b_values[:step][(episodes[:step] == i_epi)].mean() for i_epi in episodes[:step].unique()])\n",
    "\n",
    "    avg_episode_length = np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "    \n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if TB_log:\n",
    "\n",
    "        writer.add_scalar(\"charts/mean_value_per_episode_per_batch\", mean_value_per_episode_per_batch, global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, global_step)\n",
    "\n",
    "        \n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        \n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    if global_step%100 ==0:\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    if TB_log:  \n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "env.close()\n",
    "writer.close()    \n",
    "\n",
    "            \n",
    "#help if the player was killed before even his first play??? but that cannot happen .. to be killed we need to place something before hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9ed6e-f67a-43a3-836c-4e83783e2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_episode_length = np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2122,
   "id": "29efe207-dda8-4b73-9a28-2a5c84d70cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.target_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2113,
   "id": "142bfff5-e453-49b9-8295-a1666d98a0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 2113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_actions.long()[mb_inds].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2103,
   "id": "3008cc68-0e4b-4196-ac8c-069c4c0c870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(250)"
      ]
     },
     "execution_count": 2103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(current_agent[:,0] == agents_id)\n",
    "\n",
    "( episodes == i_epi).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2102,
   "id": "a00b7a07-5253-4d72-8840-d5241cf8de92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 2102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where((current_agent[:,0] == agents_id) &( episodes == i_epi))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2097,
   "id": "14f7eb8c-3aed-4da2-9bb2-82dee544e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 2097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode + (iteration-1)*num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2095,
   "id": "e1d2615e-becf-4e28-86c8-eedbe91f5837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., 38., 39., 47., 48., 49., 50., 51., 52., 53., 54.])"
      ]
     },
     "execution_count": 2095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2093,
   "id": "066e308e-760d-40b8-8e83-7ba496c80b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 2093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where((current_agent[:,0] == i) &( episodes == episode))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "9c08fa6b-ee16-46b6-af99-c9ccacbbafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_72257/395297320.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  oj = torch.tensor(obs[1][:,0])\n"
     ]
    }
   ],
   "source": [
    "oj = torch.tensor(obs[1][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "ebf89c3f-9daa-444c-9d9a-978b8d462949",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'nunique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[535], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m(obs[\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_project/lib/python3.12/site-packages/numpy/__init__.py:333\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoved in NumPy 1.25.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTester was removed in NumPy 1.25.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'nunique'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "cf21410b-7248-4d77-a87b-9ee67b685d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.functional.one_hot??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "2a5c4661-cf99-47aa-81cb-c75cfd22a51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.one_hot(oj.long(), num_classes = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2080,
   "id": "089886c7-4e9e-489b-9231-6d8bd7c875a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  ...,  1.,  0., 10.],\n",
       "        [ 0.,  0.,  0.,  ...,  1.,  0.,  7.],\n",
       "        [ 0.,  0.,  0.,  ...,  2.,  0., 20.],\n",
       "        ...,\n",
       "        [ 3.,  1.,  3.,  ...,  3.,  0.,  1.],\n",
       "        [ 3.,  1.,  3.,  ...,  3.,  0.,  1.],\n",
       "        [ 3.,  1.,  3.,  ...,  3.,  1.,  1.]])"
      ]
     },
     "execution_count": 2080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat(( b_obs.reshape(-1,np.prod(ob_space_shape)) ,\n",
    "                         b_action_masks.reshape(-1,np.prod(action_mask_shape)),current_agent[:step],current_phase[:step],\n",
    "                        b_current_troops_count\n",
    "                        ),axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2076,
   "id": "1df1a06f-4523-4d3f-8617-133e4ef82b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 0, 3: 0}"
      ]
     },
     "execution_count": 2076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2075,
   "id": "289419b1-8088-4fa8-9244-4cbc00df9959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.0000,  0.0000,  0.0000],\n",
       "        [ 9.0000,  0.0000,  0.0000],\n",
       "        [ 4.0100,  2.0000,  0.0000],\n",
       "        [ 4.0000,  3.0000,  0.0000],\n",
       "        [ 2.0000,  2.0000,  0.0000],\n",
       "        [-1.0000,  3.0000,  0.0000],\n",
       "        [-1.0000,  3.0000,  0.0000],\n",
       "        [ 1.0000,  6.0000,  0.0000],\n",
       "        [ 2.0000,  3.0000,  1.0000],\n",
       "        [-2.9871,  4.0000,  1.0000]])"
      ]
     },
     "execution_count": 2075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[episodes == 90][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2060,
   "id": "5d4e33eb-0da6-4950-87a6-1b2c1084c745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3479)"
      ]
     },
     "execution_count": 2060,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(episodes == 94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "id": "69718f99-aedf-487b-9963-661a2f81f656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(90.), tensor([ -10832.7793, -417176.3125, -269275.1250])),\n",
       " (tensor(91.), tensor([-105029.0078, -109808.8750,  -11357.9238])),\n",
       " (tensor(92.), tensor([-18237.9336, -17203.9180,  -1950.9956])),\n",
       " (tensor(93.), tensor([-18535.3398,  -7058.8452, -11997.2920])),\n",
       " (tensor(94.), tensor([-14566.3643,   -423.5226, -11490.5801]))]"
      ]
     },
     "execution_count": 2063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([(i_epi,advantages[episodes == i_epi].sum(axis =0)) for i_epi in episodes[:step].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2054,
   "id": "1ad2b425-9377-4125-b502-c339c7f34776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(90.), tensor([-12320.5859, -16273.6934,  -7868.3691])),\n",
       " (tensor(91.), tensor([-2840.3418, -3120.0928, -1878.1528])),\n",
       " (tensor(92.), tensor([-498.0001, -371.7140, -103.9209])),\n",
       " (tensor(93.), tensor([-506.3524, -160.7971, -258.6227])),\n",
       " (tensor(94.), tensor([-414.7703, -115.9055, -249.8394]))]"
      ]
     },
     "execution_count": 2054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "([(i_epi,rewards[episodes == i_epi].sum(axis =0)) for i_epi in episodes[:step].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2046,
   "id": "3c5fb24c-e3ab-47ed-a921-d0f0cd46e6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.3092284"
      ]
     },
     "execution_count": 2046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.mean([b_values[episodes == i_epi].mean() for i_epi in episodes[:step].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1981,
   "id": "02478aa4-51ad-4d49-a499-45d97ff2e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = agent_mod.network(b_obs_a[mb_inds]# b_obs[mb_inds].reshape(-1,np.prod(ob_space_shape)),#\n",
    "                       )\n",
    "actor_1_logit = agent_mod.actor_1(hid)\n",
    "probs = Categorical(logits=actor_1_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "id": "e7b394f1-f0e2-49ca-9101-d5c5448aa455",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_1 = probs.sample()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2001,
   "id": "9ff3ebb3-03fb-43aa-b5e1-c2c022a66453",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_2 = agent_mod.actor_2(hid)[:,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "id": "0332020e-f078-4310-a20d-199f5585a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 2002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2011,
   "id": "420f542b-e19f-4df5-b102-5ee63f202cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [ 8, 10, 12],\n",
       "        [21, 24, 27]])"
      ]
     },
     "execution_count": 2011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6],[7,8,9]])*torch.tensor([1,2,3])[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2013,
   "id": "1fa18cc6-5749-4916-88ee-86f91843fb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4960],\n",
       "        [0.4925],\n",
       "        [0.4997],\n",
       "        [0.4974],\n",
       "        [0.4937],\n",
       "        [0.4935],\n",
       "        [0.5023],\n",
       "        [0.4995],\n",
       "        [0.4965],\n",
       "        [0.4983],\n",
       "        [0.4958],\n",
       "        [0.4985],\n",
       "        [0.4963],\n",
       "        [0.4998],\n",
       "        [0.4950],\n",
       "        [0.4939],\n",
       "        [0.4910],\n",
       "        [0.5016],\n",
       "        [0.4939],\n",
       "        [0.4969],\n",
       "        [0.4946],\n",
       "        [0.4970],\n",
       "        [0.4995],\n",
       "        [0.4982],\n",
       "        [0.4887],\n",
       "        [0.4936],\n",
       "        [0.5005],\n",
       "        [0.4940],\n",
       "        [0.4949],\n",
       "        [0.4992],\n",
       "        [0.4963],\n",
       "        [0.4937]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 2013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2015,
   "id": "ffcee30f-7fba-44de-a333-f96184d3eef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7124, -1.7190, -1.7144,  ..., -1.7168, -1.7008, -1.7128],\n",
       "        [-1.7000, -1.6957, -1.6882,  ..., -1.6997, -1.6908, -1.7053],\n",
       "        [-1.7295, -1.7229, -1.7181,  ..., -1.7278, -1.7381, -1.7397],\n",
       "        ...,\n",
       "        [-1.7263, -1.7301, -1.7307,  ..., -1.7252, -1.7279, -1.7288],\n",
       "        [-1.7095, -1.7177, -1.7238,  ..., -1.7156, -1.7084, -1.7036],\n",
       "        [-1.7304, -1.7374, -1.7339,  ..., -1.7244, -1.7234, -1.7417]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 2015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.log_prob(action_1)*action_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "id": "569ff888-124e-4145-9225-d57c1b84c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4521, -3.4654, -3.4562,  ..., -3.4611, -3.4287, -3.4528],\n",
       "        [-3.4515, -3.4429, -3.4275,  ..., -3.4509, -3.4329, -3.4623],\n",
       "        [-3.4613, -3.4480, -3.4385,  ..., -3.4579, -3.4784, -3.4817],\n",
       "        ...,\n",
       "        [-3.4580, -3.4656, -3.4668,  ..., -3.4558, -3.4612, -3.4630],\n",
       "        [-3.4443, -3.4609, -3.4731,  ..., -3.4566, -3.4421, -3.4325],\n",
       "        [-3.5051, -3.5191, -3.5121,  ..., -3.4928, -3.4909, -3.5278]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 1989,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.log_prob(action_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f857d55-e782-478a-880e-3807fac4692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        \n",
    "        if action is None:\n",
    "            action_1 = probs.sample()[:,None]\n",
    "            action_2 = self.actor_2(hidden)[:,[0]]\n",
    "    \n",
    "            action = torch.concat((action_1,action_2),1)\n",
    "        else:\n",
    "            action_1,action_2 = action[:,0],action[:,1]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        return action, probs.log_prob(action_1[:,0])*action_2[:,0], probs.entropy(), self.critic(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1978,
   "id": "48114dfe-d01b-4d8e-af10-ee7c2cd9de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_2 = agent_mod.actor_2(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1980,
   "id": "e5c322d8-fcd8-4fb5-94ad-b6471e1df394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4960, 0.4925, 0.4997, 0.4974, 0.4937, 0.4935, 0.5023, 0.4995, 0.4965,\n",
       "        0.4983, 0.4958, 0.4985, 0.4963, 0.4998, 0.4950, 0.4939, 0.4910, 0.5016,\n",
       "        0.4939, 0.4969, 0.4946, 0.4970, 0.4995, 0.4982, 0.4887, 0.4936, 0.5005,\n",
       "        0.4940, 0.4949, 0.4992, 0.4963, 0.4937], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 1980,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*actor_2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1943,
   "id": "72d6c295-a018-4fbb-8024-c715dc488b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000])"
      ]
     },
     "execution_count": 1943,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1948,
   "id": "19976a24-e2c2-41b7-b9ca-d593c6baa0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 1])"
      ]
     },
     "execution_count": 1948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_agent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1953,
   "id": "b3f76218-05c8-48c4-9939-ad65c072961f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 1])"
      ]
     },
     "execution_count": 1953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_phase\n",
    "current_troops_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e1cc4-3593-449d-b50e-25e993960cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[curr_agent,env.phase_selection,env.board.agents[curr_agent].bucket ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1965,
   "id": "c5bad4cc-8540-4db1-933a-a9a4cd18c435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  9., 11.]])"
      ]
     },
     "execution_count": 1965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1967,
   "id": "d7fa127a-cf8f-45ff-bc23-491d72de89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "#print(agent_list)\n",
    "b_obs = obs.reshape(obs.shape)\n",
    "b_logprobs = logprobs.reshape(-1)\n",
    "b_actions = actions.reshape(actions.shape)\n",
    "b_current_phase = current_phase.reshape(current_phase.shape)\n",
    "b_current_troops_count = current_troops_count[np.arange(len(current_troops_count)),current_agent[:,0].to(int)-1,None]\n",
    "b_action_masks = action_masks.reshape(action_masks.shape)\n",
    "b_advantages = advantages.reshape(-1)\n",
    "b_returns = returns.reshape(-1)\n",
    "b_values = values.reshape(-1)\n",
    "b_t_next =t_next.reshape(t_next.shape)\n",
    "b_obs_a = torch.concat(( b_obs.reshape(-1,np.prod(ob_space_shape)) ,\n",
    "                        b_action_masks.reshape(-1,np.prod(action_mask_shape)),current_agent,current_phase,\n",
    "                       b_current_troops_count\n",
    "                       ),axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2af4c5-7a5a-4862-adb1-f5f54e9b06b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e54fdce-e730-40f6-9913-1deb4771f4de",
   "metadata": {},
   "source": [
    "every_person must only look at its own troops at the time ... so we dont need to look at what happend to my troop when its not my turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1964,
   "id": "238a623b-a657-4053-ab5c-3d770c9115ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 1])"
      ]
     },
     "execution_count": 1964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_phase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1962,
   "id": "4a284fea-bfb0-4f87-9e3b-4bbd141ab146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.],\n",
       "        [ 7.],\n",
       "        [ 7.],\n",
       "        ...,\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]])"
      ]
     },
     "execution_count": 1962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_troops_count[np.arange(len(current_troops_count)),current_agent[:,0].to(int)-1,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1358af-d0d4-4c17-b927-70a89feb3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_troops_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1970,
   "id": "424e61df-2238-40df-8360-0f2259267406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000000, 10, 2]),\n",
       " torch.Size([1000000]),\n",
       " torch.Size([1000000, 2]),\n",
       " torch.Size([1000000, 1]),\n",
       " torch.Size([1000000, 3]),\n",
       " torch.Size([1000000, 32]),\n",
       " torch.Size([1000000, 3]),\n",
       " torch.Size([1000000]),\n",
       " torch.Size([1000000]))"
      ]
     },
     "execution_count": 1970,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(obs.shape\n",
    ",logprobs.shape\n",
    ",actions.shape\n",
    ",current_phase.shape\n",
    ",current_troops_count.shape\n",
    ",action_masks.shape\n",
    ",advantages.shape\n",
    ",returns.shape\n",
    ",values.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1811,
   "id": "7d9e4205-3b61-4847-8c0c-121f3f520a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,  22,  23,  24,  25,  26,\n",
       "         35,  36,  37,  38,  44,  51,  52,  53,  54,  55,  70,  73,  74,  75,\n",
       "         76,  77,  78,  79,  80,  81,  90,  96, 105, 106, 118, 125, 126, 127,\n",
       "        128, 134, 143, 144, 145, 146, 147, 148, 149, 157, 158, 165, 178, 193,\n",
       "        194, 195, 196, 197, 205, 206, 207, 208, 209, 210, 221, 229, 230, 233,\n",
       "        234, 235, 240, 241, 242, 243, 251, 252, 254, 262, 268, 269, 275, 276,\n",
       "        277, 278])"
      ]
     },
     "execution_count": 1811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where((current_agent[:,0] == 1) &( episodes == 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1815,
   "id": "b8ac4817-61f7-45e6-ae8f-fd713d6fed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 13,  14,  15,  16,  17,  18,  19,  20,  21,  31,  32,  33,  34,  41,\n",
       "         42,  43,  47,  48,  49,  50,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         72,  87,  88,  89,  92,  93,  94,  95, 100, 101, 102, 103, 104, 116,\n",
       "        117, 124, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 154, 155,\n",
       "        156, 161, 162, 163, 164, 173, 174, 175, 176, 177, 185, 186, 187, 188,\n",
       "        189, 190, 191, 192, 199, 200, 201, 202, 203, 204, 213, 214, 215, 216,\n",
       "        217, 218, 219, 220, 222, 223, 224, 225, 226, 227, 228, 231, 232, 236,\n",
       "        237, 238, 239, 244, 245, 246, 247, 248, 249, 250, 253, 255, 256, 257,\n",
       "        258, 259, 260, 261, 263, 264, 265, 266, 267, 270, 271, 272, 273, 274,\n",
       "        279, 280, 281, 282, 283, 284, 285, 286])"
      ]
     },
     "execution_count": 1815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where((current_agent[:,0] == 3) &( episodes == 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1817,
   "id": "e0012bca-2bbe-413b-83f1-62d4b47f9345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.5665, -4.5665, -3.2696, -0.3031,  1.1337,  1.1337,  1.1337,  1.2421,\n",
       "         1.3567,  1.1014,  1.1014,  1.1014,  1.1014,  1.0461,  1.1337,  1.2421,\n",
       "         1.1014,  1.1014,  1.1014,  1.1337,  1.1014,  1.1014,  1.2041,  1.2041,\n",
       "         1.2041,  1.2041,  1.2041,  1.2041,  1.0461,  0.9699,  1.0692,  1.0692,\n",
       "         0.8751,  1.1436,  1.2592,  1.3845,  1.1337,  1.1337,  1.1337,  1.2421,\n",
       "         1.2421,  1.1014,  1.1014,  1.0461,  0.9699,  0.9699,  1.0461,  0.9699,\n",
       "         1.1337,  1.1337,  1.1337,  1.1337,  1.2421,  1.3567,  1.1014,  1.1014,\n",
       "         1.1014,  1.0461,  1.1014,  1.1014,  1.1436,  1.1337,  1.1337,  1.1337,\n",
       "         1.1337,  1.2421,  1.1014,  1.1337,  1.1337,  1.1337,  1.2421,  1.2421,\n",
       "         1.2421,  1.3567,  1.1014,  1.1014,  1.1337,  1.1337,  1.1337,  1.2421,\n",
       "         1.1014,  1.2041,  1.2041,  1.2041,  1.2041,  1.2041,  1.2041,  1.3094,\n",
       "         1.0461,  1.1337,  1.1337,  1.1337,  1.1337,  1.2421,  1.3567,  1.1014,\n",
       "         1.1337,  1.1014,  1.1337,  1.2421,  1.3567,  1.1014,  1.1337,  1.1337,\n",
       "         1.1337,  1.1337,  1.2421,  1.3567,  1.1014,  1.0461,  1.1337,  1.1337,\n",
       "         1.1337,  1.1337,  1.2421,  1.3567,  1.1014,  1.1014,  1.1337,  1.2421,\n",
       "         1.2421,  1.1014,  1.1014,  1.1014,  1.1014,  1.2041,  1.0461,  1.1014,\n",
       "         1.1337,  1.1337,  1.1337,  1.1337,  1.2421,  1.2421])"
      ]
     },
     "execution_count": 1817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[torch.where((current_agent[:,0] == 3) &( episodes == 0))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1819,
   "id": "aefc811d-935f-4ae7-a2aa-d0646f906d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5403,  0.5215,  0.6688,  0.9050,  0.7962,  1.0707,  1.0707,  1.1576,\n",
       "         1.1251,  1.1251,  1.0707,  1.1408,  0.9966,  0.9966,  0.9966,  1.1576,\n",
       "         1.1576,  1.2425,  1.1251,  1.0707,  1.0707,  1.1251,  1.2014,  1.2014,\n",
       "         1.0707,  0.9966,  1.0639,  1.0639,  0.9050,  1.1576,  1.1576,  1.1576,\n",
       "         1.1576,  1.2425,  1.2425,  1.2425,  1.3316,  1.1251,  1.1251,  1.2014,\n",
       "         1.2802,  1.2802,  1.0707,  1.0707,  0.9966,  0.9050,  0.9731,  0.9731,\n",
       "         0.9731,  0.7962,  1.0707,  0.9966,  1.1576,  1.1576,  1.1576,  1.1576,\n",
       "         1.2425,  1.3316,  1.1251,  1.1251,  1.1576,  1.1576,  1.1576,  1.2425,\n",
       "         1.1251,  1.0707,  1.0707])"
      ]
     },
     "execution_count": 1819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[torch.where((current_agent[:,0] == 2) &( episodes == 0))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1812,
   "id": "9fa90986-0a02-4616-9eda-4289728cc03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2430, 1.1102, 1.1102, 1.1885, 1.1885, 1.1885, 1.2440, 1.2440, 1.2440,\n",
       "        1.1593, 1.1885, 1.1885, 1.1885, 1.1885, 1.1593, 1.1885, 1.2440, 1.3055,\n",
       "        1.1593, 1.1102, 1.1593, 1.2056, 1.2056, 1.2575, 1.1102, 1.0430, 1.0430,\n",
       "        1.0430, 1.1102, 1.1885, 1.1885, 1.2440, 1.2440, 1.3055, 1.1593, 1.1102,\n",
       "        1.0430, 1.0430, 0.9588, 0.8571, 0.8571, 0.8944, 0.9262, 0.7360, 0.5937,\n",
       "        1.1102, 1.1885, 1.2440, 1.2440, 1.2440, 1.3055, 1.1593, 1.1885, 1.1593,\n",
       "        1.1102, 1.0430, 1.1102, 1.1593, 1.1593, 1.1885, 1.1593, 1.1885, 1.1885,\n",
       "        1.2440, 1.2440, 1.3055, 1.1593, 1.1102, 1.1102, 1.0430, 1.0430, 1.0430,\n",
       "        0.9588, 0.9941, 0.9941, 1.0273, 0.8571, 0.8571, 0.7360, 0.5937, 0.4294,\n",
       "        0.4785, 0.2430, 0.2916, 0.2916, 0.2916])"
      ]
     },
     "execution_count": 1812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[torch.where((current_agent[:,0] == 1) &( episodes == 0))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1863,
   "id": "b72085b3-154c-4627-a63a-93df8aa3e9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 1863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "id": "1d85d49b-7fc1-445d-a48a-ff1818804eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5 tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "10 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "11 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "15 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "16 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "17 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "22 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "23 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "27 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "46 tensor([ 0.0000, -0.9950,  0.0000]) tensor([ 0., 12.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "50 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "54 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "58 tensor([-0.9970,  0.0000,  0.0000]) tensor([7., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "61 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([-1.,  1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "95 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "109 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "116 tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "126 tensor([-1.9860,  0.0000,  0.0000]) tensor([10.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "132 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([-1.,  1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "134 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([-1.,  1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "138 tensor([ 0.0000, -0.9891,  0.0000]) tensor([ 0., 13.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "143 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "149 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([ 1., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "152 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 15.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "157 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "160 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "161 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "167 tensor([-0.9940,  0.0000,  0.0000]) tensor([8., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "173 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "178 tensor([-0.9940,  0.0000,  0.0000]) tensor([7., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "186 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "189 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "194 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "198 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "204 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([ 1., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "213 tensor([ 0.0000,  0.0000, -0.9861]) tensor([ 0.,  0., 15.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "218 tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "225 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "227 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "240 tensor([ 0.0000,  0.0000, -0.9940]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "257 tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "263 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "295 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "296 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "298 tensor([ 0.0000,  0.0000, -0.9950]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "300 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([ 1., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "303 tensor([1., 0., 0.]) tensor([0., 0., 0.]) tensor([ 1.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "304 tensor([-0.9980,  0.0000,  0.0000]) tensor([6., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "306 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([-1.,  1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "318 tensor([-0.9920,  0.0000,  0.0000]) tensor([10.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "322 tensor([ 0.0000, -0.9970,  0.0000]) tensor([ 0., 11.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "325 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "326 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "353 tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "355 tensor([-1.9811,  0.0000,  0.0000]) tensor([11.,  0.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "363 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "364 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "365 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([-1.,  0.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "375 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "381 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "391 tensor([ 0.0000, -1.9870,  0.0000]) tensor([ 0., 10.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "396 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "397 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "398 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "409 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "413 tensor([-100.5706,    0.0000,    0.0000]) tensor([7197.,    0.,    0.]) tensor([0., 0., 0.]) tensor([1., 0., 0.]) tensor([0., 0., 0.])\n",
      "418 tensor([0., 0., 2.]) tensor([0., 0., 0.]) tensor([-101.,    0.,    2.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "433 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "445 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "454 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "464 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "474 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "483 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "513 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "522 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "543 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "553 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "574 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "583 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "588 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "592 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "594 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "599 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "601 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "610 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "619 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "628 tensor([ 0.0000,  0.0000, -2.9910]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "630 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "631 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "632 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "641 tensor([ 0.0000,  0.0000, -3.9830]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "643 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "646 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "647 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "661 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "673 tensor([ 0.0000, -2.9880,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "676 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "678 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "692 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "697 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "698 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "700 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "709 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "712 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "717 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "718 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "720 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "725 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "726 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "728 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "732 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "734 tensor([ 0.0000,  0.0000, -1.9870]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "740 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "756 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "769 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "772 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "776 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "790 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "792 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "795 tensor([ 0.0000, -2.9820,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "799 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "800 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "801 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "802 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "808 tensor([ 0.0000,  0.0000, -1.9870]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "814 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "815 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "818 tensor([ 0.0000, -1.9900,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "822 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "824 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "839 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "867 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "871 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 12.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "876 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "877 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "889 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "899 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "903 tensor([ 0.0000,  0.0000, -3.9860]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "905 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "906 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "907 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "908 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "919 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 3.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "921 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "922 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "929 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "948 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "955 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 11.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "961 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "963 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "966 tensor([ 0.0000,  0.0000, -1.9870]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "971 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "972 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "973 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "980 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1005 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1010 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1021 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1035 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1037 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1042 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1043 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1049 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1051 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1068 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1079 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1082 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1085 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1088 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1123 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1134 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1151 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1154 tensor([ 0.0000,  0.0000, -1.9930]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1155 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1157 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1181 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1185 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1186 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1191 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1192 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1206 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1211 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1228 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1230 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1238 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1239 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1245 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1258 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1263 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1264 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1279 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1281 tensor([ 0.0000, -2.9880,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1284 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1285 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1286 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1288 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1292 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1295 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1300 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1307 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1323 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1333 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1343 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1345 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1358 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1359 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1361 tensor([ 0.0000,  0.0000, -1.9910]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1365 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1366 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1376 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1381 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1388 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1398 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 10.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1405 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1408 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 12.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1414 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1415 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1416 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1420 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1426 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1439 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1441 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1445 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1468 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1473 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1484 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1492 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1498 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1499 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1505 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1510 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1512 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1521 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1522 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 3.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1525 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1528 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1529 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1537 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1547 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1559 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1571 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1574 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1579 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1585 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1587 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1593 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1594 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1604 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1608 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1609 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1611 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1617 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1645 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1652 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1653 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1657 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1663 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1673 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1683 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1688 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1700 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1713 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1719 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1727 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1743 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1755 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1756 tensor([ 0.0000, -1.9910,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1760 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1761 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1773 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1778 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1779 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1781 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1786 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1792 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1794 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1796 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1801 tensor([ 0.0000,  0.0000, -2.9820]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1805 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1806 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1808 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1815 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1836 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1838 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1841 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1844 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1847 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1848 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1862 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1864 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1877 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1910 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1918 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1925 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1930 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1931 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1932 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1952 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1959 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1964 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1966 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1969 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1970 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1971 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1979 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "1982 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2005 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2007 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2017 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2038 tensor([ 0.0000,  0.0000, -0.9940]) tensor([ 0.,  0., 11.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2043 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2044 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2063 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2073 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2075 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2082 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2085 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2087 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2102 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2117 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2122 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2123 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2132 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2143 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2148 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2154 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2163 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2170 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2179 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2203 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2206 tensor([ 0.0000, -1.9910,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2219 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2228 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2232 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2233 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2240 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2248 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2254 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2260 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2290 tensor([ 0.0000,  0.0000, -2.9791]) tensor([ 0.,  0., 12.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2295 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2297 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2298 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2299 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2318 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2329 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2330 tensor([ 0.0000, -1.9890,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2335 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2336 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2338 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2343 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2346 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2352 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2355 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2356 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2366 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2380 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2385 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2386 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2400 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2408 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2437 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2456 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2463 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2465 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2468 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2473 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2480 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2487 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2495 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2500 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2508 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2509 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2520 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2525 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2538 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2575 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2577 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2587 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2589 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2602 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2608 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2617 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2643 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2647 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2650 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2659 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2668 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2674 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2678 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2687 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2692 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2693 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2694 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2700 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2712 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2714 tensor([ 0.0000, -0.9940,  0.0000]) tensor([ 0., 10.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2719 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2720 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2729 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2734 tensor([ 0.0000,  0.0000, -1.9910]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2738 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2739 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2750 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2761 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2773 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2794 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2796 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2808 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2821 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2822 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2825 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2836 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2844 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2855 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2903 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2914 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2949 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2952 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2953 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2967 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2972 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 13.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2978 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2980 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2985 tensor([ 0.0000,  0.0000, -1.9841]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "2992 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3004 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3006 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3008 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3010 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3024 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3037 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3050 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3061 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3076 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3078 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3083 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3093 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3123 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3133 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3141 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3156 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3165 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3166 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3172 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3173 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3175 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3177 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3187 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3202 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3206 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3221 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3238 tensor([ 0.0000,  0.0000, -1.9811]) tensor([ 0.,  0., 12.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3243 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3246 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3247 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3248 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3258 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3268 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3288 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3310 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3316 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3336 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3349 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3351 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3353 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 3.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3371 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3374 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3376 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3386 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3395 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3396 tensor([ 0.0000,  0.0000, -0.9910]) tensor([ 0.,  0., 11.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3402 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3403 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3405 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3421 tensor([ 0.0000, -1.9940,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3423 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3425 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3445 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3447 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3461 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3462 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3469 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3471 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3478 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3487 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3491 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3513 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3522 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3528 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3530 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3540 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3548 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3563 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3564 tensor([ 0.0000, -1.9910,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3568 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3569 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3593 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3596 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3615 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3649 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3657 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3660 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3661 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3663 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3664 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3665 tensor([ 0.0000,  0.0000, -2.9791]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3670 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3671 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3672 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3673 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3679 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3707 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3731 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3748 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3759 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3767 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3840 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3848 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3850 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3851 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3853 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3860 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3866 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3871 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3873 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3883 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3899 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3909 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3914 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3919 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3920 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3927 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3953 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3961 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3963 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3964 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3966 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3968 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "3976 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4013 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4020 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4021 tensor([ 0.0000, -2.9890,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4025 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4033 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4037 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4039 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4044 tensor([ 0.0000, -0.9920,  0.0000]) tensor([ 0., 12.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4045 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4051 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4054 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4061 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4065 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4068 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4071 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4081 tensor([ 0.0000,  0.0000, -1.9900]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4085 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4087 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4101 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4112 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4124 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4134 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4136 tensor([ 0.0000, -1.9890,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4140 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4142 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4143 tensor([ 0.0000,  0.0000, -2.9791]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4149 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4150 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4156 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4159 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4161 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4170 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4171 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4183 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4187 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4191 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4205 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4210 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4218 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4219 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4236 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4245 tensor([ 0.0000, -1.9940,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4247 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4256 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4261 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4266 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4274 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4285 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4287 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4290 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4295 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4298 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4299 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4310 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4327 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4332 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4333 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4345 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4371 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4379 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4382 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4384 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4390 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4391 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4394 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4399 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4401 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4404 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4405 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4418 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4423 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4424 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4438 tensor([ 0.0000,  0.0000, -1.9950]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4440 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4451 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4454 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4456 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4462 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4485 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4492 tensor([ 0.0000,  0.0000, -0.9930]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4497 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4508 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4513 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4514 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4515 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4521 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4524 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4533 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4541 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4543 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4548 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4549 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4574 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4578 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4583 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4591 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4601 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4603 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4609 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4613 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4622 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4625 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4630 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4634 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4635 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4640 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4645 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4647 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4649 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4687 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4693 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4700 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4702 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4705 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4708 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4713 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4715 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4717 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4723 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4728 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4737 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4747 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4755 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4770 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4780 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4789 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4801 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4806 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4808 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4810 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4811 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4834 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4841 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4865 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4870 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4876 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4890 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4892 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4894 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4906 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4908 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4911 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4925 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4931 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4957 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "4999 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5007 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5027 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5045 tensor([ 0.0000,  0.0000, -1.9831]) tensor([ 0.,  0., 11.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5051 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5052 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5055 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5056 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5059 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5065 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5068 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5070 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5075 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5077 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5078 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5081 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5084 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5101 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5103 tensor([ 0.0000,  0.0000, -1.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5105 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5106 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5115 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5130 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5133 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5134 tensor([ 0.0000, -2.9860,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5137 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5139 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5140 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5148 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5156 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5158 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5160 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5163 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5170 tensor([ 0.0000, -1.9910,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5174 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5175 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5176 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5181 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5187 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5194 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5215 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5217 tensor([ 0.0000,  0.0000, -1.9930]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5221 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5235 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5242 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5247 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5248 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5252 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5263 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5270 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5276 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5280 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5286 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5288 tensor([ 0.0000, -1.9870,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5292 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5293 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5294 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5295 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5304 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5313 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5323 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5359 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5387 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5437 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5451 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5459 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5460 tensor([ 0.0000,  0.0000, -1.9930]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5463 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5464 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5472 tensor([ 0.0000,  0.0000, -1.9950]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5474 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5475 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5491 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5503 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5538 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5543 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5544 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5562 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5575 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5580 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5581 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5603 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5605 tensor([ 0.0000, -1.9850,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5610 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5613 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5619 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5626 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5628 tensor([ 0.0000,  0.0000, -1.9910]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5632 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5633 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5640 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5655 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 3.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5657 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5660 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5673 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5694 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5696 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5700 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5705 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5709 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5711 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5723 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5727 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5736 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5744 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5753 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5763 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5773 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5793 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5800 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5804 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5810 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5826 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5829 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5831 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5836 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5861 tensor([ 0.0000,  0.0000, -0.9960]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5865 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5882 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5884 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5917 tensor([ 0.0000,  0.0000, -1.9910]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5922 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5924 tensor([ 0.0000, -0.9930,  0.0000]) tensor([ 0., 10.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5929 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5930 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5931 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5936 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5952 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5956 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5959 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5967 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5969 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5974 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5982 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5985 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5990 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 8., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5995 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "5997 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6027 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6030 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6032 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6042 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6044 tensor([ 0.0000,  0.0000, -1.9860]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6049 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6050 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6063 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6064 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6067 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6071 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6080 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6081 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6098 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6103 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6119 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6130 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6141 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6142 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6158 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6161 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6164 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6165 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6172 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6174 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6178 tensor([ 0.0000,  0.0000, -0.9930]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6184 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6185 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6186 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6193 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6207 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6212 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6213 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6230 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6254 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6282 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6294 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6304 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6343 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6345 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6347 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6356 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6363 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6374 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6376 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6383 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6385 tensor([ 0.0000, -2.9820,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6390 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6391 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6392 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6397 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6409 tensor([ 0.0000,  0.0000, -3.9860]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6411 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6412 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6414 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6417 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6423 tensor([ 0.0000,  0.0000, -1.9850]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6429 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6430 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6431 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6435 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6440 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6441 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6443 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6451 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6466 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6481 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6482 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6484 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6492 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6499 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6515 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6518 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6525 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6529 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6540 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6547 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6551 tensor([ 0.0000, -0.9970,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6554 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6556 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 8.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6561 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6562 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6578 tensor([ 0.0000,  0.0000, -0.9920]) tensor([ 0.,  0., 10.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6583 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6586 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6594 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6596 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6617 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6624 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6632 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6641 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6643 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6644 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 6.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6651 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6661 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6668 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6685 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6693 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6708 tensor([ 0.0000, -0.9940,  0.0000]) tensor([ 0., 10.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6713 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6714 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6718 tensor([ 0.0000,  0.0000, -1.9811]) tensor([ 0.,  0., 14.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6723 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6728 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6729 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6737 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6739 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6741 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6754 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6772 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6782 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6819 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6827 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6829 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6835 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6847 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6876 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6884 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6900 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6902 tensor([ 0.0000, -0.9940,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6908 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6926 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6928 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6930 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6932 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6944 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6953 tensor([ 0.0000,  0.0000, -1.9850]) tensor([ 0.,  0., 11.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6958 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6960 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6973 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6974 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6977 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6979 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6980 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "6990 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7007 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7009 tensor([ 0.0000, -0.9920,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7014 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7017 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7030 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7046 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7049 tensor([ 0.0000, -0.9930,  0.0000]) tensor([0., 9., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7064 tensor([ 0.0000,  0.0000, -0.9940]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7069 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7072 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7078 tensor([ 0.0000,  0.0000, -1.9860]) tensor([0., 0., 9.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7083 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7086 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7097 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7098 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7101 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7112 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7123 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7136 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7150 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7170 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7188 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7201 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7213 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7236 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7241 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7246 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7268 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7280 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7311 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7328 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7330 tensor([ 0.0000, -1.9950,  0.0000]) tensor([0., 4., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7332 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7333 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7339 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7359 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7379 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7400 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7405 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 3.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7407 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7409 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7412 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7440 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7445 tensor([ 0.0000, -1.9930,  0.0000]) tensor([0., 5., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7448 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7449 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7456 tensor([ 0.0000, -0.9960,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7472 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7479 tensor([ 0.0000,  0.0000, -0.9970]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7482 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7504 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7509 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7520 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7522 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 3., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7524 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7530 tensor([ 0.0000,  0.0000, -0.9980]) tensor([0., 0., 4.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7532 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7544 tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7562 tensor([ 0.0000,  0.0000, -0.9950]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7567 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7575 tensor([ 0.0000, -0.9950,  0.0000]) tensor([0., 7., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7579 tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([ 0.,  0., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7580 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7590 tensor([ 0.0000, -0.9980,  0.0000]) tensor([0., 6., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7592 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([ 0., -1.,  1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7596 tensor([ 0.0000,  0.0000, -2.9900]) tensor([0., 0., 7.]) tensor([0., 0., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7598 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7604 tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7605 tensor([   0.0000,    0.0000, -100.4970]) tensor([0., 0., 5.]) tensor([0., 0., 0.]) tensor([0., 0., 1.]) tensor([0., 0., 0.])\n",
      "7608 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7609 tensor([0., 1., 0.]) tensor([0., 0., 0.]) tensor([ 0.,  1., -1.]) tensor([0., 0., 0.]) tensor([0., 0., 0.])\n",
      "7610 tensor([  0., 100.,   0.]) tensor([0., 0., 0.]) tensor([-100.,  100., -100.]) tensor([0., 1., 0.]) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(step):\n",
    "    if not(all(rewards[i] == torch.zeros(3))):\n",
    "        print(i,rewards[i],t_next[i],rewards_2[i],dones[i],advantages[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "id": "6e5175dc-a318-48dd-afd8-410c31387326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-249.3551, -293.2074,  192.1446])"
      ]
     },
     "execution_count": 1786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.sum(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "id": "322c46d1-29c0-4373-8db4-839357b6076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-189.3775, -200.0503,   98.1860], dtype=torch.float64)"
      ]
     },
     "execution_count": 1787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rewards*(np.power(0.999, np.arange(len(rewards))   )[:,None])).sum(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "id": "82e8ca79-70e6-4e66-a891-4961f0b088f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  19,  20,  21,\n",
       "        22,  23,  24,  41,  42,  43,  44,  63,  64,  65,  66,  67,  68,\n",
       "        69,  70,  71,  77,  78,  79,  80,  81,  89,  90,  91,  92,  93,\n",
       "        94,  95,  96, 105, 106, 107, 108, 109, 110, 125, 126, 127, 128,\n",
       "       143, 144, 145, 146, 147, 148, 168, 169, 170, 171, 172, 173, 193,\n",
       "       206, 226, 227, 228, 231, 232, 233, 234, 235, 236, 250, 251, 259,\n",
       "       260, 261, 262, 263, 264, 271])"
      ]
     },
     "execution_count": 1502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "id": "affccddd-c87e-4373-9d21-689859d5aa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 1506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_list[19+1]-1 != "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "id": "8517578d-3d2e-47a8-9f7c-e1e3c36fabf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 1581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "id": "0256052a-001f-4042-84be-188c0aa3bcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "604"
      ]
     },
     "execution_count": 1583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "id": "862bbaf4-abc4-445d-a136-ee430a4bee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 1582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(epi_list[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "id": "dfd36ef8-e54f-4037-bb0b-6ee969de60dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,    0.,    0.],\n",
       "        [   1.,    0.,    0.],\n",
       "        [   0.,    0.,    0.],\n",
       "        ...,\n",
       "        [  -1.,    0.,    1.],\n",
       "        [   0.,    0.,    0.],\n",
       "        [-100., -100.,  100.]])"
      ]
     },
     "execution_count": 1580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[episodes == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "id": "29743f65-095d-4279-9af8-3b7753b0ce28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.) tensor(0.2430)\n",
      "values tensor(0.2430) tensor(0.2430)\n",
      "1 tensor(1.) tensor(0.5937)\n",
      "values tensor(1.5937) tensor(0.2430)\n",
      "2 tensor(0.) tensor(0.5937)\n",
      "values tensor(0.5937) tensor(0.5937)\n",
      "3 tensor(0.) tensor(0.5937)\n",
      "values tensor(0.5937) tensor(0.5937)\n",
      "4 tensor(-1.) tensor(0.6397)\n",
      "values tensor(-0.3603) tensor(0.5937)\n",
      "5 tensor(0.) tensor(0.6397)\n",
      "values tensor(0.6397) tensor(0.6397)\n",
      "6 tensor(0.) tensor(0.6397)\n",
      "values tensor(0.6397) tensor(0.6397)\n",
      "7 tensor(0.) tensor(0.6397)\n",
      "values tensor(0.6397) tensor(0.6397)\n",
      "8 tensor(0.) tensor(-2.5403)\n",
      "9 - tensor(-2.5403) 0.9801 tensor(0.)\n",
      "10 - tensor(-2.5403) 0.9702989999999999 tensor(0.)\n",
      "11 - tensor(-2.5403) 0.96059601 tensor(0.)\n",
      "12 - tensor(-2.5403) 0.9509900498999999 tensor(0.)\n",
      "13 - tensor(-2.5403) 0.9414801494009999 tensor(0.)\n",
      "14 - tensor(-2.5403) 0.9320653479069899 tensor(0.)\n",
      "15 - tensor(-2.5403) 0.92274469442792 tensor(0.)\n",
      "16 - tensor(-2.5403) 0.9135172474836407 tensor(0.)\n",
      "values tensor(-2.5403) tensor(0.6397)\n",
      "17 tensor(0.) tensor(0.4785)\n",
      "values tensor(0.4785) tensor(0.4294)\n",
      "18 tensor(0.) tensor(0.5210)\n",
      "values tensor(0.5210) tensor(0.4785)\n",
      "19 tensor(0.) tensor(-0.2677)\n",
      "20 - tensor(-0.2677) 0.9801 tensor(0.)\n",
      "21 - tensor(-0.2677) 0.9702989999999999 tensor(0.)\n",
      "22 - tensor(-0.2677) 0.96059601 tensor(0.)\n",
      "23 - tensor(-0.2677) 0.9509900498999999 tensor(0.)\n",
      "24 - tensor(-0.2677) 0.9414801494009999 tensor(0.)\n",
      "25 - tensor(-0.2677) 0.9320653479069899 tensor(0.)\n",
      "26 - tensor(-0.2677) 0.92274469442792 tensor(0.)\n",
      "27 - tensor(-0.2677) 0.9135172474836407 tensor(0.)\n",
      "28 - tensor(-0.2677) 0.9043820750088043 tensor(0.)\n",
      "29 - tensor(-0.2677) 0.8953382542587163 tensor(0.)\n",
      "30 - tensor(-0.2677) 0.8863848717161291 tensor(0.)\n",
      "values tensor(-0.2677) tensor(0.5210)\n",
      "31 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(0.2430)\n",
      "32 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "33 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1593)\n",
      "34 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "35 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "36 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "37 tensor(0.) tensor(0.1670)\n",
      "38 - tensor(0.1670) 0.9801 tensor(0.)\n",
      "39 - tensor(0.1670) 0.9702989999999999 tensor(0.)\n",
      "40 - tensor(0.1670) 0.96059601 tensor(0.)\n",
      "41 - tensor(0.1670) 0.9509900498999999 tensor(0.)\n",
      "42 - tensor(0.1670) 0.9414801494009999 tensor(0.)\n",
      "43 - tensor(0.1670) 0.9320653479069899 tensor(0.)\n",
      "44 - tensor(0.1670) 0.92274469442792 tensor(0.)\n",
      "45 - tensor(0.1670) 0.9135172474836407 tensor(0.)\n",
      "46 - tensor(0.1670) 0.9043820750088043 tensor(0.)\n",
      "47 - tensor(0.1670) 0.8953382542587163 tensor(0.)\n",
      "48 - tensor(0.1670) 0.8863848717161291 tensor(0.)\n",
      "49 - tensor(0.1670) 0.8775210229989678 tensor(0.)\n",
      "50 - tensor(0.1670) 0.8687458127689781 tensor(0.)\n",
      "51 - tensor(0.1670) 0.8600583546412883 tensor(0.)\n",
      "52 - tensor(-0.6931) 0.8514577710948754 tensor(-1.)\n",
      "53 - tensor(-0.6931) 0.8429431933839266 tensor(0.)\n",
      "54 - tensor(-0.6931) 0.8345137614500874 tensor(0.)\n",
      "55 - tensor(-0.6931) 0.8261686238355865 tensor(0.)\n",
      "56 - tensor(-0.6931) 0.8179069375972307 tensor(0.)\n",
      "57 - tensor(-0.6931) 0.8097278682212583 tensor(0.)\n",
      "values tensor(-0.6931) tensor(1.2440)\n",
      "58 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1593)\n",
      "59 tensor(0.) tensor(1.0707)\n",
      "60 - tensor(1.0707) 0.9801 tensor(0.)\n",
      "61 - tensor(1.0707) 0.9702989999999999 tensor(0.)\n",
      "62 - tensor(1.0707) 0.96059601 tensor(0.)\n",
      "63 - tensor(1.0707) 0.9509900498999999 tensor(0.)\n",
      "64 - tensor(1.0707) 0.9414801494009999 tensor(0.)\n",
      "65 - tensor(1.0707) 0.9320653479069899 tensor(0.)\n",
      "66 - tensor(1.0707) 0.92274469442792 tensor(0.)\n",
      "67 - tensor(1.0707) 0.9135172474836407 tensor(0.)\n",
      "68 - tensor(1.0707) 0.9043820750088043 tensor(0.)\n",
      "69 - tensor(1.0707) 0.8953382542587163 tensor(0.)\n",
      "70 - tensor(1.0707) 0.8863848717161291 tensor(0.)\n",
      "values tensor(1.0707) tensor(1.1885)\n",
      "71 tensor(0.) tensor(0.9966)\n",
      "72 - tensor(0.9966) 0.9801 tensor(0.)\n",
      "73 - tensor(0.9966) 0.9702989999999999 tensor(0.)\n",
      "74 - tensor(0.9966) 0.96059601 tensor(0.)\n",
      "75 - tensor(0.9966) 0.9509900498999999 tensor(0.)\n",
      "76 - tensor(0.9966) 0.9414801494009999 tensor(0.)\n",
      "77 - tensor(0.9966) 0.9320653479069899 tensor(0.)\n",
      "78 - tensor(0.9966) 0.92274469442792 tensor(0.)\n",
      "79 - tensor(0.9966) 0.9135172474836407 tensor(0.)\n",
      "80 - tensor(0.9966) 0.9043820750088043 tensor(0.)\n",
      "81 - tensor(0.9966) 0.8953382542587163 tensor(0.)\n",
      "82 - tensor(0.9966) 0.8863848717161291 tensor(0.)\n",
      "83 - tensor(0.9966) 0.8775210229989678 tensor(0.)\n",
      "84 - tensor(0.9966) 0.8687458127689781 tensor(0.)\n",
      "values tensor(0.9966) tensor(1.1593)\n",
      "85 tensor(0.) tensor(0.9050)\n",
      "86 - tensor(0.9050) 0.9801 tensor(0.)\n",
      "87 - tensor(0.9050) 0.9702989999999999 tensor(0.)\n",
      "88 - tensor(0.9050) 0.96059601 tensor(0.)\n",
      "89 - tensor(0.9050) 0.9509900498999999 tensor(0.)\n",
      "90 - tensor(0.9050) 0.9414801494009999 tensor(0.)\n",
      "91 - tensor(0.9050) 0.9320653479069899 tensor(0.)\n",
      "92 - tensor(-0.0270) 0.92274469442792 tensor(-1.)\n",
      "93 - tensor(-0.0270) 0.9135172474836407 tensor(0.)\n",
      "94 - tensor(-0.0270) 0.9043820750088043 tensor(0.)\n",
      "95 - tensor(-0.0270) 0.8953382542587163 tensor(0.)\n",
      "96 - tensor(-0.0270) 0.8863848717161291 tensor(0.)\n",
      "values tensor(-0.0270) tensor(1.1102)\n",
      "97 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0430)\n",
      "98 tensor(0.) tensor(1.1251)\n",
      "99 - tensor(1.1251) 0.9801 tensor(0.)\n",
      "100 - tensor(1.1251) 0.9702989999999999 tensor(0.)\n",
      "101 - tensor(1.1251) 0.96059601 tensor(0.)\n",
      "102 - tensor(1.1251) 0.9509900498999999 tensor(0.)\n",
      "103 - tensor(1.1251) 0.9414801494009999 tensor(0.)\n",
      "104 - tensor(1.1251) 0.9320653479069899 tensor(0.)\n",
      "105 - tensor(1.1251) 0.92274469442792 tensor(0.)\n",
      "106 - tensor(1.1251) 0.9135172474836407 tensor(0.)\n",
      "107 - tensor(1.1251) 0.9043820750088043 tensor(0.)\n",
      "108 - tensor(1.1251) 0.8953382542587163 tensor(0.)\n",
      "109 - tensor(1.1251) 0.8863848717161291 tensor(0.)\n",
      "110 - tensor(1.1251) 0.8775210229989678 tensor(0.)\n",
      "111 - tensor(1.1251) 0.8687458127689781 tensor(0.)\n",
      "112 - tensor(1.1251) 0.8600583546412883 tensor(0.)\n",
      "113 - tensor(1.1251) 0.8514577710948754 tensor(0.)\n",
      "114 - tensor(1.1251) 0.8429431933839266 tensor(0.)\n",
      "values tensor(1.1251) tensor(1.0790)\n",
      "115 tensor(0.) tensor(0.9941)\n",
      "values tensor(0.9941) tensor(0.9588)\n",
      "116 tensor(0.) tensor(0.9941)\n",
      "values tensor(0.9941) tensor(0.9941)\n",
      "117 tensor(0.) tensor(0.9941)\n",
      "values tensor(0.9941) tensor(0.9941)\n",
      "118 tensor(0.) tensor(1.0707)\n",
      "119 - tensor(1.0707) 0.9801 tensor(0.)\n",
      "120 - tensor(1.0707) 0.9702989999999999 tensor(0.)\n",
      "121 - tensor(1.0707) 0.96059601 tensor(0.)\n",
      "122 - tensor(1.0707) 0.9509900498999999 tensor(0.)\n",
      "123 - tensor(1.0707) 0.9414801494009999 tensor(0.)\n",
      "124 - tensor(1.0707) 0.9320653479069899 tensor(0.)\n",
      "125 - tensor(1.0707) 0.92274469442792 tensor(0.)\n",
      "126 - tensor(1.0707) 0.9135172474836407 tensor(0.)\n",
      "127 - tensor(1.0707) 0.9043820750088043 tensor(0.)\n",
      "values tensor(1.0707) tensor(0.9941)\n",
      "128 tensor(0.) tensor(0.8571)\n",
      "values tensor(0.8571) tensor(0.8571)\n",
      "129 tensor(0.) tensor(0.8571)\n",
      "values tensor(0.8571) tensor(0.8571)\n",
      "130 tensor(1.) tensor(0.9966)\n",
      "131 - tensor(1.9966) 0.9801 tensor(0.)\n",
      "132 - tensor(1.9966) 0.9702989999999999 tensor(0.)\n",
      "133 - tensor(1.9966) 0.96059601 tensor(0.)\n",
      "134 - tensor(1.9966) 0.9509900498999999 tensor(0.)\n",
      "135 - tensor(1.9966) 0.9414801494009999 tensor(0.)\n",
      "136 - tensor(1.9966) 0.9320653479069899 tensor(0.)\n",
      "137 - tensor(1.9966) 0.92274469442792 tensor(0.)\n",
      "138 - tensor(1.9966) 0.9135172474836407 tensor(0.)\n",
      "values tensor(1.9966) tensor(0.8571)\n",
      "139 tensor(0.) tensor(0.7360)\n",
      "values tensor(0.7360) tensor(0.7360)\n",
      "140 tensor(0.) tensor(0.9050)\n",
      "141 - tensor(0.9050) 0.9801 tensor(0.)\n",
      "142 - tensor(0.9050) 0.9702989999999999 tensor(0.)\n",
      "143 - tensor(0.9050) 0.96059601 tensor(0.)\n",
      "144 - tensor(0.9050) 0.9509900498999999 tensor(0.)\n",
      "145 - tensor(0.9050) 0.9414801494009999 tensor(0.)\n",
      "146 - tensor(0.9050) 0.9320653479069899 tensor(0.)\n",
      "values tensor(0.9050) tensor(0.7360)\n",
      "147 tensor(0.) tensor(0.5937)\n",
      "values tensor(0.5937) tensor(0.5937)\n",
      "148 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(0.5937)\n",
      "149 tensor(-1.) tensor(1.1885)\n",
      "values tensor(0.1885) tensor(1.1885)\n",
      "150 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "151 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "152 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "153 tensor(0.) tensor(0.7962)\n",
      "154 - tensor(0.7962) 0.9801 tensor(0.)\n",
      "155 - tensor(0.7962) 0.9702989999999999 tensor(0.)\n",
      "156 - tensor(0.7962) 0.96059601 tensor(0.)\n",
      "157 - tensor(0.7962) 0.9509900498999999 tensor(0.)\n",
      "158 - tensor(0.7962) 0.9414801494009999 tensor(0.)\n",
      "159 - tensor(0.7962) 0.9320653479069899 tensor(0.)\n",
      "160 - tensor(0.7962) 0.92274469442792 tensor(0.)\n",
      "161 - tensor(0.7962) 0.9135172474836407 tensor(0.)\n",
      "162 - tensor(0.7962) 0.9043820750088043 tensor(0.)\n",
      "163 - tensor(0.7962) 0.8953382542587163 tensor(0.)\n",
      "164 - tensor(0.7962) 0.8863848717161291 tensor(0.)\n",
      "165 - tensor(0.7962) 0.8775210229989678 tensor(0.)\n",
      "166 - tensor(0.7962) 0.8687458127689781 tensor(0.)\n",
      "167 - tensor(0.7962) 0.8600583546412883 tensor(0.)\n",
      "values tensor(0.7962) tensor(1.2440)\n",
      "168 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "169 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "170 tensor(0.) tensor(0.6688)\n",
      "171 - tensor(0.6688) 0.9801 tensor(0.)\n",
      "172 - tensor(0.6688) 0.9702989999999999 tensor(0.)\n",
      "173 - tensor(0.6688) 0.96059601 tensor(0.)\n",
      "values tensor(0.6688) tensor(1.2056)\n",
      "174 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "175 tensor(0.) tensor(0.7962)\n",
      "176 - tensor(0.7962) 0.9801 tensor(0.)\n",
      "177 - tensor(0.7962) 0.9702989999999999 tensor(0.)\n",
      "178 - tensor(0.7962) 0.96059601 tensor(0.)\n",
      "179 - tensor(0.7962) 0.9509900498999999 tensor(0.)\n",
      "180 - tensor(-0.1548) 0.9414801494009999 tensor(-1.)\n",
      "181 - tensor(-0.1548) 0.9320653479069899 tensor(0.)\n",
      "182 - tensor(-0.1548) 0.92274469442792 tensor(0.)\n",
      "183 - tensor(-0.1548) 0.9135172474836407 tensor(0.)\n",
      "184 - tensor(-0.1548) 0.9043820750088043 tensor(0.)\n",
      "185 - tensor(-0.1548) 0.8953382542587163 tensor(0.)\n",
      "186 - tensor(-0.1548) 0.8863848717161291 tensor(0.)\n",
      "187 - tensor(-0.1548) 0.8775210229989678 tensor(0.)\n",
      "188 - tensor(-0.1548) 0.8687458127689781 tensor(0.)\n",
      "values tensor(-0.1548) tensor(1.1102)\n",
      "189 tensor(0.) tensor(1.0430)\n",
      "values tensor(1.0430) tensor(1.0430)\n",
      "190 tensor(0.) tensor(1.1251)\n",
      "191 - tensor(1.1251) 0.9801 tensor(0.)\n",
      "192 - tensor(1.1251) 0.9702989999999999 tensor(0.)\n",
      "193 - tensor(1.1251) 0.96059601 tensor(0.)\n",
      "194 - tensor(1.1251) 0.9509900498999999 tensor(0.)\n",
      "195 - tensor(1.1251) 0.9414801494009999 tensor(0.)\n",
      "196 - tensor(1.1251) 0.9320653479069899 tensor(0.)\n",
      "197 - tensor(1.1251) 0.92274469442792 tensor(0.)\n",
      "198 - tensor(1.1251) 0.9135172474836407 tensor(0.)\n",
      "199 - tensor(1.1251) 0.9043820750088043 tensor(0.)\n",
      "200 - tensor(1.1251) 0.8953382542587163 tensor(0.)\n",
      "201 - tensor(1.1251) 0.8863848717161291 tensor(0.)\n",
      "values tensor(1.1251) tensor(1.0430)\n",
      "202 tensor(0.) tensor(0.9588)\n",
      "values tensor(0.9588) tensor(0.9588)\n",
      "203 tensor(0.) tensor(1.0707)\n",
      "204 - tensor(1.0707) 0.9801 tensor(0.)\n",
      "205 - tensor(1.0707) 0.9702989999999999 tensor(0.)\n",
      "206 - tensor(1.0707) 0.96059601 tensor(0.)\n",
      "207 - tensor(1.0707) 0.9509900498999999 tensor(0.)\n",
      "208 - tensor(1.0707) 0.9414801494009999 tensor(0.)\n",
      "209 - tensor(1.0707) 0.9320653479069899 tensor(0.)\n",
      "210 - tensor(1.0707) 0.92274469442792 tensor(0.)\n",
      "211 - tensor(1.0707) 0.9135172474836407 tensor(0.)\n",
      "212 - tensor(1.0707) 0.9043820750088043 tensor(0.)\n",
      "213 - tensor(1.0707) 0.8953382542587163 tensor(0.)\n",
      "214 - tensor(1.0707) 0.8863848717161291 tensor(0.)\n",
      "215 - tensor(1.0707) 0.8775210229989678 tensor(0.)\n",
      "216 - tensor(1.0707) 0.8687458127689781 tensor(0.)\n",
      "217 - tensor(1.0707) 0.8600583546412883 tensor(0.)\n",
      "values tensor(1.0707) tensor(0.9588)\n",
      "218 tensor(0.) tensor(0.8944)\n",
      "values tensor(0.8944) tensor(0.8571)\n",
      "219 tensor(0.) tensor(0.8944)\n",
      "values tensor(0.8944) tensor(0.8944)\n",
      "220 tensor(-1.) tensor(1.1251)\n",
      "221 - tensor(0.1251) 0.9801 tensor(0.)\n",
      "222 - tensor(0.1251) 0.9702989999999999 tensor(0.)\n",
      "223 - tensor(0.1251) 0.96059601 tensor(0.)\n",
      "224 - tensor(0.1251) 0.9509900498999999 tensor(0.)\n",
      "225 - tensor(0.1251) 0.9414801494009999 tensor(0.)\n",
      "226 - tensor(0.1251) 0.9320653479069899 tensor(0.)\n",
      "227 - tensor(0.1251) 0.92274469442792 tensor(0.)\n",
      "228 - tensor(0.1251) 0.9135172474836407 tensor(0.)\n",
      "229 - tensor(0.1251) 0.9043820750088043 tensor(0.)\n",
      "230 - tensor(0.1251) 0.8953382542587163 tensor(0.)\n",
      "231 - tensor(0.1251) 0.8863848717161291 tensor(0.)\n",
      "values tensor(0.1251) tensor(0.8944)\n",
      "232 tensor(0.) tensor(1.1967)\n",
      "values tensor(1.1967) tensor(0.7360)\n",
      "233 tensor(0.) tensor(1.2632)\n",
      "values tensor(1.2632) tensor(1.1967)\n",
      "234 tensor(0.) tensor(1.2632)\n",
      "values tensor(1.2632) tensor(1.2632)\n",
      "235 tensor(0.) tensor(1.2632)\n",
      "values tensor(1.2632) tensor(1.2632)\n",
      "236 tensor(0.) tensor(1.2632)\n",
      "values tensor(1.2632) tensor(1.2632)\n",
      "237 tensor(1.) tensor(1.2632)\n",
      "values tensor(2.2632) tensor(1.2632)\n",
      "238 tensor(0.) tensor(1.3348)\n",
      "values tensor(1.3348) tensor(1.2632)\n",
      "239 tensor(0.) tensor(1.1251)\n",
      "240 - tensor(1.1251) 0.9801 tensor(0.)\n",
      "241 - tensor(1.1251) 0.9702989999999999 tensor(0.)\n",
      "242 - tensor(1.1251) 0.96059601 tensor(0.)\n",
      "243 - tensor(1.1251) 0.9509900498999999 tensor(0.)\n",
      "244 - tensor(1.1251) 0.9414801494009999 tensor(0.)\n",
      "245 - tensor(1.1251) 0.9320653479069899 tensor(0.)\n",
      "246 - tensor(1.1251) 0.92274469442792 tensor(0.)\n",
      "247 - tensor(1.1251) 0.9135172474836407 tensor(0.)\n",
      "248 - tensor(1.1251) 0.9043820750088043 tensor(0.)\n",
      "values tensor(1.1251) tensor(1.3348)\n",
      "249 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "250 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "251 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "252 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "253 tensor(0.) tensor(1.0707)\n",
      "254 - tensor(1.0707) 0.9801 tensor(0.)\n",
      "255 - tensor(1.0707) 0.9702989999999999 tensor(0.)\n",
      "256 - tensor(1.0707) 0.96059601 tensor(0.)\n",
      "257 - tensor(1.0707) 0.9509900498999999 tensor(0.)\n",
      "258 - tensor(1.0707) 0.9414801494009999 tensor(0.)\n",
      "259 - tensor(1.0707) 0.9320653479069899 tensor(0.)\n",
      "260 - tensor(1.0707) 0.92274469442792 tensor(0.)\n",
      "261 - tensor(1.0707) 0.9135172474836407 tensor(0.)\n",
      "262 - tensor(1.0707) 0.9043820750088043 tensor(0.)\n",
      "263 - tensor(1.0707) 0.8953382542587163 tensor(0.)\n",
      "264 - tensor(1.0707) 0.8863848717161291 tensor(0.)\n",
      "values tensor(1.0707) tensor(1.2440)\n",
      "265 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1593)\n",
      "266 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "267 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "268 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "269 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "270 tensor(0.) tensor(1.3055)\n",
      "values tensor(1.3055) tensor(1.2440)\n",
      "271 tensor(0.) tensor(1.3055)\n",
      "values tensor(1.3055) tensor(1.3055)\n",
      "272 tensor(0.) tensor(0.9966)\n",
      "273 - tensor(0.9966) 0.9801 tensor(0.)\n",
      "274 - tensor(0.9966) 0.9702989999999999 tensor(0.)\n",
      "275 - tensor(0.9966) 0.96059601 tensor(0.)\n",
      "276 - tensor(0.9966) 0.9509900498999999 tensor(0.)\n",
      "277 - tensor(0.9966) 0.9414801494009999 tensor(0.)\n",
      "278 - tensor(0.9966) 0.9320653479069899 tensor(0.)\n",
      "279 - tensor(0.9966) 0.92274469442792 tensor(0.)\n",
      "280 - tensor(0.9966) 0.9135172474836407 tensor(0.)\n",
      "281 - tensor(0.9966) 0.9043820750088043 tensor(0.)\n",
      "282 - tensor(0.9966) 0.8953382542587163 tensor(0.)\n",
      "283 - tensor(0.9966) 0.8863848717161291 tensor(0.)\n",
      "284 - tensor(0.9966) 0.8775210229989678 tensor(0.)\n",
      "285 - tensor(0.9966) 0.8687458127689781 tensor(0.)\n",
      "286 - tensor(0.9966) 0.8600583546412883 tensor(0.)\n",
      "values tensor(0.9966) tensor(1.3055)\n",
      "287 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "288 tensor(0.) tensor(1.0461)\n",
      "289 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "290 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "291 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "292 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "293 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "294 - tensor(1.0461) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.0461) tensor(1.2056)\n",
      "295 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1102)\n",
      "296 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "297 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "298 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "299 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "300 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2056)\n",
      "301 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2575)\n",
      "302 tensor(0.) tensor(1.0461)\n",
      "303 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "values tensor(1.0461) tensor(1.2575)\n",
      "304 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1102)\n",
      "305 tensor(0.) tensor(0.9699)\n",
      "306 - tensor(0.9699) 0.9801 tensor(0.)\n",
      "307 - tensor(0.9699) 0.9702989999999999 tensor(0.)\n",
      "values tensor(0.9699) tensor(1.1593)\n",
      "308 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "309 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "310 tensor(0.) tensor(1.1498)\n",
      "values tensor(1.1498) tensor(1.1102)\n",
      "311 tensor(0.) tensor(1.1498)\n",
      "values tensor(1.1498) tensor(1.1498)\n",
      "312 tensor(0.) tensor(1.0461)\n",
      "313 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "314 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "315 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "316 - tensor(0.0855) 0.9509900498999999 tensor(-1.)\n",
      "317 - tensor(0.0855) 0.9414801494009999 tensor(0.)\n",
      "values tensor(0.0855) tensor(1.1498)\n",
      "318 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.0430)\n",
      "319 tensor(-1.) tensor(1.1498)\n",
      "values tensor(0.1498) tensor(1.1102)\n",
      "320 tensor(0.) tensor(1.1498)\n",
      "values tensor(1.1498) tensor(1.1498)\n",
      "321 tensor(0.) tensor(0.9699)\n",
      "322 - tensor(0.9699) 0.9801 tensor(0.)\n",
      "323 - tensor(0.9699) 0.9702989999999999 tensor(0.)\n",
      "324 - tensor(0.9699) 0.96059601 tensor(0.)\n",
      "values tensor(0.9699) tensor(1.1498)\n",
      "325 tensor(0.) tensor(1.0430)\n",
      "values tensor(1.0430) tensor(1.0430)\n",
      "326 tensor(0.) tensor(1.0430)\n",
      "values tensor(1.0430) tensor(1.0430)\n",
      "327 tensor(0.) tensor(1.0430)\n",
      "values tensor(1.0430) tensor(1.0430)\n",
      "328 tensor(0.) tensor(1.0461)\n",
      "329 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "330 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "331 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "332 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "333 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "334 - tensor(1.0461) 0.9320653479069899 tensor(0.)\n",
      "335 - tensor(1.0461) 0.92274469442792 tensor(0.)\n",
      "336 - tensor(1.0461) 0.9135172474836407 tensor(0.)\n",
      "values tensor(1.0461) tensor(1.0430)\n",
      "337 tensor(0.) tensor(0.9941)\n",
      "values tensor(0.9941) tensor(0.9588)\n",
      "338 tensor(0.) tensor(1.0273)\n",
      "values tensor(1.0273) tensor(0.9941)\n",
      "339 tensor(0.) tensor(1.1014)\n",
      "340 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "341 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "342 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.0273)\n",
      "343 tensor(0.) tensor(1.0461)\n",
      "344 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "345 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "346 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "347 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "values tensor(1.0461) tensor(0.8571)\n",
      "348 tensor(0.) tensor(0.9588)\n",
      "values tensor(0.9588) tensor(0.7360)\n",
      "349 tensor(0.) tensor(0.9941)\n",
      "values tensor(0.9941) tensor(0.9588)\n",
      "350 tensor(0.) tensor(1.0273)\n",
      "values tensor(1.0273) tensor(0.9941)\n",
      "351 tensor(0.) tensor(1.1014)\n",
      "352 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "353 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "354 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "355 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "356 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "357 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "358 - tensor(1.1014) 0.92274469442792 tensor(0.)\n",
      "359 - tensor(0.1786) 0.9135172474836407 tensor(-1.)\n",
      "360 - tensor(0.1786) 0.9043820750088043 tensor(0.)\n",
      "361 - tensor(0.1786) 0.8953382542587163 tensor(0.)\n",
      "362 - tensor(0.1786) 0.8863848717161291 tensor(0.)\n",
      "values tensor(0.1786) tensor(1.0273)\n",
      "363 tensor(0.) tensor(0.8571)\n",
      "values tensor(0.8571) tensor(0.8571)\n",
      "364 tensor(0.) tensor(0.8571)\n",
      "values tensor(0.8571) tensor(0.8571)\n",
      "365 tensor(0.) tensor(0.8944)\n",
      "values tensor(0.8944) tensor(0.8571)\n",
      "366 tensor(0.) tensor(1.1014)\n",
      "367 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "368 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "369 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "370 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "371 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "372 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "373 - tensor(1.1014) 0.92274469442792 tensor(0.)\n",
      "374 - tensor(1.1014) 0.9135172474836407 tensor(0.)\n",
      "values tensor(1.1014) tensor(0.8944)\n",
      "375 tensor(0.) tensor(0.7360)\n",
      "values tensor(0.7360) tensor(0.7360)\n",
      "376 tensor(0.) tensor(0.7360)\n",
      "values tensor(0.7360) tensor(0.7360)\n",
      "377 tensor(0.) tensor(0.7360)\n",
      "values tensor(0.7360) tensor(0.7360)\n",
      "378 tensor(0.) tensor(0.7774)\n",
      "values tensor(0.7774) tensor(0.7360)\n",
      "379 tensor(0.) tensor(0.7774)\n",
      "values tensor(0.7774) tensor(0.7774)\n",
      "380 tensor(0.) tensor(0.7774)\n",
      "values tensor(0.7774) tensor(0.7774)\n",
      "381 tensor(0.) tensor(1.1014)\n",
      "382 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "values tensor(1.1014) tensor(0.7774)\n",
      "383 tensor(0.) tensor(0.5937)\n",
      "values tensor(0.5937) tensor(0.5937)\n",
      "384 tensor(0.) tensor(1.0461)\n",
      "385 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "386 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "387 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "388 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "values tensor(1.0461) tensor(0.5937)\n",
      "389 tensor(0.) tensor(0.4294)\n",
      "values tensor(0.4294) tensor(0.4294)\n",
      "390 tensor(0.) tensor(0.4785)\n",
      "values tensor(0.4785) tensor(0.4294)\n",
      "391 tensor(0.) tensor(0.4785)\n",
      "values tensor(0.4785) tensor(0.4785)\n",
      "392 tensor(0.) tensor(0.5210)\n",
      "values tensor(0.5210) tensor(0.4785)\n",
      "393 tensor(0.) tensor(0.5210)\n",
      "values tensor(0.5210) tensor(0.5210)\n",
      "394 tensor(0.) tensor(1.1014)\n",
      "395 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "396 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "397 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "398 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "399 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "400 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "401 - tensor(1.1014) 0.92274469442792 tensor(0.)\n",
      "402 - tensor(1.1014) 0.9135172474836407 tensor(0.)\n",
      "403 - tensor(1.1014) 0.9043820750088043 tensor(0.)\n",
      "values tensor(1.1014) tensor(0.5210)\n",
      "404 tensor(0.) tensor(0.2916)\n",
      "values tensor(0.2916) tensor(0.2430)\n",
      "405 tensor(0.) tensor(0.2916)\n",
      "values tensor(0.2916) tensor(0.2916)\n",
      "406 tensor(0.) tensor(1.0461)\n",
      "407 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "408 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "409 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "410 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "411 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "412 - tensor(1.0461) 0.9320653479069899 tensor(0.)\n",
      "413 - tensor(1.0461) 0.92274469442792 tensor(0.)\n",
      "values tensor(1.0461) tensor(0.2916)\n",
      "414 tensor(0.) tensor(1.1014)\n",
      "415 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "416 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "417 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "418 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "419 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "420 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.1014) tensor(0.0340)\n",
      "421 tensor(0.) tensor(-0.1985)\n",
      "values tensor(-0.1985) tensor(-0.1985)\n",
      "422 tensor(0.) tensor(-0.1985)\n",
      "values tensor(-0.1985) tensor(-0.1985)\n",
      "423 tensor(0.) tensor(-0.1622)\n",
      "values tensor(-0.1622) tensor(-0.1985)\n",
      "424 tensor(0.) tensor(-0.1622)\n",
      "values tensor(-0.1622) tensor(-0.1622)\n",
      "425 tensor(0.) tensor(-0.1187)\n",
      "values tensor(-0.1187) tensor(-0.1622)\n",
      "426 tensor(0.) tensor(-0.1187)\n",
      "values tensor(-0.1187) tensor(-0.1187)\n",
      "427 tensor(0.) tensor(1.1014)\n",
      "428 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "429 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "430 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "431 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "432 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "433 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.1014) tensor(-0.1187)\n",
      "434 tensor(0.) tensor(1.1014)\n",
      "435 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "436 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "437 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "values tensor(1.1014) tensor(-0.4548)\n",
      "438 tensor(0.) tensor(-0.7160)\n",
      "values tensor(-0.7160) tensor(-0.7335)\n",
      "439 tensor(0.) tensor(-0.6822)\n",
      "values tensor(-0.6822) tensor(-0.7160)\n",
      "440 tensor(0.) tensor(-0.6822)\n",
      "values tensor(-0.6822) tensor(-0.6822)\n",
      "441 tensor(0.) tensor(-0.6822)\n",
      "values tensor(-0.6822) tensor(-0.6822)\n",
      "442 tensor(0.) tensor(1.1014)\n",
      "443 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "444 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "445 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "446 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "447 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "values tensor(1.1014) tensor(-0.6822)\n",
      "448 tensor(0.) tensor(-1.0296)\n",
      "values tensor(-1.0296) tensor(-1.0296)\n",
      "449 tensor(0.) tensor(-1.0296)\n",
      "values tensor(-1.0296) tensor(-1.0296)\n",
      "450 tensor(0.) tensor(-1.0296)\n",
      "values tensor(-1.0296) tensor(-1.0296)\n",
      "451 tensor(0.) tensor(-1.0202)\n",
      "values tensor(-1.0202) tensor(-1.0296)\n",
      "452 tensor(0.) tensor(1.0461)\n",
      "453 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "454 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "455 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "456 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "457 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "458 - tensor(1.0461) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.0461) tensor(-1.0202)\n",
      "459 tensor(0.) tensor(-1.3337)\n",
      "values tensor(-1.3337) tensor(-1.3337)\n",
      "460 tensor(0.) tensor(-1.3337)\n",
      "values tensor(-1.3337) tensor(-1.3337)\n",
      "461 tensor(0.) tensor(1.0461)\n",
      "462 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "463 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "464 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "465 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "466 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "467 - tensor(1.0461) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.0461) tensor(-1.3337)\n",
      "468 tensor(-1.) tensor(0.5937)\n",
      "values tensor(-0.4063) tensor(-1.6339)\n",
      "469 tensor(1.) tensor(1.1593)\n",
      "values tensor(2.1593) tensor(0.5937)\n",
      "470 tensor(0.) tensor(1.1014)\n",
      "471 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "472 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "473 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "474 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "475 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "476 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.1593)\n",
      "477 tensor(0.) tensor(1.1498)\n",
      "values tensor(1.1498) tensor(1.1102)\n",
      "478 tensor(0.) tensor(1.1014)\n",
      "479 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "480 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "481 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "482 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "483 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "484 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.1498)\n",
      "485 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0430)\n",
      "486 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0790)\n",
      "487 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0790)\n",
      "488 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0790)\n",
      "489 tensor(0.) tensor(1.0790)\n",
      "values tensor(1.0790) tensor(1.0790)\n",
      "490 tensor(0.) tensor(1.1165)\n",
      "values tensor(1.1165) tensor(1.0790)\n",
      "491 tensor(0.) tensor(1.1165)\n",
      "values tensor(1.1165) tensor(1.1165)\n",
      "492 tensor(0.) tensor(1.1165)\n",
      "values tensor(1.1165) tensor(1.1165)\n",
      "493 tensor(0.) tensor(1.1014)\n",
      "494 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "495 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "496 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "497 - tensor(0.1408) 0.9509900498999999 tensor(-1.)\n",
      "498 - tensor(0.1408) 0.9414801494009999 tensor(0.)\n",
      "499 - tensor(0.1408) 0.9320653479069899 tensor(0.)\n",
      "500 - tensor(0.1408) 0.92274469442792 tensor(0.)\n",
      "values tensor(0.1408) tensor(1.1165)\n",
      "501 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(0.9588)\n",
      "502 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "503 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "504 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1102)\n",
      "505 tensor(1.) tensor(1.1593)\n",
      "values tensor(2.1593) tensor(1.1593)\n",
      "506 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "507 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "508 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2056)\n",
      "509 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2575)\n",
      "510 tensor(0.) tensor(1.1014)\n",
      "511 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "512 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "513 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "514 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "515 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "516 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "517 - tensor(1.1014) 0.92274469442792 tensor(0.)\n",
      "518 - tensor(0.1786) 0.9135172474836407 tensor(-1.)\n",
      "519 - tensor(0.1786) 0.9043820750088043 tensor(0.)\n",
      "520 - tensor(0.1786) 0.8953382542587163 tensor(0.)\n",
      "521 - tensor(0.1786) 0.8863848717161291 tensor(0.)\n",
      "values tensor(0.1786) tensor(1.2575)\n",
      "522 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1102)\n",
      "523 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "524 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "525 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2056)\n",
      "526 tensor(0.) tensor(1.1014)\n",
      "527 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.2575)\n",
      "528 tensor(-1.) tensor(1.1102)\n",
      "values tensor(0.1102) tensor(1.1102)\n",
      "529 tensor(0.) tensor(1.0461)\n",
      "530 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "531 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "532 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "533 - tensor(1.0461) 0.9509900498999999 tensor(0.)\n",
      "534 - tensor(1.0461) 0.9414801494009999 tensor(0.)\n",
      "values tensor(1.0461) tensor(1.1102)\n",
      "535 tensor(0.) tensor(1.1014)\n",
      "536 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "537 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "538 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "539 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "540 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "541 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "542 - tensor(1.1014) 0.92274469442792 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.0430)\n",
      "543 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(0.9588)\n",
      "544 tensor(0.) tensor(1.1102)\n",
      "values tensor(1.1102) tensor(1.1102)\n",
      "545 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1102)\n",
      "546 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "547 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "548 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "549 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "550 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "551 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "552 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "553 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "554 tensor(0.) tensor(1.3055)\n",
      "values tensor(1.3055) tensor(1.2440)\n",
      "555 tensor(0.) tensor(1.1014)\n",
      "556 - tensor(1.1014) 0.9801 tensor(0.)\n",
      "557 - tensor(1.1014) 0.9702989999999999 tensor(0.)\n",
      "558 - tensor(1.1014) 0.96059601 tensor(0.)\n",
      "559 - tensor(1.1014) 0.9509900498999999 tensor(0.)\n",
      "560 - tensor(1.1014) 0.9414801494009999 tensor(0.)\n",
      "561 - tensor(1.1014) 0.9320653479069899 tensor(0.)\n",
      "values tensor(1.1014) tensor(1.3055)\n",
      "562 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1593)\n",
      "563 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "564 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "565 tensor(0.) tensor(1.1885)\n",
      "values tensor(1.1885) tensor(1.1885)\n",
      "566 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.1885)\n",
      "567 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "568 tensor(0.) tensor(1.2440)\n",
      "values tensor(1.2440) tensor(1.2440)\n",
      "569 tensor(0.) tensor(1.0461)\n",
      "570 - tensor(1.0461) 0.9801 tensor(0.)\n",
      "571 - tensor(1.0461) 0.9702989999999999 tensor(0.)\n",
      "572 - tensor(1.0461) 0.96059601 tensor(0.)\n",
      "values tensor(1.0461) tensor(1.2440)\n",
      "573 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "574 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "575 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "576 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "577 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "578 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2056)\n",
      "579 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2575)\n",
      "580 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2575)\n",
      "581 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2575)\n",
      "582 tensor(0.) tensor(0.9699)\n",
      "583 - tensor(0.9699) 0.9801 tensor(0.)\n",
      "584 - tensor(0.9699) 0.9702989999999999 tensor(0.)\n",
      "585 - tensor(0.9699) 0.96059601 tensor(0.)\n",
      "586 - tensor(0.9699) 0.9509900498999999 tensor(0.)\n",
      "587 - tensor(0.9699) 0.9414801494009999 tensor(0.)\n",
      "588 - tensor(0.9699) 0.9320653479069899 tensor(0.)\n",
      "589 - tensor(0.9699) 0.92274469442792 tensor(0.)\n",
      "590 - tensor(0.9699) 0.9135172474836407 tensor(0.)\n",
      "591 - tensor(0.0564) 0.9043820750088043 tensor(-1.)\n",
      "592 - tensor(0.0564) 0.8953382542587163 tensor(0.)\n",
      "593 - tensor(0.0564) 0.8863848717161291 tensor(0.)\n",
      "594 - tensor(0.0564) 0.8775210229989678 tensor(0.)\n",
      "values tensor(0.0564) tensor(1.2575)\n",
      "595 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1102)\n",
      "596 tensor(0.) tensor(1.1593)\n",
      "values tensor(1.1593) tensor(1.1593)\n",
      "597 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.1593)\n",
      "598 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "599 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "600 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "601 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "602 tensor(0.) tensor(1.2056)\n",
      "values tensor(1.2056) tensor(1.2056)\n",
      "603 tensor(0.) tensor(1.2575)\n",
      "values tensor(1.2575) tensor(1.2056)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rew = rewards[episodes == 0]\n",
    "rew_2 = np.copy(rew)\n",
    "\n",
    "value_copy = values[episodes == 0]\n",
    "value_copy_next = np.copy(value_copy)\n",
    "\n",
    "age = 0\n",
    "\n",
    "gam =0.99\n",
    "gam_1 = gam\n",
    "\n",
    "\n",
    "\n",
    "values_2 = values[episodes == 0]\n",
    "\n",
    "epi_list = np.arange(sum(episodes == 0))[current_agent[episodes == 0,0] == (age+1)]\n",
    "\n",
    "for epi,edi in enumerate(epi_list[:-1]):\n",
    "    gam_1 = gam\n",
    "\n",
    "    v_add = rewards[epi,age]+values[edi+1]\n",
    "    print(edi,rewards[epi,age],values[edi+1])\n",
    "    if epi_list[epi+1] -1 != epi_list[epi]:\n",
    "        for i in range(epi_list[epi]+1,epi_list[epi+1]):\n",
    "            \n",
    "            v_add += gam_1*rewards[i,age]\n",
    "            gam_1*=gam\n",
    "            print(i,\"-\",v_add,gam_1,rewards[i,age])\n",
    "    \n",
    "    values_2[edi] = v_add \n",
    "    print('values',values_2[edi],values[edi])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "id": "5fcaff0e-1f3e-4c4f-ac2e-f7720951b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#values_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "id": "40bec742-a266-4504-b71a-14e43d372a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 True\n",
      "1 1 True\n",
      "1 2 True\n",
      "1 3 True\n",
      "1 4 True\n",
      "1 5 True\n",
      "1 6 True\n",
      "1 7 True\n",
      "1 8 True\n",
      "1 9 True\n",
      "2 10 False\n",
      "2 11 False\n",
      "3 12 False\n",
      "3 13 False\n",
      "3 14 False\n",
      "3 15 False\n",
      "3 16 False\n",
      "3 17 False\n",
      "3 18 False\n",
      "1 19 True\n",
      "1 20 True\n",
      "1 21 True\n",
      "1 22 True\n",
      "1 23 True\n",
      "1 24 True\n",
      "2 25 False\n",
      "2 26 False\n",
      "2 27 False\n",
      "2 28 False\n",
      "2 29 False\n",
      "2 30 False\n",
      "2 31 False\n",
      "3 32 False\n",
      "3 33 False\n",
      "3 34 False\n",
      "3 35 False\n",
      "3 36 False\n",
      "3 37 False\n",
      "3 38 False\n",
      "3 39 False\n",
      "3 40 False\n",
      "1 41 True\n",
      "1 42 True\n",
      "1 43 True\n",
      "1 44 True\n",
      "2 45 False\n",
      "2 46 False\n",
      "2 47 False\n",
      "2 48 False\n",
      "2 49 False\n",
      "3 50 False\n",
      "3 51 False\n",
      "3 52 False\n",
      "3 53 False\n",
      "3 54 False\n",
      "3 55 False\n",
      "3 56 False\n",
      "3 57 False\n",
      "3 58 False\n",
      "3 59 False\n",
      "3 60 False\n",
      "3 61 False\n",
      "3 62 False\n",
      "1 63 True\n",
      "1 64 True\n",
      "1 65 True\n",
      "1 66 True\n",
      "1 67 True\n",
      "1 68 True\n",
      "1 69 True\n",
      "1 70 True\n",
      "1 71 True\n",
      "2 72 False\n",
      "2 73 False\n",
      "2 74 False\n",
      "2 75 False\n",
      "3 76 False\n",
      "1 77 True\n",
      "1 78 True\n",
      "1 79 True\n",
      "1 80 True\n",
      "1 81 True\n",
      "2 82 False\n",
      "3 83 False\n",
      "3 84 False\n",
      "3 85 False\n",
      "3 86 False\n",
      "3 87 False\n",
      "3 88 False\n",
      "1 89 True\n",
      "1 90 True\n",
      "1 91 True\n",
      "1 92 True\n",
      "1 93 True\n",
      "1 94 True\n",
      "1 95 True\n",
      "1 96 True\n",
      "2 97 False\n",
      "2 98 False\n",
      "2 99 False\n",
      "2 100 False\n",
      "2 101 False\n",
      "3 102 False\n",
      "3 103 False\n",
      "3 104 False\n",
      "1 105 True\n",
      "1 106 True\n",
      "1 107 True\n",
      "1 108 True\n",
      "1 109 True\n",
      "1 110 True\n",
      "2 111 False\n",
      "2 112 False\n",
      "2 113 False\n",
      "2 114 False\n",
      "2 115 False\n",
      "2 116 False\n",
      "2 117 False\n",
      "2 118 False\n",
      "3 119 False\n",
      "3 120 False\n",
      "3 121 False\n",
      "3 122 False\n",
      "3 123 False\n",
      "3 124 False\n",
      "1 125 True\n",
      "1 126 True\n",
      "1 127 True\n",
      "1 128 True\n",
      "2 129 False\n",
      "2 130 False\n",
      "2 131 False\n",
      "2 132 False\n",
      "2 133 False\n",
      "2 134 False\n",
      "2 135 False\n",
      "2 136 False\n",
      "3 137 False\n",
      "3 138 False\n",
      "3 139 False\n",
      "3 140 False\n",
      "3 141 False\n",
      "3 142 False\n",
      "1 143 True\n",
      "1 144 True\n",
      "1 145 True\n",
      "1 146 True\n",
      "1 147 True\n",
      "1 148 True\n",
      "2 149 False\n",
      "2 150 False\n",
      "2 151 False\n",
      "2 152 False\n",
      "2 153 False\n",
      "2 154 False\n",
      "2 155 False\n",
      "2 156 False\n",
      "2 157 False\n",
      "3 158 False\n",
      "3 159 False\n",
      "3 160 False\n",
      "3 161 False\n",
      "3 162 False\n",
      "3 163 False\n",
      "3 164 False\n",
      "3 165 False\n",
      "3 166 False\n",
      "3 167 False\n",
      "1 168 True\n",
      "1 169 True\n",
      "1 170 True\n",
      "1 171 True\n",
      "1 172 True\n",
      "1 173 True\n",
      "2 174 False\n",
      "2 175 False\n",
      "2 176 False\n",
      "2 177 False\n",
      "2 178 False\n",
      "2 179 False\n",
      "2 180 False\n",
      "2 181 False\n",
      "2 182 False\n",
      "3 183 False\n",
      "3 184 False\n",
      "3 185 False\n",
      "3 186 False\n",
      "3 187 False\n",
      "3 188 False\n",
      "3 189 False\n",
      "3 190 False\n",
      "3 191 False\n",
      "3 192 False\n",
      "1 193 True\n",
      "2 194 False\n",
      "2 195 False\n",
      "2 196 False\n",
      "3 197 False\n",
      "3 198 False\n",
      "3 199 False\n",
      "3 200 False\n",
      "3 201 False\n",
      "3 202 False\n",
      "3 203 False\n",
      "3 204 False\n",
      "3 205 False\n",
      "1 206 True\n",
      "2 207 False\n",
      "2 208 False\n",
      "2 209 False\n",
      "2 210 False\n",
      "2 211 False\n",
      "3 212 False\n",
      "3 213 False\n",
      "3 214 False\n",
      "3 215 False\n",
      "3 216 False\n",
      "3 217 False\n",
      "3 218 False\n",
      "3 219 False\n",
      "3 220 False\n",
      "3 221 False\n",
      "3 222 False\n",
      "3 223 False\n",
      "3 224 False\n",
      "3 225 False\n",
      "1 226 True\n",
      "1 227 True\n",
      "1 228 True\n",
      "3 229 False\n",
      "3 230 False\n",
      "1 231 True\n",
      "1 232 True\n",
      "1 233 True\n",
      "1 234 True\n",
      "1 235 True\n",
      "1 236 True\n",
      "3 237 False\n",
      "3 238 False\n",
      "3 239 False\n",
      "3 240 False\n",
      "3 241 False\n",
      "3 242 False\n",
      "3 243 False\n",
      "3 244 False\n",
      "3 245 False\n",
      "3 246 False\n",
      "3 247 False\n",
      "3 248 False\n",
      "3 249 False\n",
      "1 250 True\n",
      "1 251 True\n",
      "3 252 False\n",
      "3 253 False\n",
      "3 254 False\n",
      "3 255 False\n",
      "3 256 False\n",
      "3 257 False\n",
      "3 258 False\n",
      "1 259 True\n",
      "1 260 True\n",
      "1 261 True\n",
      "1 262 True\n",
      "1 263 True\n",
      "1 264 True\n",
      "3 265 False\n",
      "3 266 False\n",
      "3 267 False\n",
      "3 268 False\n",
      "3 269 False\n",
      "3 270 False\n",
      "1 271 True\n",
      "3 272 False\n",
      "3 273 False\n",
      "3 274 False\n",
      "3 275 False\n",
      "3 276 False\n",
      "3 277 False\n"
     ]
    }
   ],
   "source": [
    "rew = rewards[episodes == 0]\n",
    "rew_2 = np.copy(rew)\n",
    "\n",
    "value_copy = values[episodes == 0]\n",
    "value_copy_next = np.copy(value_copy)\n",
    "\n",
    "age = 1\n",
    "\n",
    "gam =0.99\n",
    "gam_1 = gam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(sum(episodes == 0)):\n",
    "    \n",
    "    print(current_agent[i].numpy()[0].astype(int),i, current_agent[i].numpy()[0].astype(int)==age,   )\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1447,
   "id": "3428d0f5-c322-432e-a954-3148ac930ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22340"
      ]
     },
     "execution_count": 1447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d999373-ea20-4948-b785-0cbadca20d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "id": "7e69b6da-e0db-4ab7-afda-60ca4af3c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "id": "aa40b83b-72f9-4778-a297-3bbdcb342ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 1389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['action_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "d7681ee2-ed45-4545-a6bc-3d094f2ac7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19.0000,  0.4983]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 1343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "6d95c5de-138a-4332-b693-fd4fc856ae89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "SPS: 1902\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "SPS: 1898\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/9q_38sj961n1slqfmcpfjgnc0000gn/T/ipykernel_22511/208731411.py:277: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711438669293/work/aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (32, 7)) of distribution Categorical(logits: torch.Size([32, 7])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SubBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[801], line 263\u001b[0m\n\u001b[1;32m    260\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mminibatch_size\n\u001b[1;32m    261\u001b[0m mb_inds \u001b[38;5;241m=\u001b[39m b_inds[start:end]\n\u001b[0;32m--> 263\u001b[0m _, newlogprob, entropy, newvalue \u001b[38;5;241m=\u001b[39m \u001b[43magent_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_obs_a\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# b_obs[mb_inds].reshape(-1,np.prod(ob_space_shape)),#\u001b[39;49;00m\n\u001b[1;32m    264\u001b[0m \u001b[43m                                                                  \u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mb_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m logratio \u001b[38;5;241m=\u001b[39m newlogprob \u001b[38;5;241m-\u001b[39m b_logprobs[mb_inds]\n\u001b[1;32m    267\u001b[0m ratio \u001b[38;5;241m=\u001b[39m logratio\u001b[38;5;241m.\u001b[39mexp()\n",
      "Cell \u001b[0;32mIn[765], line 172\u001b[0m, in \u001b[0;36mAgent_shared_v1.get_action_and_value\u001b[0;34m(self, x, action)\u001b[0m\n\u001b[1;32m    170\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(x)\n\u001b[1;32m    171\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(hidden)\n\u001b[0;32m--> 172\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     action \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_project/lib/python3.12/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DL_project/lib/python3.12/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (32, 7)) of distribution Categorical(logits: torch.Size([32, 7])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan]], grad_fn=<SubBackward0>)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for iteration in range(1, 5000):#args.num_iterations) :\n",
    "    if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / 10000#args.num_iterations\n",
    "            lrnow = frac * lr#args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    step = 0\n",
    "    for episode in range(num_episodes):\n",
    "    \n",
    "        action=1\n",
    "        if fault_condition:\n",
    "            env = env_risk(**(env_config | {\"render_mode\" : None}))#game.env(render_mode=None)\n",
    "        env.reset()\n",
    "        fault_condition = False\n",
    "        #if step>0:\n",
    "            #print( observation['action_mask'])\n",
    "        for agent in env.agent_iter():\n",
    "            \n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "            episodes[step] = episode + (iteration-2)*num_episodes\n",
    "            current_phase[step] = phase\n",
    "            \n",
    "            #global_step += args.num_envs\n",
    "        \n",
    "            if True:#action != None: # this is when a player is removed\n",
    "                \n",
    "                obs[step] = torch.Tensor(observation['observation'],device=device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                action_masks[step] = torch.Tensor(observation['action_mask'],device=device)\n",
    "                curr_agent = agent#int(agent[-1])\n",
    "                current_agent[step] = curr_agent\n",
    "                \n",
    "            if step >0: # the only thing is we would not have a reward for him if the player is done.... \n",
    "                #actions[step-1] = action\n",
    "                rewards[step-1] = reward\n",
    "                dones[step-1] = termination or truncation\n",
    "                \n",
    "            if action == None:\n",
    "                rewards[step] = 0 # should i keep it -1? .... hm i dont think so .\n",
    "                dones[step] = 0 # frankly the guys is already done so we really dont have to do anything here.... this is the state post termination for a loser \n",
    "                # but btw this is for the next agent ... action == None means in the last action the previous agent would have been removed.\n",
    "                #values[step] = \n",
    "            \n",
    "            #obs[step] = next_obs\n",
    "            #dones[step] = next_done\n",
    "        \n",
    "            #print(agent,action,termination or truncation, reward,info,observation['action_mask'],env.terminations,env.truncations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model_in = torch.Tensor(np.hstack((observation_step['observation'].reshape(-1),observation_step['action_mask'].reshape(-1)\n",
    "                                                          ,[curr_agent]))[None,:]#.repeat(3,axis = 0)\n",
    "                                    )\n",
    "\n",
    "            \n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                with torch.no_grad():\n",
    "                    act, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                    values[step] = value.flatten() # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                                    #have taken at this point and what would have been its value\n",
    "                actions[step] = act #even after going what would have been\n",
    "                logprobs[step] = logprob        \n",
    "            else:\n",
    "                mask = observation[\"action_mask\"]\n",
    "                # this is where you would insert your policy\n",
    "                #action = env.action_space(agent).sample(mask)\n",
    "        \n",
    "                # ALGO LOGIC: action logic\n",
    "                with torch.no_grad():\n",
    "                    #print(next_obs.shape)\n",
    "                    action, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                    values[step] = value.flatten()\n",
    "                actions[step] = action\n",
    "                logprobs[step] = logprob\n",
    "                \n",
    "                if not observation['action_mask'][action]: \n",
    "                    fault_condition =True\n",
    "                    faulting_player = agent\n",
    "                    #print('here',agent, action, observation['action_mask'])\n",
    "    \n",
    "            #print('here',agent, action)\n",
    "            step +=1\n",
    "            global_step+=1\n",
    "            env.step(action.numpy() if action != None else None)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            if (global_step == num_steps):# or (fault_condition and (faulting_player != agent) and (len(env.agents)==0)):\n",
    "                break\n",
    "    \n",
    "        if global_step == num_steps:\n",
    "            break\n",
    "    if global_step == num_steps:\n",
    "        break\n",
    "\n",
    "#env.close()\n",
    "    with torch.no_grad():\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "    \n",
    "        for i_epi in range((iteration-2)*num_episodes,episode + (iteration-2)*num_episodes):\n",
    "            for i in agent_list:\n",
    "                agents_id = int(i[-1])\n",
    "                 \n",
    "                cur_index = torch.where((current_agent[:,0] == agents_id) &( episodes == i_epi))[0]\n",
    "            \n",
    "            \n",
    "                # bootstrap value if not done\n",
    "            \n",
    "                #next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                \n",
    "                lastgaelam = 0\n",
    "        \n",
    "        \n",
    "                last_index = cur_index[-1]\n",
    "                nextnonterminal = 1.0\n",
    "                delta = rewards[last_index] - values[last_index]\n",
    "                #print(agents_id,last_index,nextnonterminal,delta,rewards[last_index],values[last_index])\n",
    "                advantages[last_index] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                \n",
    "                \n",
    "                for o,t in reversed(list(enumerate(cur_index))[:-1]):\n",
    "                    \n",
    "        \n",
    "                    nextnonterminal = 1.0 - dones[cur_index[o + 1]]\n",
    "                    nextvalues = values[cur_index[o + 1]]\n",
    "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    #print(agents_id,o,t,cur_index[o + 1],nextnonterminal,nextvalues,delta,rewards[t])\n",
    "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "        \n",
    "        \n",
    "    #print(agent_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b_obs = obs.reshape(obs.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(actions.shape)\n",
    "    b_action_masks = action_masks.reshape(action_masks.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    b_obs_a = torch.concat(( b_obs.reshape(-1,np.prod(ob_space_shape)) ,b_action_masks.reshape(-1,np.prod(action_mask_shape)),current_agent ),axis =1)\n",
    "    \n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(step)#args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, step, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "    \n",
    "            _, newlogprob, entropy, newvalue = agent_mod.get_action_and_value(b_obs_a[mb_inds],# b_obs[mb_inds].reshape(-1,np.prod(ob_space_shape)),#\n",
    "                                                                              \n",
    "                                                                          b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "    \n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "    \n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "    \n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "    \n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent_mod.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "    \n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if TB_log:  \n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    if global_step%100 ==0:\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    if TB_log:  \n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "env.close()\n",
    "writer.close()\n",
    "#return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "2d5733af-73dc-44da-8e1d-dc7449543e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(playe_r).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "047488e4-0c20-4843-9f9d-eb084bc6b5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 2])"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_obs[mb_inds].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "db0219b5-b43e-4897-8283-daeb4770f306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_1', 'player_2']"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "25557154-fb22-4561-be4a-af11424c49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = agent_mod.network(b_obs_a[mb_inds].to(dtype=torch.float32))\n",
    "                                                                              \n",
    "                                        #                                  b_actions.long()[mb_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "29d2fad7-c2c6-46d9-9ea3-323fc87eee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_mod.network(b_obs_a[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "87ae6862-8027-4cf7-b231-276935b59a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(np.concatenate((observation['observation'].reshape(-1),observation['action_mask'].reshape(-1) )))\n",
    "\n",
    "\n",
    "\n",
    "hidden = agent_mod.network(x)\n",
    "#logits = self.actor(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "718624f8-c148-4fd5-a606-63fca9bb0608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0007,  0.0032, -0.0028, -0.0049, -0.0025, -0.0025,  0.0030,  0.0015,\n",
       "         0.0021], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_mod.actor(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "34b8bb26-4e59-4046-a6ea-f54474cc11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = Categorical(logits=agent_mod.actor(hidden))\n",
    "#if action is None:\n",
    "action = probs.sample()\n",
    "#return action, probs.log_prob(action),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "a0bf3459-18be-4c8c-a474-604309a31684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-2.1976, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1996, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.2017, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1993, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1993, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1938, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1953, grad_fn=<SqueezeBackward1>),\n",
       " tensor(-2.1947, grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[probs.log_prob(torch.tensor(i)) for i in [0,2,3,4,5,6,7,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "ce479342-9eae-4b66-abfa-6069cd47e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1467, 0.1517, 0.1433, 0.1264, 0.1377, 0.1450, 0.1492],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1535, 0.1435, 0.1258, 0.1375, 0.1452, 0.1482],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1466, 0.1527, 0.1430, 0.1263, 0.1375, 0.1451, 0.1488],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1532, 0.1432, 0.1261, 0.1374, 0.1451, 0.1486],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_0\n",
      "tensor([0.1463, 0.1534, 0.1432, 0.1260, 0.1374, 0.1451, 0.1485],\n",
      "       grad_fn=<SoftmaxBackward0>) player_1\n",
      "tensor(-1.8746)\n",
      "tensor(-1.9224)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = game.env(render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    \n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    \n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "        with torch.no_grad():\n",
    "            act, logprob, _, value = agent_mod.get_action_and_value(torch.Tensor(\n",
    "                \n",
    "                                                                        np.concatenate((observation['observation'].reshape(-1),\n",
    "                                                                                        observation['action_mask'].reshape(-1) ,\n",
    "                                                                                       [int(agent[-1])]))))\n",
    "            print(logprob) # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                            #have taken at this point and what would have been its value  \n",
    "    else:\n",
    "        mask = observation[\"action_mask\"]\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            #print(next_obs.shape)\n",
    "            action, logprob, _, value = agent_mod.get_action_and_value(torch.Tensor(\n",
    "                \n",
    "                                                                        np.concatenate((observation['observation'].reshape(-1),\n",
    "                                                                                        mask.reshape(-1),[int(agent[-1])] \n",
    "                                                                                       ))))\n",
    "            #values[step] = value.flatten()\n",
    "        #actions[step] = action\n",
    "        #logprobs[step] = logprob\n",
    "        print(agent_mod.get_valid_action(torch.Tensor(\n",
    "                \n",
    "                                                                        np.concatenate((observation['observation'].reshape(-1),\n",
    "                                                                                        mask.reshape(-1),[int(agent[-1])] \n",
    "                                                                                       )))),  agent)\n",
    "    env.step(action.numpy() if action != None else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1bacade5-ee3f-4d3f-b08e-d329ae782c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode + (iteration-1)*num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "b64fddc0-e12f-4920-b60b-1089ce06ae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "79a00656-d0bb-463b-9c37-46fe8b9105ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "bc04373c-80f2-438f-bb25-d0a844ccecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with torch.no_grad():\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "    \n",
    "        for i_epi in range(episode + (iteration-1)*num_episodes):\n",
    "            for i in agent_list:\n",
    "                agents_id = int(i[-1])\n",
    "                 \n",
    "                cur_index = torch.where((current_agent[:,0] == agents_id) &( episodes == i_epi))[0]\n",
    "            \n",
    "            \n",
    "                # bootstrap value if not done\n",
    "            \n",
    "                #next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                \n",
    "                lastgaelam = 0\n",
    "        \n",
    "        \n",
    "                last_index = cur_index[-1]\n",
    "                nextnonterminal = 1.0\n",
    "                delta = rewards[last_index] - values[last_index]\n",
    "                #print(agents_id,last_index,nextnonterminal,delta,rewards[last_index],values[last_index])\n",
    "                advantages[last_index] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                \n",
    "                \n",
    "                for o,t in reversed(list(enumerate(cur_index))[:-1]):\n",
    "                    \n",
    "        \n",
    "                    nextnonterminal = 1.0 - dones[cur_index[o + 1]]\n",
    "                    nextvalues = values[cur_index[o + 1]]\n",
    "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    #print(agents_id,o,t,cur_index[o + 1],nextnonterminal,nextvalues,delta,rewards[t])\n",
    "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "        \n",
    "        \n",
    "    #print(agent_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b_obs = obs.reshape(obs.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(actions.shape)\n",
    "    b_action_masks = action_masks.reshape(action_masks.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    b_obs_a = torch.concat(( b_obs.reshape(-1,84) ,b_action_masks.reshape(-1,7)),axis =1)\n",
    "    \n",
    "    \n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(step)#args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(1):#args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, step, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "    \n",
    "            _, newlogprob, entropy, newvalue = agent_mod.get_action_and_value(b_obs_a[mb_inds], \n",
    "                                                                              \n",
    "                                                                          b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "    \n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "    \n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "    \n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "    \n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent_mod.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "    \n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7414cc05-3807-40b2-836e-31bc62d8ac50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4468cb8d-e1ce-430c-822d-512e2b6885ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_agent[torch.where(current_agent == 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6a354bee-d602-481d-84d7-c0bfd2312d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_0', 'player_1']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = connect_four_v3.env(render_mode=\"human\")\n",
    "env.reset()\n",
    "[i for i in env.agents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c799c36d-4f80-4236-b2a5-b267c0b87d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, -0.0301, -0.0376, -0.0278, -0.0333, -0.0227, -0.0210, -0.0181,\n",
       "         -0.0184, -0.0121, -0.0172, -0.0083, -0.0108, -0.0084, -0.0077, -0.0048,\n",
       "          0.0000,  0.0000, -0.6175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " tensor([ 0.0000, -0.0488,  0.0353,  0.1125,  0.2168,  0.2567,  0.6146,  0.2333,\n",
       "          0.1320,  0.2982,  0.0595,  0.1919,  0.3194, -0.0020,  0.1568,  0.1759,\n",
       "          0.3862,  0.2427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " tensor([ 0.0000,  0.0188, -0.0729, -0.1403, -0.2501, -0.2794, -0.6356, -0.2514,\n",
       "         -0.1504, -0.3103, -0.0767, -0.2002, -0.3302, -0.0064, -0.1645, -0.1807,\n",
       "         -0.3862, -0.2427, -0.6175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns[:30],advantages[:30], values[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e870f2df-1124-48a9-b4a9-0c1812eed240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])\n",
      "0 tensor(0.) tensor(-0.6175) tensor(0.3862)\n",
      "0 tensor(1.) tensor(-0.3862) tensor(-0.2213)\n",
      "0 tensor(1.) tensor(-0.1645) tensor(0.1659)\n",
      "0 tensor(1.) tensor(-0.3302) tensor(-0.2532)\n",
      "0 tensor(1.) tensor(-0.0767) tensor(0.0738)\n",
      "0 tensor(1.) tensor(-0.1504) tensor(0.4854)\n",
      "0 tensor(1.) tensor(-0.6356) tensor(-0.3849)\n",
      "0 tensor(1.) tensor(-0.2501) tensor(-0.1770)\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])\n",
      "1 tensor(0.) tensor(0.) tensor(0.2427)\n",
      "1 tensor(1.) tensor(-0.2427) tensor(-0.0617)\n",
      "1 tensor(1.) tensor(-0.1807) tensor(-0.1742)\n",
      "1 tensor(1.) tensor(-0.0064) tensor(0.1939)\n",
      "1 tensor(1.) tensor(-0.2002) tensor(0.1103)\n",
      "1 tensor(1.) tensor(-0.3103) tensor(-0.0586)\n",
      "1 tensor(1.) tensor(-0.2514) tensor(0.0282)\n",
      "1 tensor(1.) tensor(-0.2794) tensor(-0.1388)\n",
      "1 tensor(1.) tensor(-0.1403) tensor(-0.1590)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    \n",
    "    for i in [0,1]:\n",
    "        agents_id = i#int(i[-1])\n",
    "         \n",
    "        cur_index = torch.where(current_agent == agents_id)[0]\n",
    "    \n",
    "        print(cur_index)\n",
    "        # bootstrap value if not done\n",
    "    \n",
    "        #next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        \n",
    "        lastgaelam = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        for o,t in reversed(list(enumerate(cur_index))[:-1]):\n",
    "            \n",
    "\n",
    "            \n",
    "            nextnonterminal = 1.0 - dones[cur_index[o + 1]]\n",
    "            nextvalues = values[cur_index[o + 1]]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "\n",
    "            print(agents_id,nextnonterminal,nextvalues,delta)\n",
    "            \n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "532061fe-5cbd-4239-90a0-141e3b38bc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.0488,  0.0353,  0.1125,  0.2168,  0.2567,  0.6146,  0.2333,\n",
       "         0.1320,  0.2982,  0.0595,  0.1919,  0.3194, -0.0020,  0.1568,  0.1759,\n",
       "         0.3862,  0.2427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3365db04-8c80-4ed4-bc0f-66372dfe48a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in env.agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "77314421-274d-422b-bace-e04d17eacd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(current_agent == np.array([1,]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a6e50c0-580f-4a79-9a52-ddb2672a85d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_space_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9732025f-74d3-4253-906c-125829b8010c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(current_agent == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c52c308-6fd1-420e-b888-ecbe1a7a85db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "1 2\n",
      "0 5\n"
     ]
    }
   ],
   "source": [
    "for i,j in reversed(list(enumerate([5,2,3]))):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f2aefc7-70dc-4aaa-9c3e-54cc53d7db9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate([5,2,3]))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4bf089c0-7142-4007-807d-5685ca5ac4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]], dtype=int8)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060dc55-1bb8-4420-95cb-c5b66a05f337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
