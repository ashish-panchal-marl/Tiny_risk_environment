{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15a50b2-601f-43ab-be81-953007ef1128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa91511-f5f3-4b12-b761-46edaceaeef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.new_models import *\n",
    "import utils_gym\n",
    "import env_model_class_2\n",
    "\n",
    "\n",
    "from board_env import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c933bf13-46e2-4a45-8f7a-c170246eb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c75b32-8ec7-4818-af8d-1fd3686cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Hero_agent(int):\n",
    "    def init_properties(self,agent_count,phases):\n",
    "        #self.draw_count = 0\n",
    "        self.init_win_count_iter(agent_count)\n",
    "        self.init_move_count_epi(phases)\n",
    "        \n",
    "\n",
    "        \n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}        \n",
    "    \n",
    "    def model_def(self, model):\n",
    "        self.model =model\n",
    "\n",
    "    def action_predict(self,data):\n",
    "        return self.model.action_predict(data)\n",
    "    def save_models(self):\n",
    "        self.model.save_models()\n",
    "    \n",
    "    #def model_forward_call(self,name,kwarg):\n",
    "    #    return self.model_dict[name](**kwarg)\n",
    "        \n",
    "\n",
    "a = Hero_agent(1)\n",
    "a.init_properties(3,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e358f8b6-7c37-4cdf-bbb7-47132889bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_module:\n",
    "    def __init__(self, qnet_config_dict, actor_config_dict,args,device,run_name,agent,writer=None):\n",
    "\n",
    "        self.agent = agent\n",
    "        self.run_name =run_name \n",
    "        self.actor_config_dict = actor_config_dict\n",
    "        self.qnet_config_dict = qnet_config_dict\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.writer = writer\n",
    "        \n",
    "        self.actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "        self.qf1 = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "        self.qf1_target = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "        self.target_actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "        \n",
    "        \n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def action_predict(self,data):\n",
    "        return self.actor(data)\n",
    "\n",
    "    def train_write(self,data,iteration,epoch):\n",
    "        #data = rb.sample(self.args.batch_size)\n",
    "        qf1_a_values, qf1_loss, actor_loss = self.train(data)\n",
    "        self.write(qf1_a_values, qf1_loss, actor_loss,epoch,iteration)\n",
    "\n",
    "    def train(self,data):\n",
    "\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            collected_t_next = data.next_observations[:,-1]\n",
    "            next_state_actions = self.target_actor(data.next_observations[:,:-1])\n",
    "            qf1_next_target = self.qf1_target(data.next_observations[:,:-1], next_state_actions)\n",
    "\n",
    "            \n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * (self.args.gamma**(collected_t_next+1)).view(-1) * (qf1_next_target).view(-1)\n",
    "    \n",
    "        qf1_a_values = self.qf1(data.observations[:,:-1], data.actions).view(-1)\n",
    "        qf1_loss = nn.functional.mse_loss(qf1_a_values, next_q_value)\n",
    "        \n",
    "        # optimize the model\n",
    "        self.q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        \n",
    "        #if global_step % args.policy_frequency == 0:\n",
    "        actor_loss = -self.qf1(data.observations[:,:-1], self.actor(data.observations[:,:-1])).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "    \n",
    "        # update the target network\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        return qf1_a_values, qf1_loss, actor_loss\n",
    "        \n",
    "\n",
    "    def write(self,qf1_a_values, qf1_loss, actor_loss,epoch,iteration):\n",
    "        \n",
    "        ind_epoch = epoch + (iteration-1)*self.args.update_epochs\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_values\", qf1_a_values.mean().item(), ind_epoch)\n",
    "        \n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_loss\", qf1_loss.item(), ind_epoch)\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/actor_loss\", actor_loss.item(), ind_epoch)\n",
    "        \n",
    "    def save_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.actor.state_dict(), newpath+\"/actor.pt\")\n",
    "        torch.save(self.qf1.state_dict(), newpath+\"/qf1.pt\")\n",
    "        torch.save(self.qf1_target.state_dict(), newpath+\"/qf1_target.pt\")\n",
    "        torch.save(self.target_actor.state_dict(), newpath+\"/target_actor.pt\")  \n",
    "    def load_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        self.actor.load_state_dict(torch.load(newpath+\"/actor.pt\"))\n",
    "        self.qf1.load_state_dict(torch.load(newpath+\"/qf1.pt\"))\n",
    "        self.qf1_target.load_state_dict(torch.load(newpath+\"/qf1_target.pt\"))\n",
    "        self.target_actor.load_state_dict(torch.load(newpath+\"/target_actor.pt\")) \n",
    "        \n",
    "\n",
    "                                          \n",
    "                   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b2549d0-9859-4fe8-9c2f-93886527cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluate_model:\n",
    "    def __init__(self,Args,param_dict =dict({}),run_name=\"MountainCar-v0__exp1_ddqn___1__1724315565\"\n",
    "                 ):\n",
    "        # #### Configurations\n",
    "\n",
    "        self.args = Args()#tyro.cli(Args)\n",
    "        self.param_dict = param_dict\n",
    "        self.update_arg(param_dict=param_dict)\n",
    "\n",
    "        \n",
    "        #self.args.batch_size = int(self.args.num_envs * self.args.num_steps)\n",
    "        self.args.minibatch_size = int(self.args.batch_size // self.args.num_minibatches)\n",
    "        #self.args.num_iterations = self.args.total_timesteps // self.args.batch_size\n",
    "        self.gam = self.args.gamma\n",
    "        #self.args.minibatch_size = 256#128 \n",
    "        self.num_steps = self.args.num_steps#120000#1000000\n",
    "        self.num_iterations = self.args.num_iterations\n",
    "        self.episode_time_lim = self.args.episode_time_lim\n",
    "        self.num_episodes = self.args.num_episodes\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.run_name = run_name\n",
    "\n",
    "        self.writer = None\n",
    "\n",
    "        # TRY NOT TO MODIFY: seeding\n",
    "        random.seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        torch.manual_seed(self.args.seed)\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = self.args.torch_deterministic\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        self.playe_r = 1#\"agent_1\" #\n",
    "        \n",
    "\n",
    "        \n",
    "        self.action_shape = (2,)\n",
    "\n",
    "        self.env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "                            agent_count  = 4\n",
    "                               ,use_placement_perc=True,render_=False)\n",
    "\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        \n",
    "        self.env.reset(seed=42)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.total_agents  = len(self.env.possible_agents)\n",
    "        self.total_phases = len(self.env.phases)\n",
    "        \n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']),\n",
    "                                        num_classes = self.total_agents+1\n",
    "                                       )\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        \n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "        \n",
    "        \n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "        \n",
    "        self.qnet_config_dict = dict(action_space = self.env.action_space(self.playe_r\n",
    "                                                                         ).shape[0],\n",
    "                                    ob_space=np.prod(self.ob_space_shape\n",
    "                                                    )+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents -1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=np.prod(self.ob_space_shape)+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        \n",
    "        self.hero_agent_count = 2\n",
    "        self.hero_agents_list = {i:Hero_agent(i) for i in range(1,self.hero_agent_count+1) } # this is a list , need to pass it as an argument\n",
    "        \n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_properties(self.total_agents,self.env.phases)        \n",
    "            \n",
    "            self.hero_agents_list[i].model_def(model = DDQN_module( self.qnet_config_dict, self.actor_config_dict,\n",
    "                                                                   self.args,device = self.device,run_name=self.run_name,agent=i)\n",
    "                                                )\n",
    "\n",
    "            self.hero_agents_list[i].model.load_models()\n",
    "            \n",
    "\n",
    "            #self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "            #self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "            #self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "            #self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def update_arg(self,param_dict=dict({})):\n",
    "       for i,j in param_dict.items():\n",
    "           setattr(self.args,i,j)\n",
    "    \n",
    "\n",
    "    \n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                            num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "    \n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "        \n",
    "        self.draw_count = 0\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_win_count_iter(self.total_agents )\n",
    "        \n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "\n",
    "    def reset_moves_hero_agents(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_move_count_epi(self.env.phases)    \n",
    "    \n",
    "    def run_eval_loop(self):\n",
    "        \"\"\"\n",
    "        ### Run training loop\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        env = env_risk(**(self.env_config | {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}))\n",
    "        env.reset(42)\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "            self.sample(env,iteration)\n",
    "            print(iteration)\n",
    "\n",
    "    def sample(self,env,iteration):\n",
    "        with torch.no_grad():\n",
    "            # sample `worker_steps` from each worker\n",
    "            #there are no worker steps... rather there are full episodes\n",
    "\n",
    "            step = 0\n",
    "            fault_condition = False\n",
    "            clear_output(wait=True)\n",
    "            phase = 0\n",
    "        \n",
    "            \n",
    "            for episode in range(self.num_episodes):#num_episodes):\n",
    "                \n",
    "                total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "                action=1\n",
    "                \n",
    "                if fault_condition:\n",
    "                    env = env_risk(**(self.env_config | {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                                                        })\n",
    "                                      )#game.env(render_mode=None)\n",
    "\n",
    "                curren_epi = episode + (iteration-1)*self.num_episodes\n",
    "                env.reset(curren_epi) #for riplication\n",
    "                \n",
    "                fault_condition = False\n",
    "                step_count = 0\n",
    "                \n",
    "                self.reset_moves_hero_agents()\n",
    "                is_draw = 0\n",
    "                \n",
    "                #draw_territory_count = 0\n",
    "                #is_third = 0\n",
    "\n",
    "                for agent in env.agent_iter():\n",
    "                    e_t = env.terminations\n",
    "                    if sum(e_t.values()) <(self.total_agents-1):\n",
    "                        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "                        observation['observation'] =  self.obs_converter(\n",
    "                                                        torch.tensor(\n",
    "                                                            observation['observation']\n",
    "                                                        ).to(self.device,dtype=torch.float32),\n",
    "                                                        num_classes = self.total_agents+1)\n",
    "                        \n",
    "                        #episodes[step] = curren_epi\n",
    "                        #obs[step] = observation['observation']#torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                        #action_masks[step] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                        \n",
    "                        #curr_agent = agent#int(agent[-1])\n",
    "                        #current_agent[step] = \n",
    "                        curr_agent = agent\n",
    "                        #current_phase[step] = \n",
    "                        phase = env.phase_selection\n",
    "                        \n",
    "                        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "                        \n",
    "                        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                                      num_classes = self.total_agents \n",
    "                                                                     ).float()\n",
    "                        \n",
    "                        #current_troops_count[step] = torch.Tensor([env.board.agents[i].bucket for i in env.possible_agents]).to(self.device)\n",
    "                    \n",
    "\n",
    "                        model_in = torch.Tensor(torch.hstack((observation['observation'].reshape(-1),torch.tensor(observation['action_mask'].reshape(-1)).to(self.device),\n",
    "                                           phase_mapping,\n",
    "                                            curr_agent_mapping,\n",
    "                                           torch.tensor([env.board.agents[curr_agent].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "                                                ).float()\n",
    "                        \n",
    "                        #if e_t[curr_agent]:\n",
    "                            #print('heeee')\n",
    "                            \n",
    "                        if termination or truncation: #this never happens ... the agent is removed from the current agent list and processed after the end of the cycle\n",
    "                            \n",
    "                            action = None\n",
    "\n",
    "                            act = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            #act = self.\n",
    "                            #act, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                            #values[step] = value.flatten() # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                                                #have taken at this point and what would have been its value\n",
    "                            #actions[step] = act #even after going what would have been\n",
    "                            #logprobs[step] = logprob        \n",
    "                        else:\n",
    "                            mask = observation[\"action_mask\"]\n",
    "                            if ( agent not in self.hero_agents_list):\n",
    "        \n",
    "                                \n",
    "                                action = env.action_space(agent).sample()\n",
    "                                part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                                action = torch.Tensor([[[part_0],[np.around(action[1],2)]]]).to(self.device)\n",
    "                                action = action[:,:,0]\n",
    "\n",
    "                                if not( observation['action_mask'][action[:,0].long()]) : \n",
    "                                    fault_condition =True\n",
    "                                    print(agent,observation['action_mask'],action[:,0].long())\n",
    "                                    break\n",
    "\n",
    "\n",
    "                            \n",
    "                            else:\n",
    "                                action = self.hero_agents_list[curr_agent].action_predict(torch.Tensor(model_in).to(self.device))\n",
    "                                #action = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            #actions[step] = action\n",
    "                            curr_agent_ = int(curr_agent)\n",
    "        \n",
    "                            if (not observation['action_mask'][action[:,0].long()]) : \n",
    "                                fault_condition =True\n",
    "                                #self.faulting_player = agent\n",
    "\n",
    "                                \n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                                if  curr_agent_ in self.hero_agents_list:\n",
    "                                    print(observation['action_mask'],action[:,0].long())\n",
    "                                    print(env.board.calculated_action_mask(agent))\n",
    "                                    print(model_in)\n",
    "                                    break\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_count+=1\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_phase_count[phase]+=1  # when is the where_is_it_performing_bad_really\n",
    "                                    #print('here',agent, action, observation['action_mask'])\n",
    "                            \n",
    "        \n",
    "                            #if  curr_agent_ in self.hero_agents_list:\n",
    "                            #    self.hero_agents_list[curr_agent_].move_count[int(current_phase[step][0])]+=1  \n",
    "                            #if self.the_hero_agent == curr_agent:\n",
    "                                #move_count[int(current_phase[step][0])]+=1        \n",
    "        \n",
    "        \n",
    "                        #print('here',agent, action)\n",
    "                        if action != None :\n",
    "                            act_2 = action.detach().cpu().numpy()[0]#list([action.detach().cpu().numpy()[0][0], max(action.detach().cpu().numpy()[0][1],0.1) ])\n",
    "                            act_2 = list([act_2[0], max(act_2[1],0.001) ])\n",
    "                        else:\n",
    "                            act_2 = action\n",
    "\n",
    "                        \n",
    "                        env.step(act_2 if action != None else None) \n",
    "\n",
    "                        if action != None:\n",
    "                            curr_reward_list =  env.curr_rewards\n",
    "                            if (step_count == (self.episode_time_lim-1)):\n",
    "                                is_draw=1\n",
    "                                curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "        \n",
    "        \n",
    "                        #if action == None:\n",
    "                        #    print('heeee')\n",
    "                        #    rewards[step] = np.zeros(self.total_agents) # should i keep it -1? .... hm i dont think so .\n",
    "                        #    dones[step] = np.zeros(self.total_agents) # frankly the guys is already done so we really dont have to do anything here.... this is the state post termination for a loser \n",
    "                        #    # but btw this is for the next agent ... action == None means in the last action the previous agent would have been removed.\n",
    "                        #    #values[step] = \n",
    "                        #else:\n",
    "        #\n",
    "                        #    rewards_2[step] = torch.Tensor([env.curr_rewards[i] for i in env.possible_agents]).to(self.device)\n",
    "                        #    curr_reward_list =  env.curr_rewards\n",
    "                        #    \n",
    "                        #    if (step_count == (self.episode_time_lim-1)):\n",
    "                        #        is_draw=1\n",
    "                        #        curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "        #\n",
    "                        #    for i in env.possible_agents:\n",
    "                        #        if i != curr_agent:\n",
    "                        #            self.gamma_t[i]+=1\n",
    "                        #        else:\n",
    "                        #            self.gamma_t[i] =0\n",
    "        #\n",
    "                        #        if (step_count == (self.episode_time_lim-1)):\n",
    "                        #            cr_rew = -100\n",
    "                        #            term = True\n",
    "                        #        else:\n",
    "                        #            cr_rew = env.curr_rewards[i]\n",
    "                        #            term = env.terminations[i]\n",
    "#\n",
    "                        #        next_step_ = step-self.gamma_t[i]\n",
    "                        #        rewards[next_step_,i-1] += (self.args.gamma**self.gamma_t[i])*cr_rew\n",
    "                        #        t_next[next_step_,i-1] = self.gamma_t[i]\n",
    "                        #        dones[next_step_,i-1] = torch.Tensor([term]).to(self.device) #so the panetly has to be added but attributions is really difficult\n",
    "        \n",
    "                        #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                        \n",
    "                        #if sum(curr_reward_list.values()) == -300:\n",
    "                            #print('here')\n",
    "                            #is_draw=1\n",
    "        \n",
    "                        \n",
    "                        for age_i in env.possible_agents:\n",
    "                            \n",
    "                            total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                    \n",
    "                        \n",
    "                        step +=1\n",
    "                        self.global_step+=1\n",
    "        \n",
    "                    else:\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        break    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e38f14b-e0c2-47dc-8a5d-fcc76d21863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_3 = dict(\n",
    "exp_name = 'exp3_ddqn_lr_bs_1',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 1,\n",
    "episode_time_lim = 10000,\n",
    "num_episodes = 1\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874fe8a-b2ff-41ed-bde6-d704e7f8ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffc36958-6dab-4071-9b49-7aa46e64016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = evaluate_model(Args,param_dict = exp_3,\n",
    "                   run_name = 'MountainCar-v0__exp1_ddqn___1__1724315565')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db911864-2cc2-484e-88e2-f342b8901b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DDQN_module at 0x248021bbe50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfd1d4-3085-4d5f-aca7-d4f81e6798d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b201cd3d-ce4c-4427-800f-c3954ca29755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1] tensor([29], device='cuda:0')\n",
      "[1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 3.]],\n",
      "       device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "T.run_eval_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "091cb0f0-8eb5-47f1-aa80-114aab62b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden = T.hero_agents_list[2].model.actor.network( torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
    "         1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 3.]],\n",
    "       device='cuda:0'))\n",
    "logits = T.hero_agents_list[2].model.actor.actor_1(hidden)\n",
    "probs = Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1ac125b-db8e-4b77-b50d-cdec8c7bd980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0297, 0.0316, 0.0309, 0.0323, 0.0323, 0.0319, 0.0315, 0.0365, 0.0295,\n",
       "         0.0301, 0.0312, 0.0319, 0.0310, 0.0322, 0.0318, 0.0334, 0.0311, 0.0317,\n",
       "         0.0306, 0.0283, 0.0320, 0.0306, 0.0299, 0.0291, 0.0303, 0.0307, 0.0314,\n",
       "         0.0315, 0.0327, 0.0315, 0.0291, 0.0316]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f996a10-63f3-422e-b6b1-0e4c94e286b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_ddqn_(nn.Module):\n",
    "    def __init__(self, action_space = None, ob_space=None):\n",
    "        super().__init__()\n",
    "        #self.fc1 = nn.Linear(ob_space, 256)\n",
    "        #self.fc2 = nn.Linear(256, 256)\n",
    "        #self.fc_mu = nn.Linear(256, action_space)\n",
    "\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(ob_space, 256)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.GELU(),\n",
    "            layer_init(nn.Linear(256, 64)),\n",
    "            nn.GELU(),\n",
    "            #layer_init(nn.Linear(64, 64)),\n",
    "            #nn.GELU(),\n",
    "        )\n",
    "        self.actor_1 = layer_init(nn.Linear(64, action_space), std=0.01)\n",
    "        self.actor_2 = nn.Sequential(layer_init(nn.Linear(64, 2), std=0.01), nn.Softmax(dim=1),)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.network(x)\n",
    "        logits = self.actor_1(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        action_1 = probs.sample()[:,None]\n",
    "        action_2 = self.actor_2(hidden)[:,[0]]\n",
    "\n",
    "        action = torch.concat((action_1,action_2),1)\n",
    "\n",
    "        return action#* self.action_scale + self.action_bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db9052c3-d556-4702-bac1-064f3f351f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Actor_ddqn_(action_space = 5, ob_space=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fccfd53-47b8-488d-a8fa-05d54df03bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = a1.network(torch.Tensor([[1,2,3,4,5]]))\n",
    "logits = a1.actor_1(hidden)\n",
    "probs = Categorical(logits=logits)\n",
    "\n",
    "#probs.sample()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ccba7e6-4a31-4024-99af-cd1df2ec99a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "tensor([0])\n",
      "tensor([4])\n",
      "tensor([3])\n",
      "tensor([4])\n",
      "tensor([4])\n",
      "tensor([0])\n",
      "tensor([1])\n",
      "tensor([4])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([1])\n",
      "tensor([0])\n",
      "tensor([3])\n",
      "tensor([0])\n",
      "tensor([2])\n",
      "tensor([4])\n",
      "tensor([4])\n",
      "tensor([0])\n",
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(probs.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3bb2ef9-35a1-42e0-9708-1c7e98231018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.4998]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1(torch.Tensor([[1,2,3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9811b0e-6a09-44f7-99fc-45c8d91cbfca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
