{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905a5491-3426-4f41-9c29-9194f5398328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20abc03-b5a9-48bc-aee4-f42c7b25ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.new_models import *\n",
    "import utils_gym\n",
    "import env_model_class_2\n",
    "\n",
    "\n",
    "from board_env import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254dca59-dda5-4fbf-b5fd-6f821996a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56145934-9391-473e-802f-92b9eb8f38e3",
   "metadata": {},
   "source": [
    "# hero agent definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2661a5d-7bae-4be9-97ae-cf181efef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Hero_agent(int):\n",
    "    def init_properties(self,agent_count,phases,cp=[],df=[],direct_action=True):\n",
    "        #self.draw_count = 0\n",
    "        self.init_win_count_iter(agent_count)\n",
    "        self.init_move_count_epi(phases)\n",
    "        self.cp = cp\n",
    "        self.df = df\n",
    "        self.direct_action = direct_action\n",
    "        self.init_reward_concern(agent_count,cp=cp,df=df)\n",
    "        \n",
    "    def init_reward_concern(self,agent_count,cp=[],df=[]):\n",
    "        if len(cp)==0:\n",
    "            cp = [int(self)]\n",
    "        self.concern=torch.tensor([(1 if i in cp \n",
    "                             else \n",
    "                             (-1 if i in df \n",
    "                                  else 0)) for i in range(1,agent_count+1) ])\n",
    "        #self.concern_2 = self.concern\n",
    "        #self.concern_2[self-1] =0\n",
    "        \n",
    "        self.multi_dependency = (sum(self.concern !=0)>1)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def init_win_count_iter(self,agent_count):\n",
    "        self.count_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.count_draw_dict = {i:0 for i in range(1,agent_count+1)}\n",
    "        self.draw_territory_count = 0\n",
    "    def init_move_count_epi(self,phases):\n",
    "        self.bad_move_count = 0\n",
    "        self.bad_move_phase_count = {i:0 for i in phases}\n",
    "        self.move_count =  {i:0 for i in phases}        \n",
    "    \n",
    "    def model_def(self, model):\n",
    "        self.model =model\n",
    "\n",
    "    def action_predict(self,data):\n",
    "        return self.model.action_predict(data)\n",
    "    def save_models(self):\n",
    "        self.model.save_models()\n",
    "\n",
    "    def process_reward(self,rewards,step,hero_steps):\n",
    "        if self.multi_dependency and self.direct_action:\n",
    "            return (rewards*self.concern.to(rewards.device)).sum(-1)[:step][hero_steps][:,None]\n",
    "        elif self.multi_dependency and not self.direct_action:\n",
    "            base_rew = torch.zeros( rewards[:step,self-1][hero_steps].shape)\n",
    "            #print(base_rew)\n",
    "\n",
    "            hero_step_list  = np.arange(0,step)[hero_steps]\n",
    "            for i,j in zip(hero_step_list[:-1],hero_step_list[1:]):\n",
    "                if j-i>1:\n",
    "                    #print(j,i,rewards[i:j],(rewards[i:j]*self.concern),(rewards[i:j]*self.concern).sum())\n",
    "                    base_rew[i]+= (rewards[i:j]*self.concern).sum()\n",
    "            #print(base_rew,rewards[hero_step_list[-1]:],(rewards[hero_step_list[-1]:]*self.concern))\n",
    "            base_rew[-1]+= (rewards[hero_step_list[-1]:]*self.concern).sum()\n",
    "            \n",
    "            return base_rew[:,None]\n",
    "            \n",
    "        else:\n",
    "            return rewards[:step][hero_steps][:,None]\n",
    "    \n",
    "    #def model_forward_call(self,name,kwarg):\n",
    "    #    return self.model_dict[name](**kwarg)\n",
    "        \n",
    "\n",
    "a = Hero_agent(1)\n",
    "a.init_properties(3,[1,2,3],cp=[1],df=[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64599fd5-357b-443a-a628-2edfdf2aca12",
   "metadata": {},
   "source": [
    "# dataset definition \n",
    "## episode trajectory\n",
    "## dataloader for per iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "89b4d63e-4634-453e-99dc-515bdd9b6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "class TrajectoryDataset_per_episode(Dataset): #this should have only 1 trajectory no matter what\n",
    "    def __init__(self, trajectories, context_len, rtg_scale,gamma=0.99,min_len = 10**6):\n",
    "        self.trajectories = trajectories\n",
    "        self.context_len = context_len\n",
    "\n",
    "        min_len = min(min_len, len(self.trajectories[0]))\n",
    "        \n",
    "        #states = []\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj_len = traj['observations'].shape[0]\n",
    "        #    min_len = min(min_len, traj_len)\n",
    "        #    states.append(traj['observations'])\n",
    "        #    # calculate returns to go and rescale them \n",
    "            \n",
    "        self.trajectories[0]['returns_to_go'] = discount_cumsum(self.trajectories[0]['rewards'], gamma) / rtg_scale\n",
    "            \n",
    "        #print(min_len)\n",
    "        \n",
    "        # used for input normalization\n",
    "        \n",
    "        #states = torch.concatenate(states, axis=0).to(dtype = torch.float32)\n",
    "        self.state_mean, self.state_std = torch.mean(self.trajectories[0]['observations'], axis=0\n",
    "                                                    ), torch.std(self.trajectories[0]['observations'], axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.trajectories),(self.trajectories[0].shape),len(self.trajectories[0])- self.context_len + 1 )\n",
    "        return sum(max(0, len(traj['observations'])- self.context_len + 1\n",
    "                      ) for traj in self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        for traj in self.trajectories:\n",
    "            \n",
    "            #print(total_len, idx - total_len, total_len + len(traj), - self.context_len + 1)\n",
    "            \n",
    "            if total_len  <= idx < total_len + len(traj['observations']) - self.context_len + 1    :\n",
    "                si = idx - total_len\n",
    "                \n",
    "                #context = traj[si:si + self.context_length]\n",
    "                states = (traj['observations'][si : si + self.context_len])\n",
    "                actions = (traj['actions'][si : si + self.context_len])\n",
    "                reward =  (traj['rewards'][si : si + self.context_len])\n",
    "                returns_to_go = (traj['returns_to_go'][si : si + self.context_len])\n",
    "                \n",
    "                action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "                current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "                current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "                current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "                current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "    \n",
    "                \n",
    "                timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "    \n",
    "                # all ones since no padding\n",
    "                traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "                if self.context_len> len(states):\n",
    "                    padding_len = self.context_len - len(states)\n",
    "    \n",
    "                    states                = torch.cat([states,\n",
    "                                    torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "                    actions               = torch.cat([actions,\n",
    "                                    torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)   \n",
    "                    reward                = torch.cat([reward,\n",
    "                                    torch.zeros(([padding_len] + list(reward.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)  \n",
    "                    returns_to_go         = torch.cat([returns_to_go,\n",
    "                                    torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)         \n",
    "                    action_masks          = torch.cat([action_masks,\n",
    "                                    torch.zeros(([padding_len] + list(action_masks.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)        \n",
    "                    current_agent         = torch.cat([current_agent,\n",
    "                                    torch.zeros(([padding_len] + list(current_agent.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)\n",
    "                    \n",
    "                    current_troops_count = torch.cat([current_troops_count,\n",
    "                                        torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                        dtype=current_troops_count.dtype\n",
    "                                                   )], \n",
    "                                       dim=0)\n",
    "                    current_phase         = torch.cat([current_phase,\n",
    "                                    torch.zeros(([padding_len] + list(current_phase.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)         \n",
    "                    current_troops_count  = torch.cat([current_troops_count,\n",
    "                                    torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)                \n",
    "                    traj_mask             = torch.cat([traj_mask,\n",
    "                                    torch.zeros(([padding_len] + list(traj_mask.shape[1:])),\n",
    "                                    dtype=reward.dtype\n",
    "                                               )], \n",
    "                                   dim=0)     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                return  timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,current_agent_simple,current_agent,current_phase,current_troops_count\n",
    "                \n",
    "                #return pad(torch.tensor(context), (0,(self.context_length - len(context))),mode=\"constant\"), [1]\n",
    "\n",
    "            total_len += len(traj) - self.context_len + 1\n",
    "\n",
    "        raise IndexError(\"Index out of range 1\")\n",
    "\n",
    "\n",
    "class TrajectoryDataset_2_through_episodes(Dataset):\n",
    "    def __init__(self, trajectories):\n",
    "        self.trajectories = trajectories\n",
    "\n",
    "        all_obs = torch.concat([ traj.dataset.trajectories[0]['observations'] for traj in self.trajectories],axis=0)\n",
    "        self.state_mean = torch.mean(all_obs,axis =0)\n",
    "        self.state_std = torch.std(all_obs,axis =0) + 1e-6\n",
    "        \n",
    "        for traj in self.trajectories:\n",
    "            traj.dataset.trajectories[0]['observations'] = (traj.dataset.trajectories[0]['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "        #print(self.state_mean,self.state_std)\n",
    "        \n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std        \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        total_len = 0\n",
    "        if total_len  <= idx < total_len + len(self.trajectories)  :\n",
    "            return [batch for batch in self.trajectories[idx] ][0]\n",
    "\n",
    "\n",
    "        raise IndexError(\"Index out of range 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dee2d-254c-4dc0-9ad8-b4d752e4484c",
   "metadata": {},
   "source": [
    "# custom sampler for the hero agent \n",
    "(but will not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "079213f9-d066-4ed3-b6f9-8296e89b543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(T.paths[0].dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "891d78f9-fbf6-418c-9d78-f884163b6403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[tensor([6, 7])], [tensor([3, 4])]]]"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.sampler import  Sampler\n",
    "\n",
    "class YourSampler(Sampler[list[int]]):\n",
    "    def __init__(self, mask,data_len):\n",
    "        self.mask = mask[:,None]\n",
    "        self.indices = np.arange(data_len)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        return (self.indices[i] for i in self.mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask)\n",
    "\n",
    "\n",
    "sampler1 = YourSampler(torch.tensor([2,3]),4)\n",
    "\n",
    "trainloader_sampler1 = torch.utils.data.DataLoader([([1],[2]),([5],[2]),([6],[3]),([7],[4])], batch_size=4,\n",
    "                                          sampler = sampler1, shuffle=False)\n",
    "[batch for batch in trainloader_sampler1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f2cad-ba8f-49f6-8aa6-001d66789d41",
   "metadata": {},
   "source": [
    "# old dataset and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "c7bed4f2-9df3-4866-a58e-14b12d51bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class D4RLTrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories, context_len, rtg_scale,gamma=0.99,min_len = 10**6):\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.trajectories = trajectories\n",
    "\n",
    "        #print(len(self.trajectories))\n",
    "        # load dataset\n",
    "        #with open(dataset_path, 'rb') as f:\n",
    "        #    self.trajectories = pickle.load(f)\n",
    "\n",
    "        \n",
    "        # calculate min len of traj, state mean and variance\n",
    "        # and returns_to_go for all traj\n",
    "\n",
    "        \n",
    "        \n",
    "        states = []\n",
    "        for traj in self.trajectories:\n",
    "            traj_len = traj['observations'].shape[0]\n",
    "            min_len = min(min_len, traj_len)\n",
    "            states.append(traj['observations'])\n",
    "            # calculate returns to go and rescale them\n",
    "            \n",
    "            traj['returns_to_go'] = discount_cumsum(traj['rewards'], gamma) / rtg_scale\n",
    "            \n",
    "        print(min_len)\n",
    "        \n",
    "        # used for input normalization\n",
    "        states = torch.concatenate(states, axis=0).to(dtype = torch.float32)\n",
    "        self.state_mean, self.state_std = torch.mean(states, axis=0), torch.std(states, axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        #for traj in self.trajectories:\n",
    "        #    traj['observations'] = (traj['observations'].to(dtype=torch.float32) - self.state_mean) / self.state_std\n",
    "\n",
    "\n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std\n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.trajectories))\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = traj['observations'].shape[0]\n",
    "\n",
    "        if traj_len >= self.context_len:\n",
    "            # sample random index to slice trajectory\n",
    "            si = random.randint(0, traj_len - self.context_len)\n",
    "\n",
    "            states = (traj['observations'][si : si + self.context_len])\n",
    "            actions = (traj['actions'][si : si + self.context_len])\n",
    "            reward =  (traj['rewards'][si : si + self.context_len])\n",
    "            returns_to_go = (traj['returns_to_go'][si : si + self.context_len])\n",
    "            \n",
    "            action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "            current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "            current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "            current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "            current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "\n",
    "            \n",
    "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "\n",
    "            # all ones since no padding\n",
    "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            padding_len = self.context_len - traj_len\n",
    "\n",
    "            # padding with zeros\n",
    "            states = (traj['observations'])\n",
    "            states = torch.cat([states,\n",
    "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                dtype=states.dtype)], \n",
    "                               dim=0)\n",
    "            \n",
    "            actions = (traj['actions'])\n",
    "            actions = torch.cat([actions,\n",
    "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
    "                                dtype=actions.dtype)], \n",
    "                               dim=0)\n",
    "            reward = (traj['rewards'])\n",
    "            reward = torch.cat([reward,\n",
    "                                torch.zeros(([padding_len] + list(reward.shape[1:])),\n",
    "                                dtype=reward.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            returns_to_go = (traj['returns_to_go'])\n",
    "            returns_to_go = torch.cat([returns_to_go,\n",
    "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
    "                                dtype=returns_to_go.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            action_masks = (traj['action_masks'][si : si + self.context_len])\n",
    "            current_agent_simple = (traj['current_agent_simple'][si : si + self.context_len])\n",
    "            current_agent = (traj['current_agent'][si : si + self.context_len])\n",
    "            current_phase = (traj['current_phase'][si : si + self.context_len])\n",
    "            current_troops_count = (traj['current_troops_count'][si : si + self.context_len])\n",
    "\n",
    "            action_masks = torch.cat([action_masks,\n",
    "                                torch.zeros(([padding_len] + list(action_masks.shape[1:])),\n",
    "                                dtype=action_masks.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            current_agent_simple = torch.cat([current_agent_simple,\n",
    "                                torch.zeros(([padding_len] + list(current_agent_simple.shape[1:])),\n",
    "                                dtype=current_agent_simple.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "            current_agent = torch.cat([current_agent,\n",
    "                                torch.zeros(([padding_len] + list(current_agent.shape[1:])),\n",
    "                                dtype=current_agent.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            current_phase = torch.cat([current_phase,\n",
    "                                torch.zeros(([padding_len] + list(current_phase.shape[1:])),\n",
    "                                dtype=current_phase.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "            current_troops_count = torch.cat([current_troops_count,\n",
    "                                torch.zeros(([padding_len] + list(current_troops_count.shape[1:])),\n",
    "                                dtype=current_troops_count.dtype\n",
    "                                           )], \n",
    "                               dim=0)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
    "\n",
    "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long), \n",
    "                                   torch.zeros(padding_len, dtype=torch.long)], \n",
    "                                  dim=0)\n",
    "            \n",
    "        return  timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,current_agent_simple,current_agent,current_phase,current_troops_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729b32d-eec1-49a3-b168-a87120dce1a1",
   "metadata": {},
   "source": [
    "#  Algorithm module definition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ed3db-1fea-4eff-be7f-7fbb1f268b18",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "1176a5a9-77d5-4dd5-94d0-be2dd912f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN_module:\n",
    "    def __init__(self, qnet_config_dict, actor_config_dict,args,device,writer,run_name,agent):\n",
    "\n",
    "        self.agent = agent\n",
    "        self.run_name =run_name \n",
    "        self.actor_config_dict = actor_config_dict\n",
    "        self.qnet_config_dict = qnet_config_dict\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.writer = writer\n",
    "        self.entropy=args.entropy\n",
    "        self.return_prob=args.return_prob\n",
    "        self.actor_wt = args.actor_wt\n",
    "        self.CE_wt = args.CE_wt\n",
    "        if self.args.small:\n",
    "            self.actor = Actor_ddqn_small(**self.actor_config_dict).to(self.device)\n",
    "            self.qf1 = QNetwork_small(**self.qnet_config_dict).to(self.device)\n",
    "            self.qf1_target = QNetwork_small(**self.qnet_config_dict).to(self.device)\n",
    "            self.target_actor = Actor_ddqn_small(**self.actor_config_dict).to(self.device)\n",
    "            \n",
    "        else:\n",
    "            self.actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "            self.qf1 = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "            self.qf1_target = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "            self.target_actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def action_predict(self,data):\n",
    "        return self.actor(data)\n",
    "\n",
    "    def train_write(self,data,iteration,epoch):\n",
    "        #data = rb.sample(self.args.batch_size)\n",
    "        qf1_a_values, qf1_loss, actor_loss = self.train(data)\n",
    "        self.write(qf1_a_values, qf1_loss, actor_loss,epoch,iteration)\n",
    "\n",
    "    def cal_q_loss(self,qf1_a_values, next_q_value):\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        \n",
    "        return criterion(qf1_a_values, next_q_value)\n",
    "        #return nn.functional.mse_loss(qf1_a_values, next_q_value)\n",
    "\n",
    "    \n",
    "    def cal_actor_loss(self,data,entropy=False,return_prob=False):\n",
    "\n",
    "        if not entropy:\n",
    "            return -self.qf1(data.observations[:,:-1], self.actor(data.observations[:,:-1])).mean()\n",
    "        else:\n",
    "            actions,probs = self.actor(data.observations[:,:-1],return_prob=return_prob)\n",
    "            action_loss = -self.qf1(data.observations[:,:-1],actions).mean()\n",
    "\n",
    "            entropy_ = -(probs*torch.log2(probs)).sum(-1).mean()\n",
    "            return self.actor_wt*action_loss + self.CE_wt*entropy_# if not torch.isnan(cross_entropy_loss) else 0)\n",
    "        \n",
    "\n",
    "    def train(self,data):\n",
    "\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            collected_t_next = data.next_observations[:,-1]\n",
    "            next_state_actions = self.target_actor(data.next_observations[:,:-1])\n",
    "            qf1_next_target = self.qf1_target(data.next_observations[:,:-1], next_state_actions)\n",
    "\n",
    "            \n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * (self.args.gamma**(collected_t_next+1)).view(-1) * (qf1_next_target).view(-1)\n",
    "    \n",
    "        qf1_a_values = self.qf1(data.observations[:,:-1], data.actions).view(-1)\n",
    "        qf1_loss = self.cal_q_loss(qf1_a_values, next_q_value)\n",
    "        \n",
    "        # optimize the model\n",
    "        self.q_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        \n",
    "        #if global_step % args.policy_frequency == 0:\n",
    "        actor_loss = self.cal_actor_loss(data,entropy=self.entropy,return_prob=self.return_prob)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "    \n",
    "        # update the target network\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):\n",
    "            target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "        return qf1_a_values, qf1_loss, actor_loss\n",
    "        \n",
    "\n",
    "    def write(self,qf1_a_values, qf1_loss, actor_loss,epoch,iteration):\n",
    "        \n",
    "        ind_epoch = epoch + (iteration-1)*self.args.update_epochs\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_values\", qf1_a_values.mean().item(), ind_epoch)\n",
    "        \n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/qf1_loss\", qf1_loss.item(), ind_epoch)\n",
    "        self.writer.add_scalar(f\"losses/{self.agent}/actor_loss\", actor_loss.item(), ind_epoch)\n",
    "        \n",
    "    def save_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.actor.state_dict(), newpath+\"/actor.pt\")\n",
    "        torch.save(self.qf1.state_dict(), newpath+\"/qf1.pt\")\n",
    "        torch.save(self.qf1_target.state_dict(), newpath+\"/qf1_target.pt\")\n",
    "        torch.save(self.target_actor.state_dict(), newpath+\"/target_actor.pt\")       \n",
    "    \n",
    "\n",
    "    def load_models(self):\n",
    "        newpath = r'./models/'+ self.run_name +'/'+str(self.agent)\n",
    "        self.actor.load_state_dict(torch.load(newpath+\"/actor.pt\"))\n",
    "        self.qf1.load_state_dict(torch.load(newpath+\"/qf1.pt\"))\n",
    "        self.qf1_target.load_state_dict(torch.load(newpath+\"/qf1_target.pt\"))\n",
    "        self.target_actor.load_state_dict(torch.load(newpath+\"/target_actor.pt\")) \n",
    "        \n",
    "\n",
    "                                          \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607adb60-7e78-4345-915e-092f9836dc6f",
   "metadata": {},
   "source": [
    "## transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeccf5-0b86-485a-90f2-a453a209dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_model:\n",
    "    def __init__(self,hero = 1):\n",
    "        self.hero = hero\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        act_dim = env.action_space.n #3#1 #env.action_space.shape[0]\n",
    "        #context_len_=200\n",
    "        model = DecisionTransformer(\n",
    "                    \n",
    "                    state_dim=state_dim,\n",
    "                    act_dim=act_dim,\n",
    "                    n_blocks=n_blocks,\n",
    "                    h_dim=embed_dim,\n",
    "                    context_len=context_len,\n",
    "                    n_heads=n_heads,\n",
    "                    drop_p=dropout_p,\n",
    "                ).to(device)\n",
    "          \n",
    "        optimizer_2 = torch.optim.AdamW(\n",
    "                            model.parameters(), \n",
    "                            lr=0.000005,#lr, \n",
    "                            weight_decay=wt_decay\n",
    "                        )\n",
    "        \n",
    "        \n",
    "        #lr = 0.00001\n",
    "        optimizer_1 = torch.optim.AdamW(\n",
    "                            model.parameters(), \n",
    "                            lr=lr,#lr, \n",
    "                            weight_decay=wt_decay\n",
    "                        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                optimizer_1,\n",
    "                lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "            )\n",
    "        \n",
    "        #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_1, T_0=150, T_mult=2, eta_min=0.01, last_epoch=-1)\n",
    "        \n",
    "        max_d4rl_score = -1000.0\n",
    "        total_updates = 0\n",
    "        \n",
    "        \n",
    "        if training_division :\n",
    "            freeze_non_value_layers(model,value=False)\n",
    "            value_cal_loss = False\n",
    "        else:\n",
    "            value_cal_loss = True\n",
    "    def action_loss_fn(self,action_preds_2,action_mask,return_preds_2,returns_target,beta = 0.2):\n",
    "        #return nn.CrossEntropyLoss().forward(action_preds_2,action_target)\n",
    "        returns_to_go_actor = action_mask*returns_target[:,None]\n",
    "    \n",
    "        loss = (F.softmax( action_preds_2,-1)*((beta*(1 - action_mask) + action_mask)*torch.pow(returns_to_go_actor - action_preds_2,2).detach())).mean()\n",
    "        return loss\n",
    "    \n",
    "    def value_loss_fn(self,return_preds,returns_target,action_mask,beta=0.2):\n",
    "        returns_to_go_actor = action_mask*returns_target[:,None]\n",
    "        loss = ((beta*(1 - action_mask) + action_mask)*torch.pow(returns_to_go_actor - return_preds,2)).mean()\n",
    "        \n",
    "        return loss#F.mse_loss(return_preds, returns_to_go_actor, reduction='mean')\n",
    "    \n",
    "    def value_loss_fn_2(self,return_preds,return_preds_2,returns_target,\n",
    "                        action_preds_2,reward,action_target,\n",
    "                        beta=0.2,alpha=1,gamma =0.99,device=device):\n",
    "    \n",
    "        part_1 =  alpha*(torch.log(torch.exp(return_preds_2).sum(axis =-1)) - (action_mask*return_preds_2).sum(axis =-1) ).mean()\n",
    "    \n",
    "        c_ = (F.one_hot(action_target).reshape(action_preds.shape)*return_preds).sum(-1)[:,:,None].to(device)\n",
    "        c = torch.roll(c_, shifts=-1, dims=-2).to(device)\n",
    "        c[:,-1,:]=0 # this is an issue when context length is not equal to the episode len, .... maybe make use of target mask? and add a sudo \n",
    "    \n",
    "        #print(reward[:,:-1,None].device, c[:,:-1,:].device, return_preds[:,:-1,:].device)\n",
    "        \n",
    "        part_2 = 0.5*torch.pow(reward[:,:-1,None] + gamma*c[:,:-1,:] - return_preds[:,:-1,:],2).mean()\n",
    "        loss =  part_2 + part_1 #+ \n",
    "        #returns_to_go_actor = action_mask*returns_target[:,None]\n",
    "        #loss = ((beta*(1 - action_mask) + action_mask)*torch.pow(returns_to_go_actor - return_preds,2)).mean()\n",
    "        #print(part_1,part_2)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_loss(self,timesteps,\n",
    "                            action_target,\n",
    "                            returns_target,\n",
    "                            state_preds, \n",
    "                            action_preds,\n",
    "                            return_preds,\n",
    "                            reward,\n",
    "                            returns_to_go):\n",
    "        \n",
    "        # only consider non padded elements\n",
    "        action_preds_2 = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0].to(device)\n",
    "        action_target = action_target.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(device)\n",
    "        return_preds_2 = return_preds.squeeze().view(-1, act_dim)[traj_mask.view(-1,) > 0].to(device)\n",
    "        return_target = returns_to_go.squeeze().view(-1)[traj_mask.view(-1,) > 0].to(device)\n",
    "        action_mask = F.one_hot(action_target).squeeze().to(device)\n",
    "        policy_loss = nn.CrossEntropyLoss().forward(action_preds_2,action_target)#action_loss_fn(action_preds_2,action_mask,return_preds_2,return_target,beta = 1)\n",
    "        \n",
    "        if value_cal_loss:\n",
    "            Q_loss = self.value_loss_fn_2(return_preds,\n",
    "                                     return_preds_2,\n",
    "                                     returns_target,\n",
    "                                     action_preds_2,\n",
    "                                     reward,\n",
    "                                     action_target,\n",
    "                                     beta=0.2,alpha=2,gamma =0.99)#value_loss_fn(return_preds_2,return_target,action_mask,beta=1)\n",
    "            total_loss = Q_loss+policy_loss\n",
    "        else:\n",
    "            total_loss = policy_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    \n",
    "    def train_write(self,data):\n",
    "        \n",
    "        timesteps, states, actions, returns_to_go,reward, traj_mask ,action_masks,current_agent_simple,current_agent,current_phase,current_troops_count = data\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        timesteps = timesteps[0].to(self.device)\t# B x T\n",
    "        states = states[0].to(self.device)\t\t\t# B x T x state_dim\n",
    "        actions = actions[0].to(self.device)\t\t# B x T x act_dim\n",
    "        reward = reward[0].to(self.device)\n",
    "        returns_to_go = returns_to_go[0].to(self.device).unsqueeze(dim=-1) # B x T x 1\n",
    "        traj_mask = traj_mask[0].to(self.device)\t# B x T\n",
    "        action_masks =action_masks[0].to(self.device)\n",
    "        current_agent_simple = current_agent_simple[0].to(self.device)\n",
    "        current_agent =current_agent[0].to(self.device)\n",
    "        current_phase =current_phase[0].to(self.device)\n",
    "        current_troops_count =current_troops_count[0].to(self.device)\n",
    "\n",
    "        \n",
    "        info = dict({})\n",
    "        \n",
    "        hero_steps = (current_agent_simple == self.hero )\n",
    "        \n",
    "        states = torch.concat(\n",
    "            \n",
    "                \n",
    "                (states,current_phase,current_agent\n",
    "            \n",
    "            ,torch.ones(len(action_masks))[:,None]*self.hero\n",
    "            ,current_troops_count[:,self.hero,None]\n",
    "            ,action_masks*hero_steps[:,None]), axis =1)[0] \n",
    "                \n",
    "            \n",
    "\n",
    "        action_target,returns_target = torch.clone(actions).detach().to(device),torch.clone(actions).detach().to(device)\n",
    "        \n",
    "        state_preds, action_preds, return_preds = model.forward(\n",
    "                                                        timesteps=timesteps,\n",
    "                                                        states=states,\n",
    "                                                        actions=actions,\n",
    "                                                        returns_to_go=returns_to_go\n",
    "                                                        ,info = info\n",
    "            \n",
    "\n",
    "            \n",
    "                                                    )\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        total_loss = self.calculate_loss(timesteps,\n",
    "                                   action_target,\n",
    "                                   returns_target,\n",
    "                                   state_preds, \n",
    "                                   action_preds,\n",
    "                                   return_preds,\n",
    "                                   reward,\n",
    "                                   returns_to_go)\n",
    "\n",
    "\n",
    "        #action_loss = #nn.CrossEntropyLoss().forward(action_preds_2,action_target)#F.mse_loss(action_preds_2, action_target.float(), reduction='mean')\n",
    "        optimizer_1.zero_grad()\n",
    "        #policy_loss.backward(retain_graph=True)\n",
    "        (total_loss).backward()   \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer_1.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        #if value_cal_loss:\n",
    "        #    q_los = Q_loss.detach().cpu().item()\n",
    "        #    writer.add_scalar(\"Q_loss\", q_los, iyr+i_train_iter*num_updates_per_iter)\n",
    "        #    log_Q_losses.append(q_los)\n",
    "        #\n",
    "        #pol_los = policy_loss.detach().cpu().item()\n",
    "        #writer.add_scalar(\"Policy_loss\", pol_los, iyr+i_train_iter*num_updates_per_iter)\n",
    "        #log_action_losses.append(pol_los)\n",
    "                \n",
    "            \n",
    "                \n",
    "            \t## evaluate on env   \n",
    "                #results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
    "            \t#                        num_eval_ep, max_eval_ep_len, state_mean, state_std, \n",
    "            \t#\t\t\t\t\t\t)\n",
    "            #\n",
    "                #writer.add_scalar(\"eval/avg_reward\", results['eval/avg_reward'], i_train_iter)\n",
    "                #writer.add_scalar(\"eval/avg_ep_len\", results['eval/avg_ep_len'], i_train_iter)\n",
    "                #writer.add_scalar(\"eval/'total_reward\",results['total_reward'],i_train_iter)\n",
    "            #\n",
    "            #\n",
    "                #\n",
    "                #eval_avg_reward = results['eval/avg_reward']\n",
    "                #eval_avg_ep_len = results['eval/avg_ep_len']\n",
    "                #eval_d4rl_score = results['eval/avg_reward']#get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
    "                #mean_action_loss,mean_Q_loss = np.mean(log_action_losses),np.mean(log_Q_losses)\n",
    "                #\n",
    "                #writer.add_scalar(\"mean_action_loss_per_iter\", mean_action_loss, i_train_iter)\n",
    "                #writer.add_scalar(\"mean_Q_loss_per_iter\", mean_Q_loss, i_train_iter)\n",
    "            #\n",
    "            #\n",
    "                #\n",
    "                #time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
    "                #total_updates += num_updates_per_iter\n",
    "                #log_str = (\"=\" * 60 + '\\n' +\n",
    "            \t#\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n",
    "            \t#\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n",
    "            \t#\t\t\"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
    "            \t#\t\t\"Q loss: \" +  format(mean_Q_loss, \".5f\") + '\\n' +\n",
    "            \t#\t\t\"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
    "            \t#\t\t\"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
    "            \t#\t\t\"eval d4rl score: \" + format(eval_d4rl_score, \".5f\") +'\\n'+\n",
    "                #        \" iterations: \"+str(i_train_iter) +\" calulate_value_loss :\" +str(value_cal_loss)\n",
    "            \t#\t\t)\n",
    "            #\n",
    "                #print(log_str)\n",
    "            #\n",
    "                #log_data = [time_elapsed, total_updates, mean_action_loss,mean_Q_loss,\n",
    "            \t#\t\t\teval_avg_reward, eval_avg_ep_len,\n",
    "            \t#\t\t\teval_d4rl_score]\n",
    "            #\n",
    "                #csv_writer.writerow(log_data)\n",
    "            \t#\n",
    "            \t## save model\n",
    "                #print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "                #if eval_d4rl_score >= max_d4rl_score:\n",
    "                #    print(\"saving max d4rl score model at: \" + save_best_model_path)\n",
    "                #    torch.save(model.state_dict(), save_best_model_path)\n",
    "                #    max_d4rl_score = eval_d4rl_score\n",
    "            #\n",
    "                #print(\"saving current model at: \" + save_model_path)\n",
    "                #torch.save(model.state_dict(), save_model_path)\n",
    "                \n",
    "            #writer.add_hparams(dict(exp_name=log_csv_name,env_name=env_name,state_dim=state_dim,\n",
    "            #\t\t\tact_dim=int(act_dim),\n",
    "            #\t\t\tn_blocks=n_blocks,\n",
    "            #\t\t\th_dim=embed_dim,\n",
    "            #\t\t\tcontext_len=context_len,\n",
    "            #\t\t\tn_heads=n_heads,\n",
    "            #\t\t\tdrop_p=dropout_p,training_division=training_division,\n",
    "            #            start_value_training_iteration=start_value_training_iteration,optim=optimizer_1.__class__.__name__,lr=lr,\n",
    "            #                       scheduler=scheduler.__class__.__name__, sch_param = \",\".join([ i+\":\"+str(j) for i,j in dict(T_0=150, T_mult=2, eta_min=0.01, last_epoch=-1).items()])\n",
    "            #                       ,batch_size=batch_size),{'hparam/eval_avg_reward':eval_avg_reward})            \n",
    "            \n",
    "            #print(\"=\" * 60)\n",
    "            #print(\"finished training!\")\n",
    "            #print(\"=\" * 60)\n",
    "            #end_time = datetime.now().replace(microsecond=0)\n",
    "            #time_elapsed = str(end_time - start_time)\n",
    "            #end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "            #print(\"started training at: \" + start_time_str)\n",
    "            #print(\"finished training at: \" + end_time_str)\n",
    "            #print(\"total training time: \" + time_elapsed)\n",
    "            #print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
    "            #print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
    "            #print(\"saved last updated model at: \" + save_model_path)\n",
    "            #print(\"=\" * 60)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "5758041a-cc98-4889-a86e-0f6d127dba0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hero = 1\n",
    "(((T.paths[0]).dataset.trajectories[0]['current_agent']*torch.tensor([[1,2,3]])).sum(-1) == (hero-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "80b8b352-52ad-4ccf-b3fe-7016d41bea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_mask = (T.paths[0]).dataset.trajectories[0]['action_masks']*hero_steps[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "1a9672ee-630c-4613-b20e-8915867e9347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['observations', 'next_observations', 'rewards', 'terminals', 'actions', 'action_masks', 'current_agent', 'current_phase', 'current_troops_count', 'returns_to_go'])"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((T.paths[0]).dataset.trajectories[0]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "db56453b-e05d-441e-b034-80fd987aea43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(timesteps, states, actions, returns_to_go,reward, traj_mask ,\n",
    "             action_masks,current_agent,current_phase,current_troops_count) = [i for i in T.traj_data_loader ][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "616f0aa3-a861-4d34-8f27-82175682c0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1601, 256, 10, 5])"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "67a7a180-e58d-471f-9f64-37f9904aec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1601, 256])"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].shape[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "4e60fc38-c88f-4b04-8bb6-d11342b976f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.6905, -1.3072,  0.0000, -1.0107, -0.0668, -1.0696, -0.7206,\n",
       "        -0.2330, -0.7806, -0.2660, -0.7691, -0.7559, -0.4148, -0.9676,  0.0000,\n",
       "        -0.7129, -0.8337, -0.5691, -1.2656, -0.4469, -0.6254, -1.0183,  0.0000,\n",
       "        -1.0231, -0.5956, -0.6374, -0.8425, -0.0758, -1.0404, -0.5329, -0.5539,\n",
       "        -0.9112,  0.0000, -1.3050, -0.5437, -0.6443, -0.8929,  0.0000, -1.3344,\n",
       "         0.0000, -1.2906, -0.6843,  0.0000, -1.2122,  0.0000, -2.4390, -0.3955,\n",
       "         0.0000, -0.7329])"
      ]
     },
     "execution_count": 959,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].reshape(list(states[0].shape[:-2]) + [np.prod(states[0].shape[-2:])])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "4352f7de-f0d7-4b6a-bde4-c54816c5113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1601, 256, 52])"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((states[0].reshape(list(states[0].shape[:-2]) + [np.prod(states[0].shape[-2:])]),\n",
    "              \n",
    "              actions[0]),axis =-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "1f512a05-ff31-45ab-bcad-91827f4ebb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, -0.6905, -1.3072,  0.0000, -1.0107, -0.0668, -1.0696, -0.7206,\n",
       "        -0.2330, -0.7806, -0.2660, -0.7691, -0.7559, -0.4148, -0.9676,  0.0000,\n",
       "        -0.7129, -0.8337, -0.5691, -1.2656, -0.4469, -0.6254, -1.0183,  0.0000,\n",
       "        -1.0231, -0.5956, -0.6374, -0.8425, -0.0758, -1.0404, -0.5329, -0.5539,\n",
       "        -0.9112,  0.0000, -1.3050, -0.5437, -0.6443, -0.8929,  0.0000, -1.3344,\n",
       "         0.0000, -1.2906, -0.6843,  0.0000, -1.2122,  0.0000, -2.4390, -0.3955,\n",
       "         0.0000, -0.7329,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "         3.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "         1.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "         1.0000])"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hero = 1\n",
    "hero_steps = (((T.paths[0]).dataset.trajectories[0]['current_agent']*torch.tensor([[1,2,3]])).sum(-1) == (hero-1) )\n",
    "\n",
    "torch.concat(\n",
    "    (\n",
    "        torch.concat(\n",
    "                    (\n",
    "                T.paths[0].dataset.trajectories[0]['observations'].reshape(-1,np.prod(T.ob_space_shape))\n",
    "                ,T.paths[0].dataset.trajectories[0]['current_phase']\n",
    "                ,T.paths[0].dataset.trajectories[0]['current_agent']), axis =1)\n",
    "\n",
    ",torch.ones(len(T.paths[0].dataset.trajectories[0]['action_masks']))[:,None]*hero\n",
    ",T.paths[0].dataset.trajectories[0]['current_troops_count'][:,hero,None]\n",
    ",T.paths[0].dataset.trajectories[0]['action_masks']*hero_steps[:,None]), axis =1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "ecfd8ead-45aa-4834-92ab-8d4315a3f41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3.,  ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(T.paths[0]).dataset.trajectories[0]['observations'].reshape(-1,np.prod(T.ob_space_shape))\n",
    "torch.ones(len((T.paths[0]).dataset.trajectories[0]['action_masks']))*1\n",
    "(T.paths[0]).dataset.trajectories[0]['action_masks']*hero_steps[:,None]\n",
    "((T.paths[0]).dataset.trajectories[0])['current_phase']\n",
    "((T.paths[0]).dataset.trajectories[0])['current_agent']\n",
    "((T.paths[0]).dataset.trajectories[0])['current_troops_count'][:,1]\n",
    "\n",
    "\n",
    "((T.paths[0]).dataset.trajectories[0])['returns_to_go'][:,1]\n",
    "((T.paths[0]).dataset.trajectories[0])['actions'][:]*hero_steps[:,None]\n",
    "\n",
    "((T.paths[0]).dataset.trajectories[0])['rewards'][:,1]#*hero_steps[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ef3c7-36d0-44bf-9ac6-76b8e52904f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero_steps = [current_agent == agent] # agent = hero_agent\n",
    "\n",
    "b_obs_a_all = torch.concat(( obs.reshape(-1,np.prod(self.ob_space_shape)) ,\n",
    "                            action_masks.reshape(-1,np.prod(self.action_mask_shape)),\n",
    "                            self.map_agent_phase_vector(current_agent,num_classes=self.total_agents+1)[:,1:],\n",
    "                            \n",
    "                            #map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)),\n",
    "                            self.map_agent_phase_vector(current_phase,num_classes=self.total_phases)#,\n",
    "                            #current_troops_count[:,self.the_hero_agent-1,None],\n",
    "                            #selected_t_next\n",
    "                           ),axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339b877-cdf8-4dd8-81b8-955389a65193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            \n",
    "\n",
    "            t_range = torch.Tensor(np.arange(0,step)).to(self.device,dtype=torch.int)\n",
    "            hero_steps = [current_agent == self.the_hero_agent][0][:,0][:step]\n",
    "            \n",
    "            next_indecies = (t_next[:step,self.the_hero_agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "        \n",
    "            selected_t_next = t_next[:,self.the_hero_agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "            infos = [dir({})]*step #t_next[:step,0]\n",
    "            b_obs_a_all = torch.concat(( obs.reshape(-1,np.prod(self.ob_space_shape)) ,\n",
    "                                    action_masks.reshape(-1,np.prod(self.action_mask_shape)),\n",
    "                                    self.map_agent_phase_vector(current_agent,num_classes=self.total_agents+1)[:,1:],\n",
    "                                    \n",
    "                                    #map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)),\n",
    "                                    self.map_agent_phase_vector(current_phase,num_classes=self.total_phases)#,\n",
    "                                    #current_troops_count[:,self.the_hero_agent-1,None],\n",
    "                                    #selected_t_next\n",
    "                                   ),axis =1)\n",
    "\n",
    "\n",
    "\n",
    "        avg_episode_length = torch.mean(torch.tensor(\n",
    "                            [(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()]).float())#np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "        if self.args.TB_log:\n",
    "            self.writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "        return paths\n",
    "        #return rb\n",
    "\n",
    "    def update_hero_rb(self,b_obs_a_all,t_next,t_range,current_agent,actions,\n",
    "                                rewards,dones,infos,current_troops_count,\n",
    "                                step):\n",
    "\n",
    "\n",
    "\n",
    "            for agent in self.hero_agents_list:\n",
    "                #the_hero_agent = agent\n",
    "                \n",
    "                \n",
    "                hero_steps = [current_agent == agent][0][:,0][:step]\n",
    "                next_indecies = (t_next[:step,agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "                selected_t_next = t_next[:,agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "\n",
    "                b_obs_a = torch.concat((b_obs_a_all,\n",
    "                                        current_troops_count[:,agent-1,None],\n",
    "                                        selected_t_next),axis =1)\n",
    "                \n",
    "                for i in zip(b_obs_a[:step][hero_steps].cpu().to(dtype=torch.float), b_obs_a[next_indecies].cpu().to(dtype=torch.float), \n",
    "                   actions[:step][hero_steps].cpu().to(dtype=torch.float32), rewards[:step][hero_steps][:,agent-1,None].cpu(), \n",
    "                   dones[:step][hero_steps][:,agent-1,None].cpu(), infos):\n",
    "                    self.hero_agents_list[agent].rb.add(*i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce951d8-af89-4bc1-8b2b-c1550a0d5977",
   "metadata": {},
   "source": [
    "# trainer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "023e851a-095e-4609-8e20-8fe69959bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ## Trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Args,param_dict =dict({})\n",
    "                 ):\n",
    "        # #### Configurations\n",
    "\n",
    "        self.args = Args()#tyro.cli(Args)\n",
    "        self.param_dict = param_dict\n",
    "        self.update_arg(param_dict=param_dict)\n",
    "        self.device = self.args.device#torch.device(\"cuda\" if torch.cuda.is_available() and self.args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        #self.args.batch_size = int(self.args.num_envs * self.args.num_steps)\n",
    "        self.args.minibatch_size = int(self.args.batch_size // self.args.num_minibatches)\n",
    "        #self.args.num_iterations = self.args.total_timesteps // self.args.batch_size\n",
    "        self.gam = self.args.gamma\n",
    "        #self.args.minibatch_size = 256#128 \n",
    "        self.num_steps = self.args.num_steps#120000#1000000\n",
    "        self.num_iterations = self.args.num_iterations\n",
    "        self.episode_time_lim = self.args.episode_time_lim\n",
    "        self.hero_agent_count = self.args.hero_agent_count\n",
    "        self.env_config = self.args.env_config\n",
    "        self.num_episodes = self.args.num_episodes\n",
    "        self.context_len=200\n",
    "        \n",
    "        #self.env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "        #                    agent_count  = 4\n",
    "        #                       ,use_placement_perc=True,render_=False)        \n",
    "        \n",
    "        self.run_name = f\"{self.args.env_id}__{self.args.exp_name}__{self.args.seed}__{int(time.time())}\"\n",
    "\n",
    "        \n",
    "\n",
    "        TB_log = self.args.TB_log \n",
    "        if TB_log:    \n",
    "            self.writer = SummaryWriter(f\"runs/{self.run_name}\")\n",
    "            self.writer.add_text(\n",
    "                \"hyperparameters\",\n",
    "                \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "            )\n",
    "        else:\n",
    "            self.writer = None\n",
    "        \n",
    "        # TRY NOT TO MODIFY: seeding\n",
    "        random.seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        #torch.manual_seed(self.args.seed)\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = self.args.torch_deterministic\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.playe_r = 1#\"agent_1\" #\n",
    "        \n",
    "\n",
    "        \n",
    "        self.action_shape = (2,)\n",
    "\n",
    "\n",
    "\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        \n",
    "        self.env.reset(seed=42)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.total_agents  = len(self.env.possible_agents)\n",
    "        self.total_phases = len(self.env.phases)\n",
    "        #print(self.env.last()[0]['observation'])\n",
    "\n",
    "        print(torch.tensor(self.env.last()[0]['observation']\n",
    "                          ,device=self.device))\n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']\n",
    "                                                    ).to(device=self.device),\n",
    "                                        num_classes = self.total_agents+1\n",
    "                                       )\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        \n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "        \n",
    "        \n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "        \n",
    "        self.qnet_config_dict = dict(action_space = self.env.action_space(self.playe_r\n",
    "                                                                         ).shape[0],\n",
    "                                    ob_space=np.prod(self.ob_space_shape\n",
    "                                                    )+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents -1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=np.prod(self.ob_space_shape)+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        \n",
    "        \n",
    "        self.hero_agents_list = {i:Hero_agent(i) for i in range(1,self.hero_agent_count+1) } # this is a list , need to pass it as an argument\n",
    "        \n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_properties(self.total_agents,self.env.phases)        \n",
    "        \n",
    "            self.hero_agents_list[i].model_def(model = DDQN_module( self.qnet_config_dict, self.actor_config_dict,\n",
    "                                                                   self.args,device = self.device,writer=self.writer,run_name=self.run_name,agent=i)\n",
    "                                                )\n",
    "            \n",
    "\n",
    "            #self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "            #self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "            #self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "            #self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "    def update_arg(self,param_dict=dict({})):\n",
    "       for i,j in param_dict.items():\n",
    "           setattr(self.args,i,j)\n",
    "\n",
    "        \n",
    "\n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "\n",
    "            #print(data.device)\n",
    "            #print(nn.functional.one_hot(data[:4,col].detach().long(), \n",
    "            #                                            num_classes = num_classes).to(self.device))\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes).to(self.device),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                            num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "\n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "        \n",
    "        self.draw_count = 0\n",
    "\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_win_count_iter(self.total_agents )\n",
    "        \n",
    "        \n",
    "        #self.first_count = 0\n",
    "        #self.second_count = 0\n",
    "        #self.third_count = 0\n",
    "        #self.third_count_draw = 0\n",
    "        \n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "        #self.faulting_player = \"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_training_loop(self):\n",
    "        \"\"\"\n",
    "        ### Run training loop\n",
    "        \"\"\"\n",
    "\n",
    "        # last 100 episode information\n",
    "        #tracker.set_queue('reward', 100, True)\n",
    "        #tracker.set_queue('length', 100, True)\n",
    "\n",
    "        obs = torch.zeros((self.num_steps,) + self.ob_space_shape).to(self.device)\n",
    "        actions = torch.zeros((self.num_steps, ) + self.action_shape).to(self.device)\n",
    "        action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape).to(self.device)\n",
    "        current_agent = torch.ones((self.num_steps,1)).to(self.device)*0#-1\n",
    "        current_phase = torch.zeros((self.num_steps,1)).to(self.device)\n",
    "        current_troops_count = torch.zeros((self.num_steps,self.total_agents)).to(self.device)\n",
    "        logprobs = torch.zeros((self.num_steps, )).to(self.device)\n",
    "        rewards = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        rewards_2 = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        dones = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "        episodes = torch.ones((self.num_steps, )).to(self.device)*-1\n",
    "        t_next = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "\n",
    "\n",
    "        for i in self.hero_agents_list: # each agent has his own buffer, this is kinda pain because now this information is stored and not discarded\n",
    "    \n",
    "            self.hero_agents_list[i].rb = ReplayBuffer(\n",
    "                    self.args.buffer_size,\n",
    "                    Box(low =0, high=2000, shape =(self.qnet_config_dict['ob_space']+1,), dtype=np.float32),\n",
    "                    Box(low =0, high=2000, shape =(2,), dtype=np.float32),\n",
    "                    self.device,\n",
    "                    handle_timeout_termination=False,\n",
    "                )\n",
    "\n",
    "        env = env_risk(**(self.env_config #| {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}\n",
    "                         ))\n",
    "        env.reset(42)\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        self.paths=[]\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "\n",
    "            obs = torch.zeros((self.num_steps,) + self.ob_space_shape).to(self.device)\n",
    "            actions = torch.zeros((self.num_steps, ) + self.action_shape).to(self.device)\n",
    "            action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape).to(self.device)\n",
    "            current_agent = torch.ones((self.num_steps,1)).to(self.device)*0#-1\n",
    "            current_phase = torch.zeros((self.num_steps,1)).to(self.device)\n",
    "            current_troops_count = torch.zeros((self.num_steps,self.total_agents)).to(self.device)\n",
    "            logprobs = torch.zeros((self.num_steps, )).to(self.device)\n",
    "            rewards = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            rewards_2 = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            dones = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            dones_2 = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            #values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "            episodes = torch.ones((self.num_steps, )).to(self.device)*-1\n",
    "            t_next = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            self.paths = self.sample(\n",
    "                                env,iteration,\n",
    "                                obs\n",
    "                                ,actions\n",
    "                                ,action_masks\n",
    "                                ,current_agent\n",
    "                                ,current_phase\n",
    "                                ,current_troops_count\n",
    "                                ,logprobs\n",
    "                                ,rewards\n",
    "                                ,rewards_2\n",
    "                                ,dones\n",
    "                                ,dones_2\n",
    "                                ,values\n",
    "                                ,episodes\n",
    "                                ,t_next\n",
    "                                \n",
    "                 \n",
    "                            )\n",
    "\n",
    "            \n",
    "\n",
    "            #self.traj_dataset = D4RLTrajectoryDataset(self.paths, context_len=self.args.context_len,\n",
    "            #                                             rtg_scale=self.args.rtg_scale,gamma=self.args.gamma)\n",
    "            #                                            #rtg_scale=1,gamma=0.99\n",
    "    \n",
    "            #traj_dataset = traj_dataset.to(self.args.pin_memory_device)\n",
    "            \n",
    "            #self.traj_data_loader = DataLoader(self.traj_dataset,\n",
    "            #\t\t\t\t\t\tbatch_size=self.args.batch_size,\n",
    "            #                        shuffle= self.args.shuffle,\n",
    "            #                        pin_memory= self.args.pin_memory,\n",
    "            #                        drop_last = self.args.drop_last,\n",
    "            #                        pin_memory_device=self.args.pin_memory_device\n",
    "            #                        #shuffle=True,\n",
    "            #                        #pin_memory=True,\n",
    "            #                        #drop_last=True\n",
    "            #                        )\n",
    "\n",
    "            self.traj_dataset = TrajectoryDataset_2_through_episodes(self.paths ) # a dataset of dataloaders\n",
    "        \n",
    "            self.traj_data_loader = DataLoader(self.traj_dataset, batch_size=1,shuffle=self.args.shuffle,\n",
    "                                    pin_memory= self.args.pin_memory,\n",
    "                                    drop_last = self.args.drop_last,\n",
    "                                    pin_memory_device=self.args.pin_memory_device) # only spit 1 episode a time\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            #self.data_iter = iter(self.traj_data_loader)\n",
    "            \n",
    "            ## get state stats from dataset\n",
    "            #state_mean, state_std = self.traj_dataset.get_state_stats()\n",
    "\n",
    "            #print(len(self.paths))\n",
    "            #print(len(traj_dataset))\n",
    "            \n",
    "            #print(len(self.data_iter))\n",
    "            #print(next(data_iter))\n",
    "            #(timesteps, states, actions, returns_to_go,reward, traj_mask ,\n",
    "            # action_masks,current_agent,current_phase,current_troops_count) = next(self.data_iter)\n",
    "\n",
    "            #for batch in self.traj_data_loader\n",
    "\n",
    "            #print(states.shape)\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            self.train(iteration)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            #if self.global_step%100 ==0:\n",
    "            #    SPS = int(self.global_step / (time.time() - self.start_time))\n",
    "            #    print(\"SPS:\", SPS)       \n",
    "            #    self.writer.add_scalar(\"charts/SPS\", SPS, self.global_step)\n",
    "        \n",
    "            \n",
    "            self.save_models()\n",
    "\n",
    "    def train(self,iteration):\n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            \n",
    "            if self.global_step > self.args.learning_starts:\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    for batch in self.traj_data_loader\n",
    "                    self.hero_agents_list[i].model.train_write(batch,iteration,epoch)\n",
    "                    \n",
    "    \n",
    "    def train_(self,iteration):\n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            \n",
    "            if self.global_step > self.args.learning_starts:\n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].model.train_write(self.hero_agents_list[i].rb.sample(self.args.batch_size)\n",
    "                                                         ,iteration,epoch)\n",
    "                    \n",
    "    def save_models(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].save_models()  \n",
    "\n",
    "    def reset_moves_hero_agents(self):\n",
    "        for i in self.hero_agents_list:\n",
    "            self.hero_agents_list[i].init_move_count_epi(self.env.phases)\n",
    "    \n",
    "    def sample(self,env,iteration,obs\n",
    "                                ,actions\n",
    "                                ,action_masks\n",
    "                                ,current_agent\n",
    "                                ,current_phase\n",
    "                                ,current_troops_count\n",
    "                                ,logprobs\n",
    "                                ,rewards\n",
    "                                ,rewards_2\n",
    "                                ,dones\n",
    "                                ,dones_2\n",
    "                                ,values\n",
    "                                ,episodes\n",
    "                                ,t_next\n",
    "                                ):\n",
    "        \n",
    "\n",
    "        paths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # sample `worker_steps` from each worker\n",
    "            #there are no worker steps... rather there are full episodes\n",
    "\n",
    "            step = 0\n",
    "            fault_condition = False\n",
    "            clear_output(wait=True)\n",
    "            phase = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            for episode in range(self.num_episodes):#num_episodes):\n",
    "                \n",
    "                total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "                action=1\n",
    "                \n",
    "                if fault_condition:\n",
    "                    env = env_risk(**(self.env_config  #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                                                        # }\n",
    "                                     )\n",
    "                                      )#game.env(render_mode=None)\n",
    "\n",
    "                curren_epi = episode + (iteration-1)*self.num_episodes\n",
    "                env.reset(curren_epi) #for riplication\n",
    "                \n",
    "                fault_condition = False\n",
    "                step_count = 0\n",
    "                \n",
    "                self.reset_moves_hero_agents()\n",
    "                is_draw = 0\n",
    "                \n",
    "                #draw_territory_count = 0\n",
    "                #is_third = 0\n",
    "\n",
    "                for agent in env.agent_iter():\n",
    "                    e_t = env.terminations\n",
    "                    if sum(e_t.values()) <(self.total_agents-1):\n",
    "                        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "                        observation['observation'] =  self.obs_converter(\n",
    "                                                        torch.tensor(\n",
    "                                                            observation['observation']\n",
    "                                                        ).to(self.device,dtype=torch.float32),\n",
    "                                                        num_classes = self.total_agents+1)\n",
    "                        \n",
    "                        episodes[step] = curren_epi\n",
    "                        obs[step] = observation['observation']#torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                        action_masks[step] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                        \n",
    "                        #curr_agent = agent#int(agent[-1])\n",
    "                        current_agent[step] = curr_agent = agent\n",
    "                        current_phase[step] = phase = env.phase_selection\n",
    "                        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "                        \n",
    "                        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                                      num_classes = self.total_agents \n",
    "                                                                     ).float()\n",
    "                        \n",
    "                        current_troops_count[step] = torch.Tensor([env.board.agents[i].bucket for i in env.possible_agents]).to(self.device)\n",
    "                    \n",
    "\n",
    "                        model_in = torch.Tensor(torch.hstack((observation['observation'].reshape(-1),torch.tensor(observation['action_mask'].reshape(-1)).to(self.device),\n",
    "                                           phase_mapping,\n",
    "                                            curr_agent_mapping,\n",
    "                                           torch.tensor([env.board.agents[curr_agent].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "                                                ).float()\n",
    "                        \n",
    "                        #if e_t[curr_agent]:\n",
    "                            #print('heeee')\n",
    "                            \n",
    "                        if termination or truncation: #this never happens ... the agent is removed from the current agent list and processed after the end of the cycle\n",
    "                            \n",
    "                            action = None\n",
    "\n",
    "                            act = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            #act = self.\n",
    "                            #act, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                            #values[step] = value.flatten() # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                                                #have taken at this point and what would have been its value\n",
    "                            actions[step] = act #even after going what would have been\n",
    "                            #logprobs[step] = logprob        \n",
    "                        else:\n",
    "                            mask = observation[\"action_mask\"]\n",
    "                            if (self.global_step < self.args.learning_starts) or (\n",
    "                                np.random.rand() > min(\n",
    "                                                ((curren_epi)/((self.num_iterations*self.num_episodes)/10))\n",
    "                                                , 0.95)\n",
    "                                                #) or (agent != self.the_hero_agent) \n",
    "                                                )or ( agent not in self.hero_agents_list):\n",
    "        \n",
    "                                \n",
    "                                action = env.action_space(agent).sample()\n",
    "                                #part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                                part_0 =np.random.choice(np.where(observation['action_mask'])[0])\n",
    "                                action = torch.Tensor([[[part_0],[np.around(action[1],2)]]]).to(self.device)\n",
    "                                action = action[:,:,0]\n",
    "                            else:\n",
    "                                action = self.hero_agents_list[curr_agent].action_predict(torch.Tensor(model_in).to(self.device))\n",
    "                                #action = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            actions[step] = action\n",
    "                            curr_agent_ = int(curr_agent)\n",
    "        \n",
    "                            if not observation['action_mask'][action[:,0].long()]: \n",
    "                                fault_condition =True\n",
    "                                #self.faulting_player = agent\n",
    "\n",
    "                                \n",
    "\n",
    "\n",
    "                                if  curr_agent_ in self.hero_agents_list:\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_count+=1\n",
    "                                    self.hero_agents_list[curr_agent_].bad_move_phase_count[int(current_phase[step][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                                    #print('here',agent, action, observation['action_mask'])\n",
    "                            \n",
    "        \n",
    "                            if  curr_agent_ in self.hero_agents_list:\n",
    "                                self.hero_agents_list[curr_agent_].move_count[int(current_phase[step][0])]+=1  \n",
    "                            #if self.the_hero_agent == curr_agent:\n",
    "                                #move_count[int(current_phase[step][0])]+=1        \n",
    "        \n",
    "        \n",
    "                        #print('here',agent, action)\n",
    "                        if action != None :\n",
    "                            act_2 = action.detach().cpu().numpy()[0]#list([action.detach().cpu().numpy()[0][0], max(action.detach().cpu().numpy()[0][1],0.1) ])\n",
    "                            act_2 = list([act_2[0], max(act_2[1],0.001) ])\n",
    "                        else:\n",
    "                            act_2 = action\n",
    "                            \n",
    "                        env.step(act_2 if action != None else None)        \n",
    "        \n",
    "        \n",
    "                        if action == None:\n",
    "                            print('heeee')\n",
    "                            rewards[step] = np.zeros(self.total_agents) # should i keep it -1? .... hm i dont think so .\n",
    "                            dones[step] = np.zeros(self.total_agents) # frankly the guys is already done so we really dont have to do anything here.... this is the state post termination for a loser \n",
    "                            # but btw this is for the next agent ... action == None means in the last action the previous agent would have been removed.\n",
    "                            #values[step] = \n",
    "                        else:\n",
    "        \n",
    "                            \n",
    "                            curr_reward_list =  env.curr_rewards\n",
    "                            \n",
    "                            if (step_count == (self.episode_time_lim-1)): # draw reward\n",
    "                                is_draw=1\n",
    "                                curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "\n",
    "                            rewards_2[step] = torch.Tensor([curr_reward_list[i] for i in env.possible_agents]).to(self.device)\n",
    "                            if step >1:\n",
    "                                dones_2[step] = torch.Tensor([ int(env.terminations[i]) - dones_2[step-1,i-1]  for i in env.possible_agents]).to(self.device)\n",
    "                            else:\n",
    "                                dones_2[step] = torch.Tensor([env.terminations[i] for i in env.possible_agents]).to(self.device)\n",
    "                                \n",
    "                            for i in env.possible_agents:\n",
    "                                if i != curr_agent:\n",
    "                                    self.gamma_t[i]+=1\n",
    "                                else:\n",
    "                                    self.gamma_t[i] =0\n",
    "        \n",
    "                                if (step_count == (self.episode_time_lim-1)):\n",
    "                                    cr_rew = -100\n",
    "                                    term = True\n",
    "                                else:\n",
    "                                    cr_rew = env.curr_rewards[i]\n",
    "                                    term = env.terminations[i]\n",
    "\n",
    "                                next_step_ = step-self.gamma_t[i]\n",
    "                                rewards[next_step_,i-1] += (self.args.gamma**self.gamma_t[i])*cr_rew\n",
    "                                t_next[next_step_,i-1] = self.gamma_t[i]\n",
    "                                dones[next_step_,i-1] = torch.Tensor([term]).to(self.device) #so the panetly has to be added but attributions is really difficult\n",
    "        \n",
    "                        #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                        \n",
    "                        #if sum(curr_reward_list.values()) == -300:\n",
    "                            #print('here')\n",
    "                            #is_draw=1\n",
    "        \n",
    "                        \n",
    "                        for age_i in env.possible_agents:\n",
    "                            \n",
    "                            total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                    \n",
    "                        \n",
    "                        step +=1\n",
    "                        self.global_step+=1\n",
    "        \n",
    "                    else:\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        break    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                    step_count+=1\n",
    "                    \n",
    "                    if (self.global_step == self.num_steps) :# or (fault_condition and (fa ulting_player != agent) and (len(env.agents)==0)):\n",
    "                        break\n",
    "                    elif (step_count == self.episode_time_lim):\n",
    "                        break\n",
    "                        \n",
    "                #print(rewards[step-2])\n",
    "                if self.global_step == self.num_steps:\n",
    "                    break \n",
    "\n",
    "                for i in self.hero_agents_list:\n",
    "                    self.hero_agents_list[i].position =self.total_agents\n",
    "                    \n",
    "                #[ position = 3 for i in ] \n",
    "                for k_,(i_,j_) in enumerate(sorted([(j_,i_) for i_,j_ in total_rewards.items()],reverse=True) \n",
    "                      ):\n",
    "                    if int(j_) in self.hero_agents_list:\n",
    "                        self.hero_agents_list[int(j_)].position = k_+1\n",
    "                        \n",
    "                        \n",
    "                    #if j_==self.the_hero_agent:\n",
    "                    #    position = k_+1\n",
    "\n",
    "                cur_epi_list = (episodes == curren_epi)\n",
    "                if self.args.TB_log:\n",
    "                    self.write_exploring(is_draw,#position,\n",
    "                            curren_epi,step,\n",
    "                            total_rewards,#bad_move_count\n",
    "                            #,bad_move_phase_count,\n",
    "                            #move_count,\n",
    "                            observation,\n",
    "                            env,\n",
    "                            cur_epi_list)\n",
    "\n",
    "                #paths = []\n",
    "\n",
    "                data_ = collections.defaultdict(torch.tensor)\n",
    "                data_['observations'] = obs[:step_count].to(device =self.args.pin_memory_device).reshape(-1,np.prod(T.ob_space_shape))\n",
    "                #data_['next_observations'] = obs[1:step_count+1].to(device =self.args.pin_memory_device).reshape(-1,np.prod(T.ob_space_shape)) #torch.tensor([1,2,3,4])\n",
    "                data_['rewards'] = rewards_2[:step_count].to(device =self.args.pin_memory_device)#torch.tensor([1,2,3,4])\n",
    "                data_['terminals'] = dones_2[:step_count].to(device =self.args.pin_memory_device)#torch.tensor([1,2,3,4])\n",
    "                data_['actions'] = actions[:step_count].to(device =self.args.pin_memory_device)#torch.tensor([1,2,3,4])\n",
    "                data_['action_masks'] = action_masks[:step_count].to(device =self.args.pin_memory_device)\n",
    "                data_['current_agent_simple'] = current_agent[:step_count].to(device =self.args.pin_memory_device)\n",
    "                data_['current_agent'] = self.map_agent_phase_vector(current_agent[:step_count],num_classes=self.total_agents+1)[:,1:].to(device =self.args.pin_memory_device)\n",
    "                data_['current_phase'] = self.map_agent_phase_vector(current_phase[:step_count],num_classes=self.total_phases).to(device =self.args.pin_memory_device)\n",
    "                data_['current_troops_count'] = current_troops_count[:step_count].to(device =self.args.pin_memory_device)\n",
    "                \n",
    "                #data_['state'] = torch.concat(\n",
    "                #                    (\n",
    "                #                data_['observations'].reshape(-1,np.prod(T.ob_space_shape))\n",
    "                #                ,data_['current_phase']\n",
    "                #                ,data_['current_agent']), axis =1)\n",
    "                #hero = 1\n",
    "                #hero_steps = (((T.paths[0]).dataset.trajectories[0]['current_agent']*torch.tensor([[1,2,3]])).sum(-1) == (hero-1) )\n",
    "                \n",
    "                #torch.concat(\n",
    "                #    (\n",
    "                #        torch.concat(\n",
    "                #                    (\n",
    "                #                T.paths[0].dataset.trajectories[0]['observations'].reshape(-1,np.prod(T.ob_space_shape))\n",
    "                #                ,T.paths[0].dataset.trajectories[0]['current_phase']\n",
    "                #                ,T.paths[0].dataset.trajectories[0]['current_agent']), axis =1)\n",
    "                #\n",
    "                #,torch.ones(len(T.paths[0].dataset.trajectories[0]['action_masks']))[:,None]*hero\n",
    "                #,T.paths[0].dataset.trajectories[0]['current_troops_count'][:,hero,None]\n",
    "                #,T.paths[0].dataset.trajectories[0]['action_masks']*hero_steps[:,None]), axis =1)[0]\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                datase = TrajectoryDataset_per_episode([data_],context_len=self.args.context_len,\n",
    "                                                         rtg_scale=self.args.rtg_scale,gamma=self.args.gamma)\n",
    "                paths.append(DataLoader(datase, batch_size=len(datase)))\n",
    "\n",
    "                    # Example usage:\n",
    "        #trajectories_1 = T.paths[:1]\n",
    "        \n",
    "        #trajectories_2 = T.paths[1:2]\n",
    "        #dataset_1 = TrajectoryDataset_per_episode(trajectories_1,rtg_scale=1, context_len=200)\n",
    "        #dataset_2 = TrajectoryDataset_per_episode(trajectories_2,rtg_scale=1, context_len=200)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #dataset = TrajectoryDataset(trajectories, context_length=5)\n",
    "        #dataloader_1 = DataLoader(dataset_1, batch_size=len(dataset_1))\n",
    "        #dataloader_2 = DataLoader(dataset_2, batch_size=len(dataset_2))\n",
    "        #dataset_3_ = TrajectoryDataset_2_through_episodes([dataloader_1,dataloader_2])\n",
    "        \n",
    "        #dataloader_3 = DataLoader(dataset_3_, batch_size=1,shuffle=True)\n",
    "        \n",
    "        # Iterate through the dataloader\n",
    "        \n",
    "        #torch.concat([batch for batch in dataloader])\n",
    "        #for batch in dataloader_3:\n",
    "        #    print(batch[1][0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            t_range = torch.Tensor(np.arange(0,step)).to(self.device,dtype=torch.int)\n",
    "            hero_steps = [current_agent == self.the_hero_agent][0][:,0][:step]\n",
    "            \n",
    "            next_indecies = (t_next[:step,self.the_hero_agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "        \n",
    "            selected_t_next = t_next[:,self.the_hero_agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "            infos = [dir({})]*step #t_next[:step,0]\n",
    "            b_obs_a_all = torch.concat(( obs.reshape(-1,np.prod(self.ob_space_shape)) ,\n",
    "                                    action_masks.reshape(-1,np.prod(self.action_mask_shape)),\n",
    "                                    self.map_agent_phase_vector(current_agent,num_classes=self.total_agents+1)[:,1:],\n",
    "                                    \n",
    "                                    #map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)),\n",
    "                                    self.map_agent_phase_vector(current_phase,num_classes=self.total_phases)#,\n",
    "                                    #current_troops_count[:,self.the_hero_agent-1,None],\n",
    "                                    #selected_t_next\n",
    "                                   ),axis =1)\n",
    "\n",
    "            #self.update_hero_rb(b_obs_a_all,t_next,t_range,current_agent,actions,\n",
    "            #                    rewards,dones,infos,current_troops_count,\n",
    "            #                    step)\n",
    "\n",
    "            \n",
    "            #for i in zip(b_obs_a[:step][hero_steps].cpu().to(dtype=torch.float), b_obs_a[next_indecies].cpu().to(dtype=torch.float), \n",
    "            #   actions[:step][hero_steps].cpu().to(dtype=torch.float32), rewards[:step][hero_steps][:,self.the_hero_agent-1,None].cpu(), \n",
    "            #   dones[:step][hero_steps][:,self.the_hero_agent-1,None].cpu(), infos):\n",
    "            #    rb.add(*i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_episode_length = torch.mean(torch.tensor(\n",
    "                            [(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()]).float())#np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "        if self.args.TB_log:\n",
    "            self.writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, self.global_step)\n",
    "\n",
    "\n",
    "\n",
    "        return paths\n",
    "        #return rb\n",
    "\n",
    "    def update_hero_rb(self,b_obs_a_all,t_next,t_range,current_agent,actions,\n",
    "                                rewards,dones,infos,current_troops_count,\n",
    "                                step):\n",
    "\n",
    "\n",
    "\n",
    "            for agent in self.hero_agents_list:\n",
    "                #the_hero_agent = agent\n",
    "                \n",
    "                \n",
    "                hero_steps = [current_agent == agent][0][:,0][:step]\n",
    "                next_indecies = (t_next[:step,agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "                selected_t_next = t_next[:,agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "\n",
    "                b_obs_a = torch.concat((b_obs_a_all,\n",
    "                                        current_troops_count[:,agent-1,None],\n",
    "                                        selected_t_next),axis =1)\n",
    "                \n",
    "                for i in zip(b_obs_a[:step][hero_steps].cpu().to(dtype=torch.float), \n",
    "                             b_obs_a[next_indecies].cpu().to(dtype=torch.float), \n",
    "                            actions[:step][hero_steps].cpu().to(dtype=torch.float32),\n",
    "                             rewards[:step][hero_steps][:,agent-1,None].cpu(), \n",
    "                           dones[:step][hero_steps][:,agent-1,None].cpu(), infos):\n",
    "                            self.hero_agents_list[agent].rb.add(*i)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def write_exploring(self,is_draw,#position,\n",
    "                        curren_epi,step,\n",
    "                        total_rewards,#bad_move_count\n",
    "                        #,bad_move_phase_count,\n",
    "                        #move_count,\n",
    "                        observation,\n",
    "                        env,\n",
    "                        cur_epi_list):\n",
    "\n",
    "        if is_draw:\n",
    "            self.draw_count +=1\n",
    "\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/position_draw\",self.hero_agents_list[i].position\n",
    "                                                                                            ,self.draw_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].draw_territory_count = int(observation['observation'][:,i].sum())\n",
    "                \n",
    "                self.hero_agents_list[i].count_draw_dict[\n",
    "                                                        self.hero_agents_list[i].position\n",
    "                                                        ] +=1\n",
    "                self.writer.add_scalar(f\"draw_charts_agent_{i}/draw_territory_count\",\n",
    "                                                                               self.hero_agents_list[i].draw_territory_count,\n",
    "                                                                               self.draw_count)#self.global_step)\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_draw_dict:\n",
    "                    self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_position_prop_draw\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),self.draw_count)\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j],\n",
    "                                                                   self.draw_count)\n",
    "\n",
    "                        self.writer.add_scalar(f\"draw_charts_agent_{i}/{j}_place_in_draw_ratio\",\n",
    "                                                                   self.hero_agents_list[i].count_draw_dict[j]/self.draw_count,\n",
    "                                                                   self.draw_count)\n",
    "                        \n",
    "                    \n",
    "            self.writer.add_scalar(\"draw_charts/draw_count\",self.draw_count,self.global_step)\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",1,(curren_epi+1))\n",
    "            self.writer.add_scalar(\"draw_charts/draw_to_total_count\",self.draw_count/(curren_epi +1+0.000001),self.global_step)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            non_draw_count =(curren_epi-self.draw_count+1+0.000001)\n",
    "            for i in self.hero_agents_list:\n",
    "\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/position_win\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                self.hero_agents_list[i].count_dict[self.hero_agents_list[i].position\n",
    "                                               ] +=1\n",
    "\n",
    "                \n",
    "                for j in self.hero_agents_list[i].count_dict:\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_prop\",int(\n",
    "                                                            self.hero_agents_list[i].position==j\n",
    "                                                            ),(curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position\",self.hero_agents_list[i].count_dict[j],\n",
    "                                           (curren_epi+1))\n",
    "\n",
    "                    self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total_terminated\",self.hero_agents_list[i].count_dict[j]/non_draw_count,(curren_epi+1))\n",
    "\n",
    "                    if j not in [1,2]:\n",
    "                        self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_all_prop\",int(\n",
    "                                                                                            self.hero_agents_list[i].position==j\n",
    "                                                                                            ),(curren_epi+1))\n",
    "\n",
    "\n",
    "            self.writer.add_scalar(\"draw_charts/draw\",0,(curren_epi+1))\n",
    "            \n",
    "        for i in self.hero_agents_list:\n",
    "\n",
    "            self.writer.add_scalar(f\"win_charts_agent_{i}/position_all\",self.hero_agents_list[i].position\n",
    "                                                                                            ,(curren_epi+1))\n",
    "\n",
    "            \n",
    "            for j in self.hero_agents_list[i].count_dict:\n",
    "                self.writer.add_scalar(f\"win_charts_agent_{i}/{j}_position_to_total\",(\n",
    "                                                            self.hero_agents_list[i].count_dict[j]+\n",
    "                                                            self.hero_agents_list[i].count_draw_dict[j]\n",
    "                                                        \n",
    "                                                            )/(curren_epi +1+0.00001 ),(curren_epi+1))#global_step)\n",
    "\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count,self.global_step)\n",
    "                \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0],self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1],self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_count_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2],self.global_step)\n",
    "            \n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/total_moves\",sum(\n",
    "                                                    self.hero_agents_list[i].move_count.values()),self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_count_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_count/(sum(\n",
    "                                                   self.hero_agents_list[i].move_count.values())+1),self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_position_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[0]/( \n",
    "                                                   self.hero_agents_list[i].move_count[0]+1),self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_attack_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[1]/( \n",
    "                                                   self.hero_agents_list[i].move_count[1]+1),self.global_step)\n",
    "            self.writer.add_scalar(f\"moves/agent_{i}/bad_move_to_step_fortify_per_episode\",\n",
    "                                               self.hero_agents_list[i].bad_move_phase_count[2]/( \n",
    "                                                   self.hero_agents_list[i].move_count[2]+1),self.global_step)\n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/epsilon\",(curren_epi/((self.num_iterations*self.num_episodes)/10)),self.global_step)\n",
    "        self.writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), self.global_step)\n",
    "\n",
    "        \n",
    "\n",
    "        #values_total = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/episodic_length\", cur_epi_list[:step].sum(), self.global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            #cur_index = torch.where((current_agent[:,0] == i) &( cur_epi_list ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], self.global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7680d1-b974-4cf4-aea6-a8e20727bb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f27e1ed1-0d6a-4974-bf1e-105158acf075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9ef5c01-ecb2-4874-9975-101833a5cc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e800075-4339-4d58-938f-eb52d8f3545e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b9bbf82-aa76-4103-bde8-2a8bbe0c99c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "35a5cb1c-f454-4687-a4ae-81386a5c31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma,Torch = True):\n",
    "    if Torch:\n",
    "        disc_cumsum = torch.zeros_like(x,dtype = torch.float32)\n",
    "    else:\n",
    "        disc_cumsum = np.zeros_like(x,dtype = np.float32)\n",
    "    \n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    #print(disc_cumsum[-1])\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "        #print(x[t],disc_cumsum[t])\n",
    "    return disc_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "df9b3037-7bf1-432c-a58a-92ccd7af5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_3 = dict(\n",
    "exp_name = 'exp3_ddqn_lr_bs_1',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 \n",
    "                     #| {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False#False\n",
    "                     #                                   }\n",
    "                 \n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_4 = dict(\n",
    "exp_name = 'exp4_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_5 = dict(\n",
    "exp_name = 'exp5_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.001,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_6 = dict(\n",
    "exp_name = 'exp6_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=False,\n",
    "return_prob=False,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_7 = dict(\n",
    "exp_name = 'exp7_ddqn_2_agents_1_hero',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_8 = dict(\n",
    "exp_name = 'exp8_ddqn_2_agents_1_hero_norm',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 1,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 2\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "exp_9 = dict(\n",
    "exp_name = 'exp9_ddqn_4_agents_2_hero_norm',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = False,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.1\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_10 = dict(\n",
    "exp_name = 'exp10_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 10,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "exp_11 = dict(\n",
    "exp_name = 'exp11_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 512,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "\n",
    "context_len = 64,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=True,\n",
    "drop_last=True,\n",
    "num_episodes = 5,\n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "exp_12 = dict(\n",
    "exp_name = 'exp12_ddqn_4_agents_2_hero_norm_small',\n",
    "learning_rate = 0.0003,\n",
    "batch_size = 2,\n",
    "gamma = 0.99,\n",
    "num_steps=120000,\n",
    "num_iterations = 500,\n",
    "episode_time_lim = 10000,\n",
    "hero_agent_count = 2,\n",
    "entropy=True,\n",
    "return_prob=2,\n",
    "actor_wt = 0.5,\n",
    "CE_wt = 0.01,\n",
    "small = True,\n",
    "num_episodes = 5,\n",
    "context_len = 256,\n",
    "rtg_scale=1,\n",
    "shuffle=True,\n",
    "pin_memory=False,#False,\n",
    "drop_last=True,\n",
    "TB_log=False,\n",
    "device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "pin_memory_device= \"cpu\",#(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    \n",
    "env_config = dict(render_mode = None,#'rgb_array', \n",
    "                    default_attack_all  = True,\n",
    "                    agent_count  = 4\n",
    "                    ,use_placement_perc=True,\n",
    "                    render_=False,\n",
    "                    bad_mov_penalization = 0.01\n",
    "                 )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "506c53ba-1951-48b4-9474-55bf0ec85797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "#T = Trainer(Args,param_dict = exp_3)\n",
    "T = Trainer(Args,param_dict = exp_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "cf9e73ff-3049-4319-b9da-dc7e3f06cfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: {1: True, 2: True, 3: True, 4: True} ,total_reward: {1: -200.06, 2: 106.34000000000002, 3: -97.58999999999999, 4: -300.02} ,iteration: 1 ,episode: 0\n",
      "done: {1: True, 2: True, 3: True, 4: True} ,total_reward: {1: 108.73, 2: -97.38999999999999, 3: -300.03, 4: -200.01999999999998} ,iteration: 1 ,episode: 1\n",
      "done: {1: True, 2: True, 3: True, 4: True} ,total_reward: {1: -300.01, 2: 108.9, 3: -98.14999999999999, 4: -199.01999999999998} ,iteration: 1 ,episode: 2\n",
      "done: {1: True, 2: True, 3: True, 4: True} ,total_reward: {1: -98.17999999999999, 2: -199.04, 3: 108.68, 4: -300.0} ,iteration: 1 ,episode: 3\n",
      "done: {1: True, 2: True, 3: True, 4: True} ,total_reward: {1: -300.01, 2: -99.1, 3: 109.9, 4: -199.07999999999998} ,iteration: 1 ,episode: 4\n",
      "tensor([[0.0000e+00, 3.2287e-01, 6.3086e-01, 0.0000e+00, 3.1852e+00],\n",
      "        [4.4490e-03, 5.3362e-01, 3.4181e-01, 5.1481e-02, 1.9830e+00],\n",
      "        [6.6099e-02, 3.7168e-01, 3.6367e-01, 1.4682e-01, 2.9017e+00],\n",
      "        [0.0000e+00, 3.3698e-01, 4.1007e-01, 2.4469e-01, 6.4810e+00],\n",
      "        [1.6652e-01, 2.8117e-01, 5.0909e-01, 0.0000e+00, 3.4783e+00],\n",
      "        [2.6185e-01, 2.8893e-01, 4.1515e-01, 5.7201e-03, 2.8178e+00],\n",
      "        [2.2118e-01, 2.3478e-01, 4.5367e-01, 0.0000e+00, 2.3426e+00],\n",
      "        [2.2817e-01, 2.9338e-01, 4.4363e-01, 0.0000e+00, 1.7342e+00],\n",
      "        [0.0000e+00, 6.2489e-01, 3.1893e-01, 0.0000e+00, 2.4103e+00],\n",
      "        [0.0000e+00, 8.5611e-01, 1.3525e-01, 0.0000e+00, 4.5137e+00]]) tensor([[1.0000e-06, 4.6760e-01, 4.8260e-01, 1.0000e-06, 3.1515e+00],\n",
      "        [6.6557e-02, 4.9890e-01, 4.7435e-01, 2.2099e-01, 2.5403e+00],\n",
      "        [2.4847e-01, 4.8329e-01, 4.8109e-01, 3.5395e-01, 2.9990e+00],\n",
      "        [1.0000e-06, 4.7271e-01, 4.9188e-01, 4.2993e-01, 5.1209e+00],\n",
      "        [3.7257e-01, 4.4960e-01, 4.9995e-01, 1.0000e-06, 3.3997e+00],\n",
      "        [4.3967e-01, 4.5329e-01, 4.9278e-01, 7.5420e-02, 2.7083e+00],\n",
      "        [4.1507e-01, 4.2389e-01, 4.9788e-01, 1.0000e-06, 1.7950e+00],\n",
      "        [4.1968e-01, 4.5534e-01, 4.9684e-01, 1.0000e-06, 1.2996e+00],\n",
      "        [1.0000e-06, 4.8418e-01, 4.6609e-01, 1.0000e-06, 1.9884e+00],\n",
      "        [1.0000e-06, 3.5100e-01, 3.4201e-01, 1.0000e-06, 6.1589e+00]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'traj_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[491], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[483], line 297\u001b[0m, in \u001b[0;36mTrainer.run_training_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraj_data_loader)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m## get state stats from dataset\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m state_mean, state_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraj_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mget_state_stats()\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m#print(len(self.paths))\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m#print(len(traj_dataset))\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_iter))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'traj_dataset'"
     ]
    }
   ],
   "source": [
    "T.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fb76b-73ef-4c32-a3d5-b9b6ebb14f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93c5e2c7-f84a-4499-ae4b-b312ef1d0d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1.0]\n",
      " [2 1.0]\n",
      " [2 4.0]\n",
      " [2 4.0]\n",
      " [2 9.0]\n",
      " [2 1.0]\n",
      " [2 1.0]\n",
      " [2 4.0]\n",
      " [2 1.0]\n",
      " [2 1.0]]\n",
      "done dict_values([True, True, True, True]) {1: -100, 2: 100, 3: -100, 4: -100}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3578"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "                render_ = True,agent_count  = 3,use_placement_perc=True)\n",
    "env = env_risk(**(env_config | {\"render_mode\" : None,\"verbose\":False, \"agent_count\" :4,\"render_\":False}))\n",
    "\n",
    "\n",
    "total_rewards = {i:0 for i in env.possible_agents}\n",
    "\n",
    "step =0\n",
    "for i__ in range(1):\n",
    "    env.reset()#seed=42)\n",
    "    #print(env_1.infos.keys())\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    total_rewards = {i:0 for i in env.possible_agents}\n",
    "    \n",
    "    #so there is an issue ...reward =2 sometimes ... doubling?\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        step+=1\n",
    "        e_t = env.terminations\n",
    "        if sum(e_t.values()) <3:\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            action = env.action_space(agent).sample()\n",
    "            \n",
    "            #if env_1.phase_selection ==1:\n",
    "            #    action = [int(action[0]),action[1]]\n",
    "            part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "            #print(part_0)\n",
    "            \n",
    "            #action = [part_0,np.around(action[1],2) #min(action[1],env_1.board.agents[agent].bucket ) \n",
    "            #                  if env_1.phase_selection==0 else  action[1]]\n",
    "    \n",
    "            action = [part_0,np.around(action[1],2)]\n",
    "    \n",
    "    \n",
    "            for i in env.possible_agents:\n",
    "                total_rewards[i]+=env.curr_rewards[i]\n",
    "            \n",
    "            env.step(action)\n",
    "\n",
    "            if step%10000 ==0:\n",
    "                print(step)\n",
    "                print(env.board.territories)\n",
    "                \n",
    "                print('\\naction',action,\n",
    "                      '\\ninfo',info,\n",
    "                      '\\ninfos',env.infos,\n",
    "                      '\\naction_valid',observation['action_mask'][action[0]],\n",
    "                      '\\nagent',agent,\n",
    "                      '\\nselected_agent',env.agent_selection,\n",
    "                      '\\ncurr_agent',env.board.current_agent,\n",
    "                      '\\ncurr phase',env.phase_selection,\n",
    "                      '\\nbad_trail count',env.board.bad_trials,\n",
    "                      '\\nmax_bad trails', env.board.max_bad_trials,\n",
    "                      '\\nreward',reward,\n",
    "                      '\\nreward',env.curr_rewards,\n",
    "                      '\\ntotal_reward',total_rewards,\n",
    "                      '\\nrewards',env.rewards,\n",
    "                      '\\nbuckets', [env.board.agents[i].bucket for i in env.agents]\n",
    "                     )\n",
    "                print('kill_list',env.kill_list,'term',e_t, 'buckets',[i.bucket for i in env.board.agents])\n",
    "                #if sum(env.curr_rewards.values()) <0:\n",
    "                #    input()\n",
    "        else:\n",
    "            print(env.board.territories)\n",
    "            print('done',env.terminations.values(),env.curr_rewards)\n",
    "            break\n",
    "step\n",
    "\n",
    "\n",
    "#need to debug .... so the action is valid... but there is no change\n",
    "#so either the phase is not changing and the agent is not changing... why??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e76bfb-f0fc-419c-851b-5dda75142ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
