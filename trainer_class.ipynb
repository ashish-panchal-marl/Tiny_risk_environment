{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905a5491-3426-4f41-9c29-9194f5398328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
      "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "C:\\Users\\31721\\.conda\\envs\\DL_project\\lib\\site-packages\\pkg_resources\\__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "#import gym\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete, Box, Dict\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "\n",
    "\n",
    "\n",
    "import gymnasium\n",
    "\n",
    "\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\n",
    "\n",
    "from statistics import NormalDist\n",
    "\n",
    "import pygame\n",
    "\n",
    "from typing import Any , Generic, Iterable, Iterator, TypeVar\n",
    "ActionType = TypeVar(\"ActionType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20abc03-b5a9-48bc-aee4-f42c7b25ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.new_models import *\n",
    "import utils_gym\n",
    "import env_model_class_2\n",
    "\n",
    "\n",
    "from board_env import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254dca59-dda5-4fbf-b5fd-6f821996a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023e851a-095e-4609-8e20-8fe69959bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ## Trainer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Args\n",
    "                 ):\n",
    "        # #### Configurations\n",
    "\n",
    "        self.args = Args()#tyro.cli(Args)\n",
    "        self.args.batch_size = int(self.args.num_envs * self.args.num_steps)\n",
    "        self.args.minibatch_size = int(self.args.batch_size // self.args.num_minibatches)\n",
    "        self.args.num_iterations = self.args.total_timesteps // self.args.batch_size\n",
    "        self.run_name = f\"{self.args.env_id}__{self.args.exp_name}__{self.args.seed}__{int(time.time())}\"\n",
    "\n",
    "        \n",
    "\n",
    "        TB_log = True\n",
    "        if TB_log:    \n",
    "            self.writer = SummaryWriter(f\"runs/{self.run_name}\")\n",
    "            self.writer.add_text(\n",
    "                \"hyperparameters\",\n",
    "                \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "            )\n",
    "        \n",
    "        # TRY NOT TO MODIFY: seeding\n",
    "        random.seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        torch.manual_seed(self.args.seed)\n",
    "        torch.backends.cudnn.deterministic = self.args.torch_deterministic\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        self.playe_r = 1#\"agent_1\" #\n",
    "        \n",
    "        self.num_steps = 60000#1000000\n",
    "        self.action_shape = (2,)\n",
    "\n",
    "        self.env_config = dict(render_mode = 'rgb_array', default_attack_all  = True,\n",
    "                            agent_count  = 3,use_placement_perc=True,render_=False)\n",
    "\n",
    "        self.env = env_risk(**self.env_config)\n",
    "        \n",
    "        self.env.reset(seed=42)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        sample_obs = self.obs_converter(torch.tensor(self.env.last()[0]['observation']),num_classes = 4)\n",
    "        \n",
    "        self.ob_space_shape = sample_obs.shape #env.observation_space(playe_r)['observation'].shape\n",
    "        self.action_mask_shape = self.env.observation_space(self.playe_r)['action_mask'].shape\n",
    "        self.total_agents  = len(self.env.possible_agents)\n",
    "        self.total_phases = len(self.env.phases)\n",
    "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.agent_list = list(self.env.possible_agents)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.num_episodes = 10\n",
    "        self.args.gamma = self.gam = 0.99\n",
    "        self.args.minibatch_size = 128        \n",
    "        self.the_hero_agent = 1\n",
    "\n",
    "        \n",
    "        self.qnet_config_dict = dict(action_space = self.env.action_space(self.playe_r\n",
    "                                                                         ).shape[0],\n",
    "                                    ob_space=np.prod(self.ob_space_shape\n",
    "                                                    )+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents -1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        self.actor_config_dict =  dict(env=self.env,\n",
    "                        action_space = self.env.observation_space(self.playe_r)['action_mask'].shape[0],\n",
    "                        ob_space=np.prod(self.ob_space_shape)+np.prod(self.action_mask_shape)\n",
    "                                         +1*( self.total_agents-1) #the current_agent +1#who actor agent was\n",
    "                                         +1*(self.total_phases -1)#the current phase\n",
    "                                         +1 # the number of troops\n",
    "                               )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "        self.qf1 = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "        self.qf1_target = QNetwork(**self.qnet_config_dict).to(self.device)\n",
    "        self.target_actor = Actor_ddqn(**self.actor_config_dict).to(self.device)\n",
    "        \n",
    "        \n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
    "        self.q_optimizer = optim.Adam(list(self.qf1.parameters()), lr=self.args.learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(list(self.actor.parameters()), lr=self.args.learning_rate)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def obs_converter(self,  data, num_classes = 4, col =0 ):\n",
    "\n",
    "        if col != None:\n",
    "            return torch.concat((nn.functional.one_hot(data[:,col].detach().long(), \n",
    "                                                        num_classes = num_classes),\n",
    "                                      data[:,~col,None]\n",
    "                                ),axis=1\n",
    "                               )[:,1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_hot(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(torch.tensor(data),num_classes = num_classes)[1:].to(self.device)\n",
    "    \n",
    "    def map_agent_phase_vector(self, data,num_classes = 3):\n",
    "        return nn.functional.one_hot(data[:,0].long(), \n",
    "                                                            num_classes = num_classes)[:,1:].to(self.device)\n",
    "\n",
    "\n",
    "    def train_loop_init(self):\n",
    "        self.gamma_t = {i:0 for i in self.env.possible_agents}\n",
    "        self.num_iterations = 500\n",
    "        self.episode_time_lim = 5000\n",
    "        self.draw_count = 0\n",
    "        self.first_count = 0\n",
    "        self.second_count = 0\n",
    "        self.third_count = 0\n",
    "        self.third_count_draw = 0\n",
    "        self.start_time = time.time()\n",
    "        self.global_step = 0\n",
    "        #self.faulting_player = \"\"\n",
    "        \n",
    "    \n",
    "    def run_training_loop(self):\n",
    "        \"\"\"\n",
    "        ### Run training loop\n",
    "        \"\"\"\n",
    "\n",
    "        # last 100 episode information\n",
    "        #tracker.set_queue('reward', 100, True)\n",
    "        #tracker.set_queue('length', 100, True)\n",
    "\n",
    "        obs = torch.zeros((self.num_steps,) + self.ob_space_shape).to(self.device)\n",
    "        actions = torch.zeros((self.num_steps, ) + self.action_shape).to(self.device)\n",
    "        action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape).to(self.device)\n",
    "        current_agent = torch.ones((self.num_steps,1)).to(self.device)*0#-1\n",
    "        current_phase = torch.zeros((self.num_steps,1)).to(self.device)\n",
    "        current_troops_count = torch.zeros((self.num_steps,self.total_agents)).to(self.device)\n",
    "        logprobs = torch.zeros((self.num_steps, )).to(self.device)\n",
    "        rewards = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        rewards_2 = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        dones = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "        values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "        episodes = torch.ones((self.num_steps, )).to(self.device)*-1\n",
    "        t_next = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "        rb = ReplayBuffer(\n",
    "                self.args.buffer_size,\n",
    "                Box(low =0, high=2000, shape =(self.qnet_config_dict['ob_space']+1,), dtype=np.float32),\n",
    "                Box(low =0, high=2000, shape =(2,), dtype=np.float32),\n",
    "                self.device,\n",
    "                handle_timeout_termination=False,\n",
    "            )\n",
    "\n",
    "        env = env_risk(**(self.env_config | {\"render_mode\" : None, \"bad_mov_penalization\" : 0.01,\"render_\":False}))\n",
    "        env.reset(42)\n",
    "        \n",
    "        self.train_loop_init()\n",
    "        \n",
    "        for iteration in range(1, self.num_iterations+1):\n",
    "\n",
    "            obs = torch.zeros((self.num_steps,) + self.ob_space_shape).to(self.device)\n",
    "            actions = torch.zeros((self.num_steps, ) + self.action_shape).to(self.device)\n",
    "            action_masks = torch.zeros((self.num_steps, ) + self.action_mask_shape).to(self.device)\n",
    "            current_agent = torch.ones((self.num_steps,1)).to(self.device)*0#-1\n",
    "            current_phase = torch.zeros((self.num_steps,1)).to(self.device)\n",
    "            current_troops_count = torch.zeros((self.num_steps,self.total_agents)).to(self.device)\n",
    "            logprobs = torch.zeros((self.num_steps, )).to(self.device)\n",
    "            rewards = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            rewards_2 = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            dones = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "            #values = torch.zeros((self.num_steps,  )).to(self.device)\n",
    "            episodes = torch.ones((self.num_steps, )).to(self.device)*-1\n",
    "            t_next = torch.zeros((self.num_steps, self.total_agents)).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            rb = self.sample(\n",
    "                                env,iteration,\n",
    "                                obs\n",
    "                                ,actions\n",
    "                                ,action_masks\n",
    "                                ,current_agent\n",
    "                                ,current_phase\n",
    "                                ,current_troops_count\n",
    "                                ,logprobs\n",
    "                                ,rewards\n",
    "                                ,rewards_2\n",
    "                                ,dones\n",
    "                                ,values\n",
    "                                ,episodes\n",
    "                                ,t_next,\n",
    "                                rb\n",
    "                 \n",
    "                            )\n",
    "            \n",
    "            self.train(rb,iteration)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            if self.global_step%100 ==0:\n",
    "                SPS = int(self.global_step / (time.time() - self.start_time))\n",
    "                print(\"SPS:\", SPS)       \n",
    "                self.writer.add_scalar(\"charts/SPS\", SPS, self.global_step)\n",
    "        \n",
    "\n",
    "            self.save_models()\n",
    "    \n",
    "    def sample(self,env,iteration,obs\n",
    "                                ,actions\n",
    "                                ,action_masks\n",
    "                                ,current_agent\n",
    "                                ,current_phase\n",
    "                                ,current_troops_count\n",
    "                                ,logprobs\n",
    "                                ,rewards\n",
    "                                ,rewards_2\n",
    "                                ,dones\n",
    "                                ,values\n",
    "                                ,episodes\n",
    "                                ,t_next,\n",
    "                                rb):\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # sample `worker_steps` from each worker\n",
    "            #there are no worker steps... rather there are full episodes\n",
    "\n",
    "            step = 0\n",
    "            fault_condition = False\n",
    "            clear_output(wait=True)\n",
    "            phase = 0\n",
    "        \n",
    "            \n",
    "            for episode in range(self.num_episodes):#num_episodes):\n",
    "                \n",
    "                total_rewards = {i:0 for i in env.possible_agents} #i can report this\n",
    "                action=1\n",
    "                \n",
    "                if fault_condition:\n",
    "                    env = env_risk(**(self.env_config | {\"render_mode\" : None,\"bad_mov_penalization\" : 0.01,\"render_\":False})\n",
    "                                      )#game.env(render_mode=None)\n",
    "\n",
    "                curren_epi = episode + (iteration-1)*self.num_episodes\n",
    "                env.reset(curren_epi) #for riplication\n",
    "                \n",
    "                fault_condition = False\n",
    "                step_count = 0\n",
    "                bad_move_count = 0\n",
    "                bad_move_phase_count = {i:0 for i in env.phases}\n",
    "                move_count =  {i:0 for i in env.phases}\n",
    "                is_draw = 0\n",
    "                \n",
    "                draw_territory_count = 0\n",
    "                is_third = 0\n",
    "\n",
    "                for agent in env.agent_iter():\n",
    "                    e_t = env.terminations\n",
    "                    if sum(e_t.values()) <2:\n",
    "                        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "                        observation['observation'] =  self.obs_converter(\n",
    "                                                        torch.tensor(\n",
    "                                                            observation['observation']\n",
    "                                                        ).to(self.device,dtype=torch.float32))\n",
    "                        \n",
    "                        episodes[step] = curren_epi\n",
    "                        obs[step] = observation['observation']#torch.Tensor(observation['observation']).to(self.device) #sould i not add it .... meaning this is the last observation after the player dies\n",
    "                        action_masks[step] = torch.Tensor(observation['action_mask']).to(self.device)\n",
    "                        \n",
    "                        #curr_agent = agent#int(agent[-1])\n",
    "                        current_agent[step] = curr_agent = agent\n",
    "                        current_phase[step] = phase = env.phase_selection\n",
    "                        phase_mapping = self.map_agent_phase_hot(phase,num_classes = self.total_phases).float()\n",
    "                        \n",
    "                        curr_agent_mapping = self.map_agent_phase_hot(int(curr_agent)-1,\n",
    "                                                                      num_classes = self.total_agents \n",
    "                                                                     ).float()\n",
    "                        \n",
    "                        current_troops_count[step] = torch.Tensor([env.board.agents[i].bucket for i in env.possible_agents]).to(self.device)\n",
    "                    \n",
    "\n",
    "                        model_in = torch.Tensor(torch.hstack((observation['observation'].reshape(-1),torch.tensor(observation['action_mask'].reshape(-1)).to(self.device),\n",
    "                                           phase_mapping,\n",
    "                                            curr_agent_mapping,\n",
    "                                           torch.tensor([env.board.agents[curr_agent].bucket ]).to(self.device)))[None,:]#.repeat(3,axis = 0)\n",
    "                                                ).float()\n",
    "                        \n",
    "                        #if e_t[curr_agent]:\n",
    "                            #print('heeee')\n",
    "                            \n",
    "                        if termination or truncation:\n",
    "                            \n",
    "                            action = None\n",
    "\n",
    "                            act = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            #act, logprob, _, value = agent_mod.get_action_and_value(model_in)\n",
    "                            #values[step] = value.flatten() # so even if we are removing the guy ... we need to know what is the action he would \n",
    "                                                                #have taken at this point and what would have been its value\n",
    "                            actions[step] = act #even after going what would have been\n",
    "                            #logprobs[step] = logprob        \n",
    "                        else:\n",
    "                            mask = observation[\"action_mask\"]\n",
    "                            if (self.global_step < self.args.learning_starts) or (\n",
    "                                np.random.rand() > min(\n",
    "                                                ((curren_epi)/((self.num_iterations*self.num_episodes)/10))\n",
    "                                                , 0.95)\n",
    "                                                ) or (\n",
    "                                agent != self.the_hero_agent):\n",
    "        \n",
    "                                \n",
    "                                action = env.action_space(agent).sample()\n",
    "                                part_0 =np.random.choice(np.where(env.board.calculated_action_mask(agent))[0])\n",
    "                                action = torch.Tensor([[[part_0],[np.around(action[1],2)]]]).to(self.device)\n",
    "                                action = action[:,:,0]\n",
    "                            else:\n",
    "                                \n",
    "                                action = self.actor(torch.Tensor(model_in).to(self.device))\n",
    "                            actions[step] = action\n",
    "        \n",
    "                            if not observation['action_mask'][action[:,0].long()]: \n",
    "                                fault_condition =True\n",
    "                                #self.faulting_player = agent\n",
    "        \n",
    "                                if self.the_hero_agent == curr_agent:\n",
    "                                    bad_move_count+=1\n",
    "                                    bad_move_phase_count[int(current_phase[step][0])]+=1  # when is the where_is_it_performing_bad_really\n",
    "                                    #print('here',agent, action, observation['action_mask'])\n",
    "                            \n",
    "        \n",
    "                            if self.the_hero_agent == curr_agent:\n",
    "                                move_count[int(current_phase[step][0])]+=1        \n",
    "        \n",
    "        \n",
    "                        #print('here',agent, action)\n",
    "                        if action != None :\n",
    "                            act_2 = action.detach().cpu().numpy()[0]#list([action.detach().cpu().numpy()[0][0], max(action.detach().cpu().numpy()[0][1],0.1) ])\n",
    "                            act_2 = list([act_2[0], max(act_2[1],0.001) ])\n",
    "                        else:\n",
    "                            act_2 = action\n",
    "                            \n",
    "                        env.step(act_2 if action != None else None)        \n",
    "        \n",
    "        \n",
    "                        if action == None:\n",
    "                            print('heeee')\n",
    "                            rewards[step] = np.zeros(self.total_agents) # should i keep it -1? .... hm i dont think so .\n",
    "                            dones[step] = np.zeros(self.total_agents) # frankly the guys is already done so we really dont have to do anything here.... this is the state post termination for a loser \n",
    "                            # but btw this is for the next agent ... action == None means in the last action the previous agent would have been removed.\n",
    "                            #values[step] = \n",
    "                        else:\n",
    "        \n",
    "                            rewards_2[step] = torch.Tensor([env.curr_rewards[i] for i in env.possible_agents]).to(self.device)\n",
    "                            curr_reward_list =  env.curr_rewards\n",
    "                            if (step_count == (self.episode_time_lim-1)):\n",
    "                                curr_reward_list = {i:-100 for i in env.possible_agents }\n",
    "        \n",
    "                            for i in env.possible_agents:\n",
    "                                if i != curr_agent:\n",
    "                                    self.gamma_t[i]+=1\n",
    "                                else:\n",
    "                                    self.gamma_t[i] =0\n",
    "        \n",
    "                                if (step_count == (self.episode_time_lim-1)):\n",
    "                                    cr_rew = -100\n",
    "                                    term = True\n",
    "                                else:\n",
    "                                    cr_rew = env.curr_rewards[i]\n",
    "                                    term = env.terminations[i]\n",
    "\n",
    "                                next_step_ = step-self.gamma_t[i]\n",
    "                                rewards[next_step_,i-1] += (self.args.gamma**self.gamma_t[i])*cr_rew\n",
    "                                t_next[next_step_,i-1] = self.gamma_t[i]\n",
    "                                dones[next_step_,i-1] = torch.Tensor([term]).to(self.device) #so the panetly has to be added but attributions is really difficult\n",
    "        \n",
    "                        #list_curr_reward_list = np.array(list(curr_reward_list.values()))\n",
    "                        \n",
    "                        if sum(curr_reward_list.values()) == -300:\n",
    "                            #print('here')\n",
    "                            is_draw=1\n",
    "        \n",
    "                        \n",
    "                        for age_i in env.possible_agents:\n",
    "                            \n",
    "                            total_rewards[age_i]+=curr_reward_list[age_i] #env.curr_rewards[age_i] if (step_count != episode_time_lim) else -100\n",
    "                                    \n",
    "                        \n",
    "                        step +=1\n",
    "                        self.global_step+=1\n",
    "        \n",
    "                    else:\n",
    "                        print('done:',env.terminations,#env.terminations.values(),\n",
    "                              \",total_reward:\",total_rewards, ',iteration:',iteration,\",episode:\", episode )\n",
    "                        break    \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                    step_count+=1\n",
    "                    \n",
    "                    if (self.global_step == self.num_steps) :# or (fault_condition and (fa ulting_player != agent) and (len(env.agents)==0)):\n",
    "                        break\n",
    "                    elif (step_count == self.episode_time_lim):\n",
    "                        break\n",
    "                        \n",
    "                #print(rewards[step-2])\n",
    "                if self.global_step == self.num_steps:\n",
    "                    break \n",
    "                \n",
    "                position = 3\n",
    "                for k_,(i_,j_) in enumerate(sorted([(j_,i_) for i_,j_ in total_rewards.items()],reverse=True) \n",
    "                      ):\n",
    "                    if j_==self.the_hero_agent:\n",
    "                        position = k_+1\n",
    "\n",
    "                cur_epi_list = (episodes == curren_epi)\n",
    "                \n",
    "                self.write_exploring(is_draw,position,curren_epi,step,total_rewards,bad_move_count\n",
    "                                    ,bad_move_phase_count,\n",
    "                                    move_count,observation,env,cur_epi_list)\n",
    "\n",
    "\n",
    "            t_range = torch.Tensor(np.arange(0,step)).to(self.device,dtype=torch.int)\n",
    "            hero_steps = [current_agent == self.the_hero_agent][0][:,0][:step]\n",
    "            \n",
    "            next_indecies = (t_next[:step,self.the_hero_agent-1].to(dtype=torch.int) + t_range +1)[hero_steps].long()\n",
    "        \n",
    "            selected_t_next = t_next[:,self.the_hero_agent-1,None]#t_next[:step,0,None][hero_steps]\n",
    "            infos = [dir({})]*step #t_next[:step,0]\n",
    "            b_obs_a = torch.concat(( obs.reshape(-1,np.prod(self.ob_space_shape)) ,\n",
    "                                    action_masks.reshape(-1,np.prod(self.action_mask_shape)),\n",
    "                                    self.map_agent_phase_vector(current_agent,num_classes=self.total_agents+1)[:,1:],\n",
    "                                    \n",
    "                                    #map_agent_phase_vector(current_agent,num_classes=len(env.possible_agents)),\n",
    "                                    self.map_agent_phase_vector(current_phase,num_classes=self.total_phases),\n",
    "                                    current_troops_count[:,self.the_hero_agent-1,None],\n",
    "                                    selected_t_next\n",
    "                                   ),axis =1)\n",
    "\n",
    "\n",
    "            \n",
    "            for i in zip(b_obs_a[:step][hero_steps].cpu().to(dtype=torch.float), b_obs_a[next_indecies].cpu().to(dtype=torch.float), \n",
    "               actions[:step][hero_steps].cpu().to(dtype=torch.float32), rewards[:step][hero_steps][:,self.the_hero_agent-1,None].cpu(), \n",
    "               dones[:step][hero_steps][:,self.the_hero_agent-1,None].cpu(), infos):\n",
    "                rb.add(*i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_episode_length = torch.mean(torch.tensor(\n",
    "                            [(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()]).float())#np.mean([(episodes[:step] == i_epi).sum() for i_epi in episodes[:step].unique()])\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/avg_episodic_length\", avg_episode_length, self.global_step)\n",
    "\n",
    "\n",
    "        \n",
    "        return rb\n",
    "\n",
    "\n",
    "    def write_exploring(self,is_draw,position,curren_epi,step,total_rewards,bad_move_count\n",
    "                        ,bad_move_phase_count,\n",
    "                        move_count,observation,env,cur_epi_list):\n",
    "\n",
    "        if is_draw:\n",
    "            self.draw_count +=1\n",
    "            if position ==3:\n",
    "                self.third_count_draw  +=1\n",
    "                \n",
    "            #    draw_territory_count = 0\n",
    "            #else:\n",
    "            draw_territory_count = int(observation['observation'][:,self.the_hero_agent].sum())\n",
    "            self.writer.add_scalar(\"win_charts/1st_position_prop_draw\",int(position==1),self.draw_count)#(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/2nd_position_prop_draw\",int(position==2),self.draw_count)#(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/3rd_position_prop_draw\",int(position==3),self.draw_count)#(curren_epi+1))\n",
    "            \n",
    "            self.writer.add_scalar(\"win_charts/3rd_position_all_prop\",int(position ==3),(curren_epi+1))\n",
    "            \n",
    "            self.writer.add_scalar(\"win_charts/draw_count\",self.draw_count,self.global_step)\n",
    "            self.writer.add_scalar(\"win_charts/draw\",1,(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/draw_to_total_count\",self.draw_count/(curren_epi +0.000001),self.global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"win_charts/draw_territory_count\",draw_territory_count,self.draw_count)#self.global_step)\n",
    "\n",
    "        \n",
    "            self.writer.add_scalar(\"win_charts/third_place_in_draw\",self.third_count_draw,self.draw_count)#self.global_step)\n",
    "            self.writer.add_scalar(\"win_charts/third_place_in_draw_ratio\",self.third_count_draw/self.draw_count,self.draw_count)#self.global_step)\n",
    "        else:\n",
    "            \n",
    "            self.first_count += (position == 1)\n",
    "            self.second_count += (position == 2)\n",
    "            self.third_count += (position == 3)\n",
    "            self.writer.add_scalar(\"win_charts/1st_position_prop\",int(position==1),(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/2nd_position_prop\",int(position==2),(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/3rd_position_prop\",int(position==3),(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/3rd_position_all_prop\",int(position==3),(curren_epi+1))\n",
    "            self.writer.add_scalar(\"win_charts/draw\",0,(curren_epi+1))\n",
    "            \n",
    "            self.writer.add_scalar(\"win_charts/1st_position\",self.first_count,(curren_epi+1))#global_step)\n",
    "\n",
    "        \n",
    "            self.writer.add_scalar(\"win_charts/2nd_position\",self.second_count,(curren_epi+1))#global_step)\n",
    "            self.writer.add_scalar(\"win_charts/3rd_position\",self.third_count,(curren_epi+1))#global_step)  \n",
    "            non_draw_count =(curren_epi-self.draw_count+0.000001)\n",
    "            self.writer.add_scalar(\"win_charts/1st_position_to_total_terminated\",self.first_count/non_draw_count,(curren_epi+1))#global_step)\n",
    "\n",
    "        \n",
    "            self.writer.add_scalar(\"win_charts/2nd_position_to_total_terminated\",self.second_count/non_draw_count,(curren_epi+1))#global_step)\n",
    "            self.writer.add_scalar(\"win_charts/3rd_position_to_total_terminated\",self.third_count/non_draw_count,(curren_epi+1))#global_step)             \n",
    "\n",
    "        self.writer.add_scalar(\"win_charts/3rd_position_to_total\",(self.third_count+self.third_count_draw)/(curren_epi +0.00001 ),(curren_epi+1))#global_step)\n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/epsilon\",(curren_epi/((self.num_iterations*self.num_episodes)/10)),self.global_step)\n",
    "        self.writer.add_scalar(\"charts/avg_per_epi_total_reward\", np.mean(list(total_rewards.values())), self.global_step)\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_count_per_episode\",bad_move_count,self.global_step)\n",
    "        \n",
    "        self.writer.add_scalar(\"new_charts/bad_move_count_position_per_episode\",bad_move_phase_count[0],self.global_step)\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_count_attack_per_episode\",bad_move_phase_count[1],self.global_step)\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_count_fortify_per_episode\",bad_move_phase_count[2],self.global_step)\n",
    "\n",
    "        self.writer.add_scalar(\"new_charts/total_moves\",sum(move_count.values()),self.global_step)\n",
    "\n",
    "\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_to_step_count_per_episode\",bad_move_count/(sum(move_count.values())+1),self.global_step)\n",
    "\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_to_step_position_per_episode\",bad_move_phase_count[0]/( move_count[0]+1),self.global_step)\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_to_step_attack_per_episode\",bad_move_phase_count[1]/( move_count[1]+1),self.global_step)\n",
    "        self.writer.add_scalar(\"new_charts/bad_move_to_step_fortify_per_episode\",bad_move_phase_count[2]/( move_count[2]+1),self.global_step)\n",
    "\n",
    "\n",
    "        #values_total = {i:0 for i in self.env.possible_agents}\n",
    "        \n",
    "\n",
    "        \n",
    "        self.writer.add_scalar(\"charts/episodic_length\", cur_epi_list[:step].sum(), self.global_step)\n",
    "        \n",
    "        for i in env.possible_agents:\n",
    "            #cur_index = torch.where((current_agent[:,0] == i) &( cur_epi_list ))[0]\n",
    "\n",
    "            #values_total[i] = values[cur_index].mean()\n",
    "            #writer.add_scalar(\"charts/mean_value_per_epi_agent_\"+str(i), values_total[i], global_step)\n",
    "            \n",
    "            self.writer.add_scalar(\"charts/total_reward_per_epi_agent_\"+str(i), total_rewards[i], self.global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "    def train(self,rb,iteration):\n",
    "        \n",
    "        for epoch in range(self.args.update_epochs):\n",
    "            \n",
    "            if self.global_step > self.args.learning_starts:\n",
    "                data = rb.sample(self.args.batch_size)\n",
    "                with torch.no_grad():\n",
    "                    collected_t_next = data.next_observations[:,-1]\n",
    "                    next_state_actions = self.target_actor(data.next_observations[:,:-1])\n",
    "                    qf1_next_target = self.qf1_target(data.next_observations[:,:-1], next_state_actions)\n",
    "    \n",
    "                    \n",
    "                    next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * (self.args.gamma**(collected_t_next+1)).view(-1) * (qf1_next_target).view(-1)\n",
    "            \n",
    "                qf1_a_values = self.qf1(data.observations[:,:-1], data.actions).view(-1)\n",
    "                qf1_loss = nn.functional.mse_loss(qf1_a_values, next_q_value)\n",
    "                \n",
    "                # optimize the model\n",
    "                self.q_optimizer.zero_grad()\n",
    "                qf1_loss.backward()\n",
    "                self.q_optimizer.step()\n",
    "                \n",
    "                #if global_step % args.policy_frequency == 0:\n",
    "                actor_loss = -self.qf1(data.observations[:,:-1], self.actor(data.observations[:,:-1])).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "            \n",
    "                # update the target network\n",
    "                for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "                    target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "                for param, target_param in zip(self.qf1.parameters(), self.qf1_target.parameters()):\n",
    "                    target_param.data.copy_(self.args.tau * param.data + (1 - self.args.tau) * target_param.data)\n",
    "    \n",
    "                ind_epoch = epoch + (iteration-1)*self.args.update_epochs\n",
    "                \n",
    "                self.writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), ind_epoch)\n",
    "                \n",
    "                self.writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), ind_epoch)\n",
    "                self.writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), ind_epoch)\n",
    "                #print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                \n",
    "        \n",
    "    def save_models(self):\n",
    "        newpath = r'./models/' + self.run_name \n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        torch.save(self.actor.state_dict(), newpath+\"/actor.pt\")\n",
    "        torch.save(self.qf1.state_dict(), newpath+\"/qf1.pt\")\n",
    "        torch.save(self.qf1_target.state_dict(), newpath+\"/qf1_target.pt\")\n",
    "        torch.save(self.target_actor.state_dict(), newpath+\"/target_actor.pt\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "506c53ba-1951-48b4-9474-55bf0ec85797",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Trainer(Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e73ff-3049-4319-b9da-dc7e3f06cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: 105.05000000000004, 2: -202.07, 3: -105.32} ,iteration: 300 ,episode: 0\n",
      "done: {1: True, 2: True, 3: True} ,total_reward: {1: 104.68000000000005, 2: -201.04000000000002, 3: -102.46999999999998} ,iteration: 300 ,episode: 1\n"
     ]
    }
   ],
   "source": [
    "T.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c5e2c7-f84a-4499-ae4b-b312ef1d0d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "T.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49fc8e-f593-4eda-a3ed-382ab50cffd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "dl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
